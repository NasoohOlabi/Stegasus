{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pygtrie\n",
    "\n",
    "# Get the path of the script\n",
    "# script_path = os.path.abspath(__file__)\n",
    "\n",
    "ROOT_DIR = '.'\n",
    "# ROOT_DIR = os.path.dirname(script_path)\n",
    "\n",
    "dict_path = ROOT_DIR + '/dict.pkl'\n",
    "\n",
    "def saveDict():\n",
    "\twith open(dict_path, 'wb') as f:\n",
    "\t\tpickle.dump(trie, f)\n",
    "\n",
    "if not os.path.exists(dict_path):\n",
    "\ttrie = pygtrie.CharTrie()\n",
    "\tsaveDict()\n",
    "\n",
    "# Load the trie from the file using pickle\n",
    "with open(dict_path, 'rb') as f:\n",
    "\ttrie = pickle.load(f)\n",
    "\n",
    "def add_word(word:str) -> None:\n",
    "\ttrie[word] = True\n",
    "\tsaveDict()\n",
    "\t\n",
    "def check_word(word:str) -> bool:\n",
    "\treturn word in trie\n",
    "\n",
    "def del_word(word:str) -> None:\n",
    "\ttry:\n",
    "\t\tdel trie[word]\n",
    "\texcept:\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from math import log2\n",
    "import regex as re\n",
    "import sys\n",
    "from statistics import mode \n",
    "\n",
    "\n",
    "# Get the path of the script\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Generator, List, Optional, Tuple\n",
    "\n",
    "# from .UDict import add_word, check_word \n",
    "import Levenshtein\n",
    "from language_tool_python import LanguageTool\n",
    "from icecream import ic\n",
    "\n",
    "# Initialize the LanguageTool tool\n",
    "lang_tool = LanguageTool('en-US', config={ 'cacheSize': 1000, 'pipelineCaching': True })\n",
    "\n",
    "# Get the path of the script\n",
    "script_path = '.'\n",
    "# script_path = os.path.abspath(__file__)\n",
    "\n",
    "# ROOT_DIR = os.path.dirname(script_path)\n",
    "ROOT_DIR = '.'\n",
    "\n",
    "def parseRules(name, ROOT_DIR=ROOT_DIR+'/rules') -> Generator[Tuple[str, str], None, None]:\n",
    "   with open(ROOT_DIR + f'/{name}.tsv', 'r') as f:\n",
    "     for line in f:\n",
    "         line = line.strip()\n",
    "         if len(line) == 0:\n",
    "            continue\n",
    "         line = line.split('\\t')\n",
    "         if len(line) > 1:\n",
    "            yield (line[0], line[1])\n",
    "def compile_first(x:Tuple[str,str])->Tuple[re.Pattern[str],str]:\n",
    "   try:\n",
    "     return (re.compile(x[0]),x[1])\n",
    "   except:\n",
    "     print(x)\n",
    "     raise ValueError(f'compilable {x}')\n",
    "WORD_CORRECTION_RULES = list(map(compile_first , chain(parseRules('anti.variant'), parseRules('anti.misspelling'))))\n",
    "KEYBOARD_CORRECTION_RULES = list(map(compile_first , parseRules('anti.keyboard')))\n",
    "FAT_CORRECTION_RULES = list(map(compile_first , parseRules('fat.keyboard')))\n",
    "WORD_RULES = list(map(compile_first ,  chain(parseRules('variant'), parseRules('grammatical'), parseRules('misspelling'))))\n",
    "KEYBOARD_RULES = list(map(compile_first,  parseRules('keyboard')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileWriter:\n",
    "\tdef __init__(self, file_path):\n",
    "\t\tself.file = open(file_path, 'w')\n",
    "\n",
    "\tdef write(self, msg):\n",
    "\t\tself.file.write(msg)\n",
    "\t\tself.flush()\n",
    "\n",
    "\tdef flush(self):\n",
    "\t\tself.file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "from util import StringSpans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uppercase_letters(s: str) -> int:\n",
    "   count = 0\n",
    "   for char in s:\n",
    "      if ord(char) < 97 or ord(char) > 122:\n",
    "         count += 1\n",
    "   return count\n",
    "def normal_word(word:str)->bool:\n",
    "   return lang_tool.correct(word) == word or check_word(word)\n",
    "def string_mutation_distance(str1: str, str2: str) -> int:\n",
    "   \"\"\"Returns the number of mutations required to transform str1 into str2\"\"\"\n",
    "   return Levenshtein.distance(str1, str2)\n",
    "def show_diff(a: str, b: str):\n",
    "   l_a = StringSpans(a).get_words()\n",
    "   l_b = StringSpans(b).get_words()\n",
    "   for i in range(min(len(l_a), len(l_b))):\n",
    "      if l_a[i] != l_b[i]:\n",
    "         print(f'i:{i} a:\"{l_a[i]}\" b:\"{l_b[i]}\"')\n",
    "def diff(a: str, b: str) -> List[Tuple[str,str]]:\n",
    "   l_a = StringSpans(a).get_words()\n",
    "   l_b = StringSpans(b).get_words()\n",
    "   return [(l_a[i], l_b[i]) for i in range(min(len(l_a), len(l_b)))\n",
    "      if l_a[i] != l_b[i]]\n",
    "def apply_match(text:str, match_result: Tuple[Tuple[int,int],str,re.Pattern], verbose: bool = False) -> str:\n",
    "   span, repl, regex = match_result\n",
    "   if verbose:\n",
    "      print(f\"Before replace: {text}\")\n",
    "   replaced_text = regex.sub(repl, text[span[0]:span[1]])\n",
    "   after_replace_text = text[:span[0]] + replaced_text + text[span[1]:]\n",
    "   if verbose:\n",
    "      print(f\"After replace: {after_replace_text}\")\n",
    "   return after_replace_text\n",
    "def keyboard_rules_scan(text: str)->List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   matches = []\n",
    "   rules = KEYBOARD_RULES\n",
    "   for regex, repl in rules:\n",
    "     for x in regex.finditer(text,overlapped=True):\n",
    "       matches.append((x.span(), repl, regex))\n",
    "   return matches\n",
    "def word_rules_scan(text: str)->List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   matches = []\n",
    "   for regex, repl in WORD_RULES:\n",
    "     x = regex.match(text)\n",
    "     if x is not None:\n",
    "       start, end = x.span()\n",
    "       matches.append(((start, end), repl, regex))\n",
    "   return matches\n",
    "def rules_scan(text: str)-> List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   result = word_rules_scan(text) + keyboard_rules_scan(text)\n",
    "   result.sort()\n",
    "   return result\n",
    "def expand_span_to_word(words:List[Tuple[int,int]],span:Tuple[int,int])->Tuple[Tuple[int,int],Tuple[int,int],int]:\n",
    "   ss, se = span\n",
    "   for i, (start,end) in enumerate(words):\n",
    "      if start <= ss and se <= end:\n",
    "         return (start,end),(ss-start,se-start),i\n",
    "   for i, (start,end) in enumerate(words):\n",
    "      if start > ss and se <= end:\n",
    "         return (start,end),(start,se-start),i\n",
    "      elif start <= ss and se > end:\n",
    "         return (start,end),(ss-start,end-start),i\n",
    "   \n",
    "   raise ValueError(f'sth is wrong {words} {span}')\n",
    "def valid_matches(text:str, slots:List[Tuple[Tuple[int, int], str, re.Pattern]], verbose=False):\n",
    "   texas = StringSpans(text)\n",
    "   mutations: List[str] = list(map(lambda x: '', slots))\n",
    "\n",
    "   # Apply the match to the text and get the resulting strings\n",
    "   for match_index, match_result in enumerate(slots):\n",
    "      span, repl, regex = match_result\n",
    "      ex_span,relative_span,ex_span_index = expand_span_to_word(texas.words,span)\n",
    "      old_word = texas.get(ex_span)\n",
    "      new_word = apply_match(old_word,(relative_span,repl,regex),verbose)\n",
    "      new_word_corrections = corrections(new_word,verbose=verbose)\n",
    "      if len(new_word_corrections) > 0:\n",
    "         new_word_corrections_mode = mode(new_word_corrections)\n",
    "      else:\n",
    "         if verbose:\n",
    "            print(f'new_word: {new_word} can\\'t be fixed')\n",
    "         new_word_corrections_mode = ''\n",
    "      if normal_word(old_word) \\\n",
    "            and not normal_word(new_word) \\\n",
    "            and new_word[0].lower() == old_word[0].lower() \\\n",
    "            and new_word[-1].lower() == old_word[-1].lower() \\\n",
    "            and new_word_corrections_mode == old_word:\n",
    "         mutations[match_index] = texas.replace_word(ex_span_index,new_word)\n",
    "      else:\n",
    "         if verbose:\n",
    "            print(f'rule undetectable or modify looks! new word \"{new_word}\" != \"{old_word}\" original and will be corrected to {new_word_corrections_mode} from {new_word_corrections}')\n",
    "\n",
    "\n",
    "\n",
    "   # Check for ambiguous and invalid matches\n",
    "   ambiguous_invalid_matches = [i for i, new_string in enumerate(mutations)\n",
    "         if not new_string or new_string in mutations[:i] or normalize(new_string,verbose=verbose) != text]\n",
    "\n",
    "   # Create a list of valid matches\n",
    "   valid_slots = [elem for i, elem in enumerate(slots) if i not in ambiguous_invalid_matches]\n",
    "   \n",
    "   # Print the list of matches and their mutations if verbose output is enabled\n",
    "   if verbose:\n",
    "      print('\\n'+('%'*20)+'valid slots!'+('%'*20))\n",
    "      for v in list(zip(slots, mutations)):\n",
    "         print(v)\n",
    "\n",
    "   return valid_slots\n",
    "def valid_rules_scan(text:str,verbose=False):\n",
    "   proposed_slots = rules_scan(text)\n",
    "   if verbose:\n",
    "      print('proposed_slots: ',proposed_slots)\n",
    "   valid_slots = valid_matches(text,proposed_slots,verbose=verbose)\n",
    "   if verbose:\n",
    "      print('valid_slots: ')\n",
    "      for s in valid_slots:\n",
    "         print(s)\n",
    "   return valid_slots\n",
    "def chunker(text:str,span_size = 6) -> List[Tuple[int,int]]:\n",
    "   words = StringSpans(text).words\n",
    "   if len(words) < span_size:\n",
    "      return [(0,len(text))]\n",
    "   chunks = []\n",
    "   last_start = 0\n",
    "   for i in range(span_size-1,len(words),span_size):\n",
    "      chunks.append((last_start,words[i][1]))\n",
    "      if i+1<len(words):\n",
    "         last_start = words[i+1][0] \n",
    "\n",
    "   # last word ends with last word\n",
    "   chunks[-1] = (chunks[-1][0], words[-1][1])\n",
    "   return chunks\n",
    "def word_we_misspelled(word:str,spelling:str,verbose=False):\n",
    "   uls = count_uppercase_letters(word)\n",
    "   if string_mutation_distance(spelling,word) == 1 \\\n",
    "     and spelling[0].lower() == word[0].lower() \\\n",
    "     and spelling[-1].lower() == word[-1].lower() \\\n",
    "     and uls == 2 \\\n",
    "     and uls < len(word):\n",
    "\n",
    "     for regex,repl in FAT_CORRECTION_RULES:\n",
    "       if regex.sub(repl,word) != spelling:\n",
    "         if verbose:\n",
    "            print(f\"FAT_CORRECTION_RULES ({regex}) ({repl}): {regex.sub(repl,word)} == {spelling}\")\n",
    "         return True\n",
    "     return False\n",
    "   else:\n",
    "     return False # speller is wrong since input is ai generated and the only source for bad spelling is us and it's probably a name of sth\n",
    "def spell_word(word:str,verbose=False) -> str:\n",
    "   if normal_word(word):\n",
    "      return word\n",
    "   spellingOpt = lang_tool.check(word)[0].replacements[0]\n",
    "   spelling = spellingOpt if spellingOpt is not None else word\n",
    "   return spelling if word_we_misspelled(word,spelling,verbose) else word \n",
    "def correction_rules_subset(text:str,verbose=False):\n",
    "   return [rule for rule in lang_tool.check(text) if rule.category in ['TYPOS','SPELLING','GRAMMAR','TYPOGRAPHY']]\n",
    "def normalize(text:str,verbose=False,learn=False):\n",
    "   chunks: List[Tuple[int,int]] = chunker(text) # size = chunks\n",
    "   to_be_original = text\n",
    "   offsets: List[int]= [x.offset for x in correction_rules_subset(text,verbose=verbose)] # size = offsets\n",
    "   empty_chunks = [False for _ in chunks] # size = chunks\n",
    "   text_sss = StringSpans(text)\n",
    "   affected_words = []\n",
    "   for o in offsets:\n",
    "      closest_word = None\n",
    "      closest_distance = float('inf')\n",
    "      for s, e in text_sss.words:\n",
    "         if o < s:  # o is to the left of the current word\n",
    "               distance = s - o\n",
    "         elif o > e:  # o is to the right of the current word\n",
    "               distance = o - e\n",
    "         else:  # o is inside the current word\n",
    "               distance = 0\n",
    "         if distance < closest_distance:\n",
    "               closest_distance = distance\n",
    "               closest_word = text[s:e]\n",
    "      affected_words.append(closest_word)\n",
    "\n",
    "   if verbose:\n",
    "      print(f'text={text}')\n",
    "      print(f'chunks={chunks}')\n",
    "      print(f'offsets={offsets}')\n",
    "      print(f'text_sss.words={text_sss.words}')\n",
    "      print(f'text_sss.get_words()={text_sss.get_words()}')\n",
    "      print(f'affected_words={affected_words}')\n",
    "   offsets_chunks = []\n",
    "   for chunk_start, chunk_end in chunks:\n",
    "      chunk_offsets = []\n",
    "      for i, o in enumerate(offsets):\n",
    "         if verbose:\n",
    "            print(f'iter={i}, o={o}')\n",
    "         if chunk_start <= o < chunk_end:\n",
    "            affected_word = affected_words[i]\n",
    "            affected_word_corrections = corrections(affected_word)\n",
    "            chunk_offsets.append((o, affected_word, affected_word_corrections))\n",
    "            if verbose:\n",
    "               print(f\"Added ({o}, {affected_word}, {affected_word_corrections}) to chunk_offsets\")\n",
    "      offsets_chunks.append(chunk_offsets)\n",
    "      if verbose:\n",
    "         print(f\"Added {chunk_offsets} to offsets_chunks\")\n",
    "   if verbose:\n",
    "      print(f'chunks_offsets={offsets_chunks}')\n",
    "   for i, offsets_chunk in enumerate(offsets_chunks):\n",
    "      if len(offsets_chunk) > 1:\n",
    "         if verbose:\n",
    "            print(f'len({offsets_chunk})={len(offsets_chunk)} > 1')\n",
    "         empty_chunks[i] = True\n",
    "         if learn:\n",
    "            for o,w,cs in offsets_chunk:\n",
    "               add_word(w)\n",
    "      elif len(offsets_chunk) == 1 and len(offsets_chunk[0][2]) == 0:\n",
    "         if verbose:\n",
    "            print(f'no suggestions for {offsets_chunk[0][1]} added to dict')\n",
    "         empty_chunks[i] = True\n",
    "         if learn:\n",
    "            add_word(offsets_chunk[0][1])\n",
    "      elif len(offsets_chunk) == 1:\n",
    "         cs = offsets_chunk[0][2]\n",
    "         if verbose:\n",
    "            print(f'typo={offsets_chunk[0][1]}\\nsuggestion={mode(cs)}')\n",
    "            print(f'votes={cs}')\n",
    "         to_be_original = to_be_original.replace(offsets_chunk[0][1],mode(cs))\n",
    "      else:\n",
    "         empty_chunks[i] = True\n",
    "   \n",
    "   return to_be_original\n",
    "def corrections (typo,verbose=False):\n",
    "   suggestion = spell_word(typo)\n",
    "   votes = [suggestion] if string_mutation_distance(suggestion,typo) == 1 and normal_word(suggestion) else []\n",
    "   for regex,repl in FAT_CORRECTION_RULES:\n",
    "      matches = ((x.span(), repl, regex) for x in regex.finditer(typo,overlapped=True))\n",
    "      for match in matches:\n",
    "         votes.append(apply_match(typo,match))\n",
    "         \n",
    "   for regex,repl in WORD_CORRECTION_RULES:\n",
    "      if regex.match(typo) is not None:\n",
    "         votes.append(regex.sub(repl,typo))\n",
    "\n",
    "   for regex,repl in KEYBOARD_CORRECTION_RULES:\n",
    "      matches = ((x.span(), repl, regex) for x in regex.finditer(typo,overlapped=True))\n",
    "      for match in matches:\n",
    "         votes.append(apply_match(typo,match))\n",
    "         \n",
    "   if verbose:\n",
    "      print(f'unfiltered votes {votes}')\n",
    "   votes = [v for v in votes if  normal_word(v)]\n",
    "   if verbose:\n",
    "      print(f'filtered votes {votes}')\n",
    "   return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Typo:\n",
    "   \"\"\"Class for Typo Engine.\"\"\"\n",
    "\n",
    "   text: str = field(repr=False)\n",
    "   _length: int = field(init=False, repr=False)\n",
    "   _slots: Optional[List[Tuple[Tuple[int, int], str, re.Pattern]]] = field(init=True, repr=False,default=None)\n",
    "   _spaces: Optional[List[int]] = field(init=True, repr=False,default=None)\n",
    "   verbose: bool = field(init=True,repr=False,default=False)\n",
    "\n",
    "   def __post_init__(self):\n",
    "      if self.text != normalize(self.text,self.verbose):\n",
    "         raise ValueError(\"Text isn't spelled correctly\")\n",
    "   @staticmethod\n",
    "   def isAcceptable(text:str,verbose:bool=False):\n",
    "      return text == normalize(text,verbose)\n",
    "   @staticmethod\n",
    "   def FixText(text:str,verbose=False):\n",
    "      return normalize(text,verbose)\n",
    "   def apply(self, space: int, offset: int, text: str) -> str:\n",
    "      if self.verbose:\n",
    "         print(f\"apply: space={space}, offset={offset}, text={text}\")\n",
    "      if offset == 0:\n",
    "         return text\n",
    "      match_tuple = self.slots[sum(self.spaces[0:space]) + offset - 1]\n",
    "      applied = apply_match(text, match_tuple,self.verbose)\n",
    "      if self.verbose:\n",
    "         print(f\"applied: {applied}\")\n",
    "      return applied\n",
    "   @property\n",
    "   def slots(self):\n",
    "      if self._slots is None:\n",
    "         self._slots = valid_rules_scan(self.text,self.verbose)\n",
    "      return self._slots\n",
    "   @property\n",
    "   def length(self) -> int:\n",
    "      return len(self.slots)\n",
    "   @length.setter\n",
    "   def length(self, length: int):\n",
    "      pass\n",
    "   @property\n",
    "   def spaces(self) -> List[int]:\n",
    "      if self._spaces is not None:\n",
    "         return self._spaces\n",
    "      \n",
    "      sentence_ranges = chunker(self.text)\n",
    "      \n",
    "      # Initialize an empty list of buckets\n",
    "      num_buckets = len(sentence_ranges)\n",
    "      buckets: List[int] = [0 for _ in range(num_buckets)]\n",
    "      \n",
    "      # Iterate through each element range and put it in the corresponding bucket\n",
    "      for i, (start, end) in enumerate(span for span,_,_ in self.slots):\n",
    "         for j, (sent_start, sent_end) in enumerate(sentence_ranges):\n",
    "            if sent_start <= start < sent_end and sent_start < end <= sent_end:\n",
    "               buckets[j] += 1\n",
    "               break\n",
    "      return buckets\n",
    "   @spaces.setter\n",
    "   def spaces(self, value):\n",
    "      pass\n",
    "   @property\n",
    "   def bits(self):\n",
    "      return list(map(int, map(lambda x : log2(x + 1), self.spaces)))\n",
    "   @bits.setter\n",
    "   def bits(self, bits: int):\n",
    "      pass\n",
    "   def encode(self, values:List[int]):\n",
    "      spaces = self.spaces\n",
    "      if len(values) > len(spaces):\n",
    "         raise ValueError(\"Can't encode\")\n",
    "      for i in range(len(values)):\n",
    "         # spaces[i] = 0 means that the chunk has a birth defect\n",
    "         # a typo not by us making the chunk unusable and in that case \n",
    "         # values[i] = 0 and the fact that it's an un-fixable typo will\n",
    "         # tell the decoder to learn it\n",
    "         if values[i] >= spaces[i] and spaces[i] != 0:\n",
    "            raise ValueError(\"Won't fit\")\n",
    "      result = self.text\n",
    "      for i in range(len(values) - 1, -1, -1):\n",
    "         result = self.apply(i, values[i], result)\n",
    "      return result\n",
    "   @staticmethod\n",
    "   def decode(text:str,verbose=False,test_self=None) -> Tuple[str,List[int]]:\n",
    "      original = normalize(text,verbose)\n",
    "      if test_self is not None:\n",
    "         if original != test_self.text:\n",
    "            print(f'original=\\n{original}')\n",
    "            print(f'test_self.text=\\n{test_self.text}')\n",
    "         assert original == test_self.text\n",
    "      t = Typo(original)\n",
    "\n",
    "      return original, t._decode(text,test_self)\n",
    "   def _decode(self, text:str,test=None) -> List[int]:\n",
    "      a_self = test if test is not None else self\n",
    "      spaces = a_self.spaces\n",
    "      cnt = len(diff(text,a_self.text))\n",
    "      if a_self.verbose:\n",
    "         print(f'cnt={cnt}')\n",
    "         print(f'diff(text,a_self.text)={diff(text,a_self.text)}')\n",
    "      values = [0 for s in spaces]\n",
    "      for index, space in enumerate(spaces):\n",
    "         isZero = True\n",
    "         for i in range(space):\n",
    "            values[index] = i\n",
    "            dif = diff(text, a_self.encode(values))\n",
    "            if len(dif) == cnt - 1:\n",
    "               if a_self.verbose:\n",
    "                  print(f'values={values}')\n",
    "                  print(f'dif={dif}')\n",
    "               cnt -= 1\n",
    "               isZero = False\n",
    "               break \n",
    "         if isZero:\n",
    "            values[index] = 0     \n",
    "            if a_self.verbose:\n",
    "               print(f'chunk is empty values={values}')\n",
    "      return  values\n",
    "   def encode_encoder(self, bytes_str: str) -> Tuple[List[int], str]:\n",
    "      if not set(bytes_str) <= set('01'):\n",
    "         raise ValueError(f\"bytes_str isn't a bytes string : '{bytes_str}'\")\n",
    "      values = self.bits\n",
    "      bit_values = []\n",
    "      remaining_bits = bytes_str\n",
    "      for i, val in enumerate(values):\n",
    "         if len(remaining_bits) >= val + 1 and int(remaining_bits[:val+1]) < self.spaces[i]:\n",
    "            bit_value = int(remaining_bits[:val+1], 2)\n",
    "            bit_values.append(bit_value)\n",
    "            remaining_bits = remaining_bits[val+1:]\n",
    "         elif len(remaining_bits) >= val and val > 0:\n",
    "            bit_value = int(remaining_bits[:val], 2)\n",
    "            bit_values.append(bit_value)\n",
    "            remaining_bits = remaining_bits[val:]\n",
    "         else:\n",
    "            bit_values.append(0)\n",
    "      return bit_values, remaining_bits\n",
    "   def learn(self,text:str)->None:\n",
    "      normalize(text,learn=True,verbose=self.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t= Typo('Hi How 🤷 are you 👧')\n",
    "t.spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTypoInstance(t,verbose=False,testName='test'):\n",
    "    # patch\n",
    "   ORIGINAL_STDOUT = sys.stdout\n",
    "   sys.stdout = FileWriter(testName+'.txt')\n",
    "\n",
    "   if isinstance (t,str):\n",
    "     t = Typo(t,verbose=verbose)\n",
    "   spaces = t.spaces\n",
    "\n",
    "   print(f\"t.spaces = {spaces}\")\n",
    "   print(f\"t.bits = {t.bits}\")\n",
    "   print(f\"max={max(spaces)}\")\n",
    "   print(f\"len={len(spaces)}\")\n",
    "   \n",
    "   g = (list(map(lambda x: i % x ,spaces)) for i in range(max(spaces)))\n",
    "   for v in g:\n",
    "     print(f'{v}')\n",
    "     encoded = t.encode(v)\n",
    "     print(f\"after encoding {v} {encoded}\")\n",
    "     org, x = Typo.decode(encoded,test_self=t)\n",
    "     print(f'original text candidate \"{org}\"')\n",
    "     if not x == v:\n",
    "       print(f'\\nt.decode(t.encode(v)):{x}')\n",
    "       print(f't.text:{t.text}')\n",
    "     if org == t.text and x == v:\n",
    "       print(('>'*100)+\" passed!\")\n",
    "     else:\n",
    "       print(f'org == t.text and x == v\\n{org == t.text} and {x == v}\\n{org} == {t.text} and {x} == {v}')\n",
    "       assert org == t.text and x == v\n",
    "   # restore\n",
    "   sys.stdout = ORIGINAL_STDOUT\n",
    "   return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🔥?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bugtext = 'Hi ✌️, How 🚴😡👆🏽🚍 are yiou 🔥?'\n",
    "bugtext[26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=Hi ✌️, How 🚴😡👆🏽🚍 are yiou 🔥?\n",
      "chunks=[(0, 28)]\n",
      "offsets=[26]\n",
      "text_sss.words=[(0, 2), (7, 10), (17, 20), (21, 25)]\n",
      "text_sss.get_words()=['Hi', 'How', 'are', 'yiou']\n",
      "affected_words=['yiou']\n",
      "iter=0, o=26\n",
      "Added (26, yiou, ['you']) to chunk_offsets\n",
      "Added [(26, 'yiou', ['you'])] to offsets_chunks\n",
      "chunks_offsets=[[(26, 'yiou', ['you'])]]\n",
      "typo=yiou\n",
      "suggestion=you\n",
      "votes=['you']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi ✌️, How 🚴😡👆🏽🚍 are you 🔥?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(bugtext,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTexts = [\n",
    "   '''Hey, How are you? Did you see the last John Cena movie?'''\n",
    ",'''Hi, How are you?'''\n",
    ",'However, you may as well just use a function statement instead; the only advantage that a lambda offers is that you can put a function definition for a simple expression inside a larger expression.'\n",
    ", '''However, you may as well just use a function statement instead; the only advantage that a lambda offers is that you can put a function definition for a simple expression inside a larger expression. But the above lambda is not part of a larger expression, it is only ever part of an assignment statement, binding it to a name. That's exactly what a statement would achieve.'''\n",
    ", '''I’ve toyed with the idea of using GPT-3’s API to add much more intelligent capabilities to RC, but I can’t deny that I’m drawn to the idea of running this kind of language model locally and in an environment I control. I’d like to someday increase the speed of RC’s speech synthesis and add a speech-to-text translation model in order to achieve real-time communication between humans and the chatbot. I anticipate that with this handful of improvements, RC will be considered a fully-fledged member of our server. Often, we feel that it already is.'''\n",
    "]\n",
    "# tests take ~ 2 hours all passed!\n",
    "# TestTypos = [testTypoInstance(text,verbose=True,testName=f'test{i}') for i, text in enumerate(testTexts)]\n",
    "# LAST_TESTED_TYPO = testTypoInstance('Hi How 🤷 are you 👧',verbose=True,testName=f'after emojier') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = '''I’ve toyed with the idea of using GPT-3’s API to add much more intelligent capabilities to RC, but I can’t deny that I’m drawn to the idea of running this kind of language model locally and in an environment I control. I’d like to someday increase the speed of RC’s speech synthesis and add a speech-to-text translation model in order to achieve real-time communication between humans and the chatbot. I anticipate that with this handful of improvements, RC will be considered a fully-fledged member of our server. Often, we feel that it already is.'''\n",
    "# testTexts[0] == normalize('''I’ve toyed with the idea of using GPT-3’s API to add much more intelligent capabilities to RC, but I casn’t deny that I’m drawn to the idea of running this kind of language model locally and in an environment I control. I’d like to someday increase the speed of RC’s speech synthesis and add a speech-to-text translation model in order to achieve real-time communication between humans and the chatbot. I anticipate that with this handful of improvements, RC will be considered a fully-fledged member of our server. Often, we feel that it already is.''',verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
