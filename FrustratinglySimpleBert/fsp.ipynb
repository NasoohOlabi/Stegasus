{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from io import StringIO\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from icecream import ic\n",
    "from nltk.corpus import stopwords\n",
    "from torch import Tensor\n",
    "from transformers import BertForMaskedLM, BertTokenizer  # type: ignore\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "from Stegasus.SemanticMasking import MaskGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Stegasus.SemanticMasking.SemanticMask.MaskGen(text: str, tokenizer=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaskGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def getTokenizerAndModel(model_name_or_path: str = 'bert-base-cased'):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "    temp = BertForMaskedLM.from_pretrained(model_name_or_path)\n",
    "    assert isinstance(temp, BertForMaskedLM)\n",
    "    model: BertForMaskedLM = temp\n",
    "    return tokenizer, model\n",
    "TOKENIZER, MODEL = getTokenizerAndModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-type for \"numeric\" things; matches our docs\n",
    "Number = Union[int, float, bool]\n",
    "\n",
    "@dataclass\n",
    "class PreprocessedText:\n",
    "  input_ids: torch.Tensor\n",
    "  masked_ids: torch.Tensor\n",
    "  sorted_output: Tuple[torch.Tensor, torch.Tensor]\n",
    "  def __iter__(self):\n",
    "    yield self.input_ids\n",
    "    yield self.masked_ids\n",
    "    yield self.sorted_output[0]\n",
    "    yield self.sorted_output[1]\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class MaskedStegoResult:\n",
    "  encoded_text: str\n",
    "  encoded_bytes: str\n",
    "  remaining_bytes: str\n",
    "\n",
    "class MaskedStego:\n",
    "  \"\"\"\n",
    "  Examples\n",
    "  masked_stego.decode(\"The quick red fox jumps over the poor dog.\", 3, 0.01))\n",
    "  masked_stego(\"The quick brown fox jumps over the lazy dog. and said boom you lazy dog stay back\",'010101010101', 3, 0.01))\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self,tokenizer,model) -> None: \n",
    "    MaskedStego._STOPWORDS: List[str] = stopwords.words('english')\n",
    "    self._model = model\n",
    "    self._tokenizer = tokenizer\n",
    "\n",
    "  def __call__(self, cover_text: str, message: str, mask_interval: int = 3, score_threshold: float = 0.01) -> MaskedStegoResult:\n",
    "    assert set(message) <= set('01')\n",
    "    message_io = StringIO(message)\n",
    "    input_ids, masked_ids, sorted_score, indices = self._preprocess_text(cover_text, mask_interval)\n",
    "    for i_token, token in enumerate(masked_ids):\n",
    "      if token != self._tokenizer.mask_token_id:\n",
    "        continue\n",
    "      ids = indices[i_token]\n",
    "      scores = sorted_score[i_token]\n",
    "      candidates = self._pick_candidates_threshold(ids, scores, score_threshold)\n",
    "      if len(candidates) < 2:\n",
    "        continue\n",
    "      replace_token_id = self._block_encode_single(candidates, message_io).item()\n",
    "      input_ids[i_token] = replace_token_id\n",
    "    encoded_message: str = message_io.getvalue()[:message_io.tell()]\n",
    "    message_io.close()\n",
    "    stego_text = self._tokenizer.decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return MaskedStegoResult(encoded_text=stego_text,encoded_bytes=encoded_message,remaining_bytes=message[len(encoded_message):])\n",
    "\n",
    "  def decode(self, stego_text: str, mask_interval: int = 3, score_threshold: float = 0.005) -> str:\n",
    "    decoded_message: List[str] = []\n",
    "    \n",
    "    input_ids, masked_ids, sorted_score, indices = self._preprocess_text(stego_text, mask_interval)\n",
    "    \n",
    "    for i_token, token in enumerate(masked_ids):\n",
    "      if token != self._tokenizer.mask_token_id:\n",
    "        continue\n",
    "      ids = indices[i_token]\n",
    "      scores = sorted_score[i_token]\n",
    "      candidates = self._pick_candidates_threshold(ids, scores, score_threshold)\n",
    "      if len(candidates) < 2:\n",
    "        continue\n",
    "      chosen_id = int(input_ids[i_token].item())\n",
    "      decoded_message.append(self._block_decode_single(candidates, chosen_id))\n",
    "\n",
    "    return ''.join(decoded_message)\n",
    "\n",
    "  def _preprocess_text(self, sentence: str, mask_interval: int) -> \"PreprocessedText\":\n",
    "    encoded_ids = self._tokenizer([sentence], return_tensors='pt').input_ids[0]\n",
    "    masked_ids = self._mask(encoded_ids.clone(), mask_interval)\n",
    "    sorted_score, indices = self._predict(masked_ids)\n",
    "    return PreprocessedText(input_ids=encoded_ids,masked_ids=masked_ids,sorted_output=(sorted_score,indices))\n",
    "\n",
    "  def _mask(self, input_ids, mask_interval: int) -> Tensor:\n",
    "    ic(input_ids,mask_interval)\n",
    "    length = len(input_ids)\n",
    "    tokens: List[str] = self._tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    ic(tokens)\n",
    "    offset = mask_interval // 2 + 1\n",
    "    mask_count = offset\n",
    "    # TODO remove mask_interval\n",
    "    #   or  handle the case if interval -1\n",
    "    #   use [s[i[0],i[1]] for i in extract_pos]\n",
    "    #   note that token is a string list to compare is the str\n",
    "    #   is in the pos\n",
    "    #   maybe use alongside the interval ... idk\n",
    "    #   \n",
    "    for i, token in enumerate(tokens):\n",
    "      # Skip initial subword\n",
    "      if i + 1 < length and self._is_subword(tokens[i + 1]): continue\n",
    "      if not self._substitutable_single(token): continue\n",
    "      if mask_count % mask_interval == 0:\n",
    "        input_ids[i] = self._tokenizer.mask_token_id\n",
    "      mask_count += 1\n",
    "    ic(self._tokenizer.mask_token_id)\n",
    "    ic(self._tokenizer.convert_ids_to_tokens(input_ids))\n",
    "    ic(input_ids)\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "  def _predict(self, input_ids: Tensor):\n",
    "    self._model.eval()\n",
    "    with torch.no_grad():\n",
    "      output = self._model(input_ids.unsqueeze(0))['logits'][0]\n",
    "      softmaxed_score = F.softmax(output, dim=1)  # [word_len, vocab_len]\n",
    "      return softmaxed_score.sort(dim=1, descending=True)\n",
    "\n",
    "  def _pick_candidates_threshold(self, ids: Tensor, scores: Tensor, threshold: float):\n",
    "    filtered_ids = ids[scores >= threshold]\n",
    "    def filter_fun(idx: Tensor) -> bool:\n",
    "      return self._substitutable_single(self._tokenizer.convert_ids_to_tokens(int(idx.item())))\n",
    "    return list(filter(filter_fun, filtered_ids))\n",
    "\n",
    "  def _substitutable_single(self, token: str) -> bool:\n",
    "    if self._is_subword(token): return False\n",
    "    if token.lower() in MaskedStego._STOPWORDS: return False\n",
    "    if not token.isalpha(): return False\n",
    "    return True\n",
    "\n",
    "  @staticmethod\n",
    "  def _block_encode_single(ids: List[torch.Tensor], message: StringIO) -> torch.Tensor:\n",
    "    assert len(ids) > 0\n",
    "    if len(ids) == 1:\n",
    "      return ids[0]\n",
    "    capacity = len(ids).bit_length() - 1\n",
    "    bits_str = message.read(capacity)\n",
    "    if len(bits_str) < capacity:\n",
    "      padding: str = '0' * (capacity - len(bits_str))\n",
    "      bits_str = bits_str + padding\n",
    "      message.write(padding)\n",
    "    index = int(bits_str, 2)\n",
    "    return ids[index]\n",
    "\n",
    "  @staticmethod\n",
    "  def _block_decode_single(ids: List[Tensor], chosen_id: int) -> str:\n",
    "    if len(ids) < 2:\n",
    "      return ''\n",
    "    capacity = len(ids).bit_length() - 1\n",
    "    index = ids.index(chosen_id) # type: ignore\n",
    "    return format(index, '0' + str(capacity) +'b')\n",
    "\n",
    "  @staticmethod\n",
    "  def _is_subword(token: str) -> bool:\n",
    "    return token.startswith('##')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| input_ids: tensor([  101,  1109,  3613,  1894, 17594, 15457,  1166,  1103,  2869,  3676,\n",
      "                         117,  1262,   146,  4819, 22559, 17260,  1116,   119,   102])\n",
      "    mask_interval: 3\n",
      "ic| tokens: ['[CLS]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "',\n",
      "             'The',\n",
      "             'quick',\n",
      "             'red',\n",
      "             'fox',\n",
      "             'jumps',\n",
      "             'over',\n",
      "             'the',\n",
      "             'poor',\n",
      "             'dog',\n",
      "             ',',\n",
      "             'And',\n",
      "             'I',\n",
      "             'hate',\n",
      "             'token',\n",
      "             '##izer',\n",
      "             '##s',\n",
      "             '.',\n",
      "             '[SEP]']\n",
      "ic| self._tokenizer.mask_token_id: 103\n",
      "ic| self._tokenizer.convert_ids_to_tokens(input_ids): ['[CLS]',\n",
      "                                                       'The',\n",
      "                                                       'quick',\n",
      "                                                       '[MASK]',\n",
      "                                                       'fox',\n",
      "                                                       'jumps',\n",
      "                                                       'over',\n",
      "                                                       'the',\n",
      "                                                       '[MASK]',\n",
      "                                                       'dog',\n",
      "                                                       ',',\n",
      "                                                       'And',\n",
      "                                                       'I',\n",
      "                                                       'hate',\n",
      "                                                       'token',\n",
      "                                                       '##izer',\n",
      "                                                       '##s',\n",
      "                                                       '.',\n",
      "                                                       '[SEP]']\n",
      "ic| input_ids: tensor([  101,  1109,  3613,   103, 17594, 15457,  1166,  1103,   103,  3676,\n",
      "                         117,  1262,   146,  4819, 22559, 17260,  1116,   119,   102])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskedStegoResult(encoded_text='The quick talking fox jumps over the poor dog, And I hate tokenizers.', encoded_bytes='01010', remaining_bytes='1010101')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_stego = MaskedStego(tokenizer=TOKENIZER,model=MODEL)\n",
    "\n",
    "\n",
    "dmo = \"The quick red fox jumps over the poor dog, And I hate tokenizers.\"\n",
    "dmo2 = \"The quick brown fox jumps over the lazy dog. and said boom you lazy dog stay back\"\n",
    "# a = masked_stego.decode(dmo, 3, 0.01)\n",
    "masked_stego(dmo,'010101010101', 3, 0.01)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| MaskGen(dmo,TOKENIZER): SemanticPositions(string='The quick red fox jumps over the poor dog, And I '\n",
      "                                                     'hate tokenizers.',\n",
      "                                              NVA_words=[(4, 9),\n",
      "                                                         (10, 13),\n",
      "                                                         (18, 23),\n",
      "                                                         (33, 37),\n",
      "                                                         (38, 41),\n",
      "                                                         (49, 53),\n",
      "                                                         (54, 64)],\n",
      "                                              tokens=['The',\n",
      "                                                      'quick',\n",
      "                                                      'red',\n",
      "                                                      'fox',\n",
      "                                                      'jumps',\n",
      "                                                      'over',\n",
      "                                                      'the',\n",
      "                                                      'poor',\n",
      "                                                      'dog',\n",
      "                                                      ',',\n",
      "                                                      'And',\n",
      "                                                      'I',\n",
      "                                                      'hate',\n",
      "                                                      'token',\n",
      "                                                      '##izer',\n",
      "                                                      '##s',\n",
      "                                                      '.'],\n",
      "                                              mask=[False,\n",
      "                                                    False,\n",
      "                                                    True,\n",
      "                                                    False,\n",
      "                                                    True,\n",
      "                                                    False,\n",
      "                                                    False,\n",
      "                                                    False,\n",
      "                                                    True,\n",
      "                                                    False,\n",
      "                                                    False,\n",
      "                                                    False,\n",
      "                                                    False,\n",
      "                                                    True,\n",
      "                                                    False,\n",
      "                                                    False,\n",
      "                                                    False])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SemanticPositions(string='The quick red fox jumps over the poor dog, And I hate tokenizers.', NVA_words=[(4, 9), (10, 13), (18, 23), (33, 37), (38, 41), (49, 53), (54, 64)], tokens=['The', 'quick', 'red', 'fox', 'jumps', 'over', 'the', 'poor', 'dog', ',', 'And', 'I', 'hate', 'token', '##izer', '##s', '.'], mask=[False, False, True, False, True, False, False, False, True, False, False, False, False, True, False, False, False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(MaskGen(dmo,TOKENIZER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedStegoResult(encoded_text='The quick thinking fox jumps over the sly dog. and said boom you lazy dog go back', encoded_bytes='01010101', remaining_bytes='0101')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
