{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pygtrie\n",
    "\n",
    "# Get the path of the script\n",
    "# script_path = os.path.abspath(__file__)\n",
    "\n",
    "ROOT_DIR = '.'\n",
    "# ROOT_DIR = os.path.dirname(script_path)\n",
    "\n",
    "dict_path = ROOT_DIR + '/dict.pkl'\n",
    "\n",
    "def saveDict():\n",
    "\twith open(dict_path, 'wb') as f:\n",
    "\t\tpickle.dump(trie, f)\n",
    "\n",
    "if not os.path.exists(dict_path):\n",
    "\ttrie = pygtrie.CharTrie()\n",
    "\tsaveDict()\n",
    "\n",
    "# Load the trie from the file using pickle\n",
    "with open(dict_path, 'rb') as f:\n",
    "\ttrie = pickle.load(f)\n",
    "\n",
    "def add_word(word:str) -> None:\n",
    "\ttrie[word] = True\n",
    "\tsaveDict()\n",
    "\t\n",
    "def check_word(word:str) -> bool:\n",
    "\treturn word in trie\n",
    "\n",
    "def del_word(word:str) -> None:\n",
    "\ttry:\n",
    "\t\tdel trie[word]\n",
    "\texcept:\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from math import log2\n",
    "import regex as re\n",
    "import sys\n",
    "from statistics import mode \n",
    "\n",
    "\n",
    "# Get the path of the script\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Generator, List, Optional, Tuple\n",
    "\n",
    "# from .UDict import add_word, check_word \n",
    "import Levenshtein\n",
    "from language_tool_python import LanguageTool\n",
    "from icecream import ic\n",
    "\n",
    "# Initialize the LanguageTool tool\n",
    "lang_tool = LanguageTool('en-US', config={ 'cacheSize': 1000, 'pipelineCaching': True })\n",
    "\n",
    "# Get the path of the script\n",
    "script_path = '.'\n",
    "# script_path = os.path.abspath(__file__)\n",
    "\n",
    "# ROOT_DIR = os.path.dirname(script_path)\n",
    "ROOT_DIR = '.'\n",
    "\n",
    "def parseRules(name, ROOT_DIR=ROOT_DIR+'/rules') -> Generator[Tuple[str, str], None, None]:\n",
    "   with open(ROOT_DIR + f'/{name}.tsv', 'r') as f:\n",
    "     for line in f:\n",
    "         line = line.strip()\n",
    "         if len(line) == 0:\n",
    "            continue\n",
    "         line = line.split('\\t')\n",
    "         if len(line) > 1:\n",
    "            yield (line[0], line[1])\n",
    "def compile_first(x:Tuple[str,str])->Tuple[re.Pattern[str],str]:\n",
    "   try:\n",
    "     return (re.compile(x[0]),x[1])\n",
    "   except:\n",
    "     print(x)\n",
    "     raise ValueError(f'compilable {x}')\n",
    "WORD_CORRECTION_RULES = list(map(compile_first , chain(parseRules('anti.variant'), parseRules('anti.misspelling'))))\n",
    "KEYBOARD_CORRECTION_RULES = list(map(compile_first , parseRules('anti.keyboard')))\n",
    "FAT_CORRECTION_RULES = list(map(compile_first , parseRules('fat.keyboard')))\n",
    "WORD_RULES = list(map(compile_first ,  chain(parseRules('variant'), parseRules('grammatical'), parseRules('misspelling'))))\n",
    "KEYBOARD_RULES = list(map(compile_first,  parseRules('keyboard')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileWriter:\n",
    "\tdef __init__(self, file_path):\n",
    "\t\tself.file = open(file_path, 'a')\n",
    "\n",
    "\tdef write(self, msg):\n",
    "\t\tself.file.write(msg)\n",
    "\t\tself.flush()\n",
    "\n",
    "\tdef flush(self):\n",
    "\t\tself.file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable\n",
    "\n",
    "class StringSpans:\n",
    "   def __init__(self, string = None):\n",
    "      if string is not None:\n",
    "         self.string = string\n",
    "         self._set_spans(string)\n",
    "   \n",
    "   @staticmethod\n",
    "   def _get_words(text: str,verbose=False) -> Tuple[List[Tuple[int,int]],List[Tuple[int,int]],List[Tuple[int,int]],List[Tuple[int,int]]]:\n",
    "      word_regex = re.compile(r'[a-zA-Z\\'\\-]+')\n",
    "      space_regex = re.compile(r'\\s')\n",
    "      spans = []\n",
    "      words = []\n",
    "      spaces = []\n",
    "      for match in re.finditer(word_regex, text):\n",
    "         spans.append(match.span())\n",
    "         words.append(match.span())\n",
    "      for match in re.finditer(space_regex, text):\n",
    "         spans.append(match.span())\n",
    "         spaces.append(match.span())\n",
    "      spans.sort()\n",
    "      result = []\n",
    "      last = 0\n",
    "      for i in range(len(spans)):\n",
    "         start, end = spans[i]\n",
    "         if start != last:\n",
    "               result.append((last,start))\n",
    "         result.append((start,end))\n",
    "         last = end\n",
    "      if last != len(text):\n",
    "         result.append((last,len(text)))     \n",
    "      non_words = [\n",
    "         (start,end) for start,end in result \n",
    "         if (start,end) not in words and space_regex.match(text[start:end]) is None\n",
    "         ]\n",
    "      non_spaces = [\n",
    "         (start,end) for start,end in result \n",
    "         if (start,end) not in spaces\n",
    "         ]\n",
    "         \n",
    "      return result, words, non_words, non_spaces\n",
    "   \n",
    "   def _set_spans(self, string:str):\n",
    "      spans, words,non_words, non_spaces = StringSpans._get_words(string)\n",
    "      self.words = words\n",
    "      self.spans = spans\n",
    "      self.non_words = non_words\n",
    "      self.non_spaces = non_spaces\n",
    "\n",
    "   def replace_word(self, word_index: int, replacement: str) -> str:\n",
    "      # Check if the word_index is valid\n",
    "      if not (0 <= word_index < len(self.words)):\n",
    "         raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "      \n",
    "      # Get the start and end indices of the word span at the given word_index\n",
    "      start, end = self.words[word_index]\n",
    "      \n",
    "      # Replace the word span with the replacement string\n",
    "      new_string = self.string[:start] + replacement + self.string[end:]\n",
    "      \n",
    "      return new_string\n",
    "   def get_word(self, word_index: int) -> str:\n",
    "      # Check if the word_index is valid\n",
    "      if not (0 <= word_index < len(self.words)):\n",
    "         raise ValueError(f\"Invalid word index: {word_index}\")\n",
    "      \n",
    "      # Get the start and end indices of the word span at the given word_index\n",
    "      start, end = self.words[word_index]\n",
    "      \n",
    "      return self.string[start:end]\n",
    "   def get(self,span):\n",
    "      return self.string[span[0]:span[1]]\n",
    "   def replace_word_StringSpans(self, word_index: int, replacement: str):\n",
    "      ss = StringSpans()\n",
    "      ss.string = self.replace_word(word_index, replacement)\n",
    "      word_start, word_end = self.words[word_index]\n",
    "      span_index = self.spans.index((word_start,word_end))\n",
    "      new_len = len(replacement)\n",
    "      word_len_diff = new_len - (word_end - word_start) \n",
    "      def f(span):\n",
    "         start,end = span\n",
    "         return (start,end) if end < word_start else (start+word_len_diff,end+word_len_diff)\n",
    "      ss.words = [f(span) for span in self.words]\n",
    "      ss.spans = [f(span) for span in self.spans]\n",
    "      ss.non_words = [f(span) for span in self.non_words]\n",
    "      ss.non_spaces = [f(span) for span in self.spans]\n",
    "      ss.words[word_index] = (word_start,word_start+new_len)\n",
    "      ss.spans[span_index] = (word_start,word_start+new_len)\n",
    "      return ss\n",
    "   def get_words(self):\n",
    "      return [self.string[start:end] for start,end in self.words]\n",
    "   def isw(self):\n",
    "      return ((i,start,end,self.words[start:end]) for i,(start,end) in enumerate(self.words))\n",
    "   def iws(self):\n",
    "      return ((start,end,self.words[start:end]) for (start,end) in self.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_uppercase_letters(s: str) -> int:\n",
    "   count = 0\n",
    "   for char in s:\n",
    "      if ord(char) < 97 or ord(char) > 122:\n",
    "         count += 1\n",
    "   return count\n",
    "def normal_word(word:str)->bool:\n",
    "   return lang_tool.correct(word) == word or check_word(word)\n",
    "def string_mutation_distance(str1: str, str2: str) -> int:\n",
    "   \"\"\"Returns the number of mutations required to transform str1 into str2\"\"\"\n",
    "   return Levenshtein.distance(str1, str2)\n",
    "def show_diff(a: str, b: str):\n",
    "   l_a = StringSpans(a).get_words()\n",
    "   l_b = StringSpans(b).get_words()\n",
    "   for i in range(min(len(l_a), len(l_b))):\n",
    "      if l_a[i] != l_b[i]:\n",
    "         print(f'i:{i} a:\"{l_a[i]}\" b:\"{l_b[i]}\"')\n",
    "def diff(a: str, b: str) -> List[Tuple[str,str]]:\n",
    "   l_a = StringSpans(a).get_words()\n",
    "   l_b = StringSpans(b).get_words()\n",
    "   return [(l_a[i], l_b[i]) for i in range(min(len(l_a), len(l_b)))\n",
    "      if l_a[i] != l_b[i]]\n",
    "def apply_match(text:str, match_result: Tuple[Tuple[int,int],str,re.Pattern], verbose: bool = False) -> str:\n",
    "   span, repl, regex = match_result\n",
    "   if verbose:\n",
    "      print(f\"Before replace: {text}\")\n",
    "   replaced_text = regex.sub(repl, text[span[0]:span[1]])\n",
    "   after_replace_text = text[:span[0]] + replaced_text + text[span[1]:]\n",
    "   if verbose:\n",
    "      print(f\"After replace: {after_replace_text}\")\n",
    "   return after_replace_text\n",
    "def keyboard_rules_scan(text: str)->List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   matches = []\n",
    "   rules = KEYBOARD_RULES\n",
    "   for regex, repl in rules:\n",
    "     for x in regex.finditer(text,overlapped=True):\n",
    "       matches.append((x.span(), repl, regex))\n",
    "   return matches\n",
    "def word_rules_scan(text: str)->List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   matches = []\n",
    "   for regex, repl in WORD_RULES:\n",
    "     x = regex.match(text)\n",
    "     if x is not None:\n",
    "       start, end = x.span()\n",
    "       matches.append(((start, end), repl, regex))\n",
    "   return matches\n",
    "def rules_scan(text: str)-> List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   result = word_rules_scan(text) + keyboard_rules_scan(text)\n",
    "   result.sort()\n",
    "   return result\n",
    "def expand_span_to_word(words:List[Tuple[int,int]],span:Tuple[int,int])->Tuple[Tuple[int,int],Tuple[int,int],int]:\n",
    "   ss, se = span\n",
    "   for i, (start,end) in enumerate(words):\n",
    "      if start <= ss and se <= end:\n",
    "         return (start,end),(ss-start,se-start),i\n",
    "   for i, (start,end) in enumerate(words):\n",
    "      if start > ss and se <= end:\n",
    "         return (start,end),(start,se-start),i\n",
    "      elif start <= ss and se > end:\n",
    "         return (start,end),(ss-start,end-start),i\n",
    "   \n",
    "   raise ValueError(f'sth is wrong {words} {span}')\n",
    "def valid_matches(text:str, slots:List[Tuple[Tuple[int, int], str, re.Pattern]], verbose=False):\n",
    "   texas = StringSpans(text)\n",
    "   mutations: List[str] = list(map(lambda x: '', slots))\n",
    "\n",
    "   # Apply the match to the text and get the resulting strings\n",
    "   for match_index, match_result in enumerate(slots):\n",
    "      span, repl, regex = match_result\n",
    "      ex_span,relative_span,ex_span_index = expand_span_to_word(texas.words,span)\n",
    "      old_word = texas.get(ex_span)\n",
    "      new_word = apply_match(old_word,(relative_span,repl,regex),verbose)\n",
    "      new_word_corrections = corrections(new_word,verbose=verbose)\n",
    "      new_word_corrections_mode = mode(new_word_corrections)\n",
    "      if normal_word(old_word) \\\n",
    "            and not normal_word(new_word) \\\n",
    "            and new_word[0].lower() == old_word[0].lower() \\\n",
    "            and new_word[-1].lower() == old_word[-1].lower() \\\n",
    "            and new_word_corrections_mode == old_word:\n",
    "         mutations[match_index] = texas.replace_word(ex_span_index,new_word)\n",
    "      else:\n",
    "         if verbose:\n",
    "            print(f'rule undetectable or modify looks! new word \"{new_word}\" != \"{old_word}\" original and will be corrected to {new_word_corrections_mode} from {new_word_corrections}')\n",
    "\n",
    "\n",
    "   # Print the list of matches and their mutations if verbose output is enabled\n",
    "   if verbose:\n",
    "      print('\\nlist(zip(slots,mutations))'+('%'*20))\n",
    "      for v in list(zip(slots, mutations)):\n",
    "         print(v)\n",
    "\n",
    "   # Check for ambiguous and invalid matches\n",
    "   ambiguous_invalid_matches = [i for i, new_string in enumerate(mutations)\n",
    "         if not new_string or new_string in mutations[:i]]\n",
    "\n",
    "   # Create a list of valid matches\n",
    "   valid_slots = [elem for i, elem in enumerate(slots) if i not in ambiguous_invalid_matches]\n",
    "\n",
    "   return valid_slots\n",
    "def valid_rules_scan(text:str,verbose=False):\n",
    "   proposed_slots = rules_scan(text)\n",
    "   if verbose:\n",
    "     print('proposed_slots: ',proposed_slots)\n",
    "   valid_slots = valid_matches(text,proposed_slots,verbose=verbose)\n",
    "   if verbose:\n",
    "     print('valid_slots: ')\n",
    "     for s in valid_slots:\n",
    "       print(s)\n",
    "   return valid_slots\n",
    "def chunker(text:str,span_size = 6) -> List[Tuple[int,int]]:\n",
    "   words = StringSpans(text).words\n",
    "   if len(words) < span_size:\n",
    "      return [(0,len(text))]\n",
    "   chunks = []\n",
    "   last_start = 0\n",
    "   for i in range(span_size,len(words),span_size):\n",
    "      chunks.append((last_start,words[i-1][1]))\n",
    "      last_start = words[i][0]\n",
    "   \n",
    "   # last word ends with last word\n",
    "   chunks[-1][1] = words[-1][1]\n",
    "   return chunks\n",
    "def word_we_misspelled(word:str,spelling:str,verbose=False):\n",
    "   uls = count_uppercase_letters(word)\n",
    "   if string_mutation_distance(spelling,word) == 1 \\\n",
    "     and spelling[0].lower() == word[0].lower() \\\n",
    "     and spelling[-1].lower() == word[-1].lower() \\\n",
    "     and uls == 2 \\\n",
    "     and uls < len(word):\n",
    "\n",
    "     for regex,repl in FAT_CORRECTION_RULES:\n",
    "       if regex.sub(repl,word) != spelling:\n",
    "         if verbose:\n",
    "            print(f\"FAT_CORRECTION_RULES ({regex}) ({repl}): {regex.sub(repl,word)} == {spelling}\")\n",
    "         return True\n",
    "     return False\n",
    "   else:\n",
    "     return False # speller is wrong since input is ai generated and the only source for bad spelling is us and it's probably a name of sth\n",
    "def spell_word(word:str,verbose=False) -> str:\n",
    "   if normal_word(word):\n",
    "      return word\n",
    "   spellingOpt = lang_tool.check(word)[0].replacements[0]\n",
    "   spelling = spellingOpt if spellingOpt is not None else word\n",
    "   return spelling if word_we_misspelled(word,spelling,verbose) else word \n",
    "def normalize(text:str,verbose=False,learn=False):\n",
    "   chunks = chunker(text)\n",
    "   if verbose:\n",
    "      print(f'chunks={chunks}')\n",
    "   to_be_original = text\n",
    "   offsets = [x.offset for x in lang_tool.check(text)]\n",
    "   if verbose:\n",
    "      print(f'offsets={offsets}')\n",
    "   chunks_offsets = [\n",
    "      [o for o in offsets if chunk_start <= o < chunk_end]\n",
    "      for chunk_start,chunk_end in chunks]\n",
    "   if verbose:\n",
    "      print(f'chunks_offsets={chunks_offsets}')\n",
    "   empty_chunks = [False for _ in chunks]\n",
    "   text_sss = StringSpans(text)\n",
    "   affected_words = [text[s:e] for s,e in text_sss.words if s in offsets]\n",
    "   if verbose:\n",
    "      print(f\"affected_words={affected_words}\")\n",
    "   affected_words_corrections = [corrections(w) for w in affected_words]\n",
    "   if verbose:\n",
    "      print(f\"affected_words_corrections={affected_words_corrections}\")\n",
    "   for i, os in enumerate(chunks_offsets):\n",
    "      if verbose:\n",
    "         print(f'os={os}')\n",
    "      if len(os) > 1:\n",
    "         if verbose:\n",
    "            print(f'len({os})={len(os)} > 1')\n",
    "         empty_chunks[i] = True\n",
    "         if learn:\n",
    "            for o in os:\n",
    "               add_word(affected_words[offsets.index(o)])\n",
    "      elif len(os) == 1 and len(affected_words_corrections[offsets.index(os[0])]) == 0:\n",
    "         if verbose:\n",
    "            print(f'no suggestions for {affected_words[offsets.index(os[0])]} added to dict')\n",
    "         empty_chunks[i] = True\n",
    "         if learn:\n",
    "            add_word(affected_words[offsets.index(os[0])])\n",
    "      elif len(os) == 1:\n",
    "         j = offsets.index(os[0])\n",
    "         cs = affected_words_corrections[j]\n",
    "         if verbose:\n",
    "            print(f'typo={affected_words[j]}\\nsuggestion={mode(cs)}')\n",
    "            print(f'votes={cs}')\n",
    "         to_be_original = to_be_original.replace(affected_words[j],mode(cs))\n",
    "      else:\n",
    "         empty_chunks[i] = True\n",
    "   \n",
    "   return text\n",
    "def corrections (typo,verbose=False):\n",
    "   suggestion = spell_word(typo)\n",
    "   votes = [suggestion] if string_mutation_distance(suggestion,typo) == 1 and normal_word(suggestion) else []\n",
    "   for regex,repl in FAT_CORRECTION_RULES:\n",
    "      matches = ((x.span(), repl, regex) for x in regex.finditer(typo,overlapped=True))\n",
    "      for match in matches:\n",
    "         votes.append(apply_match(typo,match))\n",
    "         \n",
    "   for regex,repl in WORD_CORRECTION_RULES:\n",
    "      if regex.match(typo) is not None:\n",
    "         votes.append(regex.sub(repl,typo))\n",
    "\n",
    "   for regex,repl in KEYBOARD_CORRECTION_RULES:\n",
    "      matches = ((x.span(), repl, regex) for x in regex.finditer(typo,overlapped=True))\n",
    "      for match in matches:\n",
    "         votes.append(apply_match(typo,match))\n",
    "         \n",
    "   if verbose:\n",
    "      print(f'unfiltered votes {votes}')\n",
    "   votes = [v for v in votes if  normal_word(v)]\n",
    "   if verbose:\n",
    "      print(f'filtered votes {votes}')\n",
    "   return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Typo:\n",
    "   \"\"\"Class for Typo Engine.\"\"\"\n",
    "\n",
    "   text: str = field(repr=False)\n",
    "   _length: int = field(init=False, repr=False)\n",
    "   _slots: Optional[List[Tuple[Tuple[int, int], str, re.Pattern]]] = field(init=True, repr=False,default=None)\n",
    "   _spaces: Optional[List[int]] = field(init=True, repr=False,default=None)\n",
    "   verbose: bool = field(init=True,repr=False,default=False)\n",
    "\n",
    "   def __post_init__(self):\n",
    "      if self.text != normalize(self.text,self.verbose):\n",
    "         raise ValueError(\"Text isn't spelled correctly\")\n",
    "\n",
    "   def apply(self, space: int, offset: int, text: str) -> str:\n",
    "      if self.verbose:\n",
    "         print(f\"apply: space={space}, offset={offset}, text={text}\")\n",
    "      if offset == 0:\n",
    "         return text\n",
    "      match_tuple = self.slots[sum(self.spaces[0:space]) + offset - 1]\n",
    "      applied = apply_match(text, match_tuple,self.verbose)\n",
    "      if self.verbose:\n",
    "         print(f\"applied: {applied}\")\n",
    "      return applied\n",
    "   \n",
    "   @property\n",
    "   def slots(self):\n",
    "      if self._slots is None:\n",
    "         self._slots = valid_rules_scan(self.text,self.verbose)\n",
    "      return self._slots\n",
    "\n",
    "   @property\n",
    "   def length(self) -> int:\n",
    "      return len(self.slots)\n",
    "\n",
    "   @length.setter\n",
    "   def length(self, length: int):\n",
    "      pass\n",
    "\n",
    "   @property\n",
    "   def spaces(self) -> List[int]:\n",
    "      if self._spaces is not None:\n",
    "         return self._spaces\n",
    "      \n",
    "      sentence_ranges = chunker(self.text)\n",
    "      \n",
    "      # Initialize an empty list of buckets\n",
    "      num_buckets = len(sentence_ranges)\n",
    "      buckets: List[int] = [0 for _ in range(num_buckets)]\n",
    "      \n",
    "      # Iterate through each element range and put it in the corresponding bucket\n",
    "      for i, (start, end) in enumerate(span for span,_,_ in self.slots):\n",
    "         for j, (sent_start, sent_end) in enumerate(sentence_ranges):\n",
    "            if sent_start <= start < sent_end and sent_start < end <= sent_end:\n",
    "               buckets[j] += 1\n",
    "               break\n",
    "      return buckets\n",
    "\n",
    "   @spaces.setter\n",
    "   def spaces(self, value):\n",
    "      pass\n",
    "\n",
    "   @property\n",
    "   def bits(self):\n",
    "      return list(map(int, map(lambda x : log2(x + 1), self.spaces)))\n",
    "\n",
    "   @bits.setter\n",
    "   def bits(self, bits: int):\n",
    "      pass\n",
    "\n",
    "   def encode(self, values:List[int]):\n",
    "      spaces = self.spaces\n",
    "      if len(values) > len(spaces):\n",
    "         raise ValueError(\"Can't encode\")\n",
    "      for i in range(len(values)):\n",
    "         if values[i] >= spaces[i]:\n",
    "            raise ValueError(\"Won't fit\")\n",
    "      result = self.text\n",
    "      for i in range(len(values) - 1, -1, -1):\n",
    "         result = self.apply(i, values[i], result)\n",
    "      return result\n",
    "\n",
    "   @staticmethod\n",
    "   def decode(text:str,verbose=False,test_self=None) -> Tuple[str,List[int]]:\n",
    "      original = normalize(text)\n",
    "      if test_self is not None:\n",
    "         if original != test_self.text and verbose:\n",
    "            print(f'original=\\n{original}')\n",
    "            print(f'test_self.text=\\n{test_self.text}')\n",
    "         assert original == test_self.text\n",
    "      t = Typo(original)\n",
    "\n",
    "      return original, t._decode(text,test_self)\n",
    "   \n",
    "   def _decode(self, text:str,test=None) -> List[int]:\n",
    "      a_self = test if test is not None else self\n",
    "      spaces = a_self.spaces\n",
    "      cnt = len(diff(text,a_self.text))\n",
    "      if a_self.verbose:\n",
    "         print(f'cnt={cnt}')\n",
    "         print(f'diff(text,a_self.text)={diff(text,a_self.text)}')\n",
    "      values = [0 for s in spaces]\n",
    "      for index, space in enumerate(spaces):\n",
    "         isZero = True\n",
    "         for i in range(space):\n",
    "            values[index] = i\n",
    "            dif = diff(text, a_self.encode(values))\n",
    "            if len(dif) == cnt - 1:\n",
    "               if a_self.verbose:\n",
    "                  print(f'values={values}')\n",
    "                  print(f'dif={dif}')\n",
    "               cnt -= 1\n",
    "               isZero = False\n",
    "               break \n",
    "         if isZero:\n",
    "            values[index] = 0     \n",
    "            if a_self.verbose:\n",
    "               print(f'chunk is empty values={values}')\n",
    "      return  values\n",
    "   \n",
    "   def encode_encoder(self, bytes_str: str) -> Tuple[List[int], str]:\n",
    "      if not set(bytes_str) <= set('01'):\n",
    "         raise ValueError(f\"bytes_str isn't a bytes string : '{bytes_str}'\")\n",
    "      values = self.bits\n",
    "      bit_values = []\n",
    "      remaining_bits = bytes_str\n",
    "      for i, val in enumerate(values):\n",
    "         if len(remaining_bits) >= val + 1 and int(remaining_bits[:val+1]) < self.spaces[i]:\n",
    "            bit_value = int(remaining_bits[:val+1], 2)\n",
    "            bit_values.append(bit_value)\n",
    "            remaining_bits = remaining_bits[val+1:]\n",
    "         elif len(remaining_bits) >= val and val > 0:\n",
    "            bit_value = int(remaining_bits[:val], 2)\n",
    "            bit_values.append(bit_value)\n",
    "            remaining_bits = remaining_bits[val:]\n",
    "         else:\n",
    "            bit_values.append(0)\n",
    "      return bit_values, remaining_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = Typo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 73, 60, 14, 90, 49]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTypoInstance(t,verbose=False,testName='test'):\n",
    "    # patch\n",
    "   ORIGINAL_STDOUT = sys.stdout\n",
    "   sys.stdout = FileWriter(testName+'.txt')\n",
    "\n",
    "   if isinstance (t,str):\n",
    "     t = Typo(t,verbose=verbose)\n",
    "   spaces = t.spaces\n",
    "\n",
    "   print(f\"t.spaces = {spaces}\")\n",
    "   print(f\"t.bits = {t.bits}\")\n",
    "   print(f\"max={max(spaces)}\")\n",
    "   print(f\"len={len(spaces)}\")\n",
    "   \n",
    "   g = (list(map(lambda x: i % x ,spaces)) for i in range(max(spaces)))\n",
    "   cnt = 10\n",
    "   for v in g:\n",
    "     print(f'{v}')\n",
    "     encoded = t.encode(v)\n",
    "     print(f\"after encoding {v} {encoded}\")\n",
    "     org, x = Typo.decode(encoded,test_self=t)\n",
    "     print(f'original text candidate \"{org}\"')\n",
    "     if org == t.text and x == v:\n",
    "       print(('>'*100)+\" passed!\")\n",
    "     if not x == v:\n",
    "       print(f'\\nt.decode(t.encode(v)):{x}')\n",
    "       print(f't.text:{t.text}')\n",
    "     if cnt < 0:\n",
    "       break\n",
    "     cnt -=1\n",
    "   # restore\n",
    "   sys.stdout = ORIGINAL_STDOUT\n",
    "   return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'message': 'Possible spelling mistake found.', 'replacements': ['tied', 'timed', 'tired', 'tiled', 'toyed', 'tided'], 'offsetInContext': 5, 'context': 'I’ve tiyed with the idea of using GPT-3’s API to a...', 'offset': 5, 'errorLength': 5, 'category': 'TYPOS', 'ruleIssueType': 'misspelling', 'sentence': \"I've tiyed with the idea of using GPT-3's API to add much more intelligent capabilities to RC\"})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testTexts = [\n",
    "   '''Hey, How are you? Did you see the last John Cena movie?'''\n",
    ",'''Hi, How are you?'''\n",
    ",'However, you may as well just use a function statement instead; the only advantage that a lambda offers is that you can put a function definition for a simple expression inside a larger expression.'\n",
    ", '''However, you may as well just use a function statement instead; the only advantage that a lambda offers is that you can put a function definition for a simple expression inside a larger expression. But the above lambda is not part of a larger expression, it is only ever part of an assignment statement, binding it to a name. That's exactly what a statement would achieve.'''\n",
    ", '''I’ve toyed with the idea of using GPT-3’s API to add much more intelligent capabilities to RC, but I can’t deny that I’m drawn to the idea of running this kind of language model locally and in an environment I control. I’d like to someday increase the speed of RC’s speech synthesis and add a speech-to-text translation model in order to achieve real-time communication between humans and the chatbot. I anticipate that with this handful of improvements, RC will be considered a fully-fledged member of our server. Often, we feel that it already is.'''\n",
    "]\n",
    "# LAST_TESTED_TYPO = testTypoInstance(text,testName='mon1208',verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
