{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import chain\n",
    "from math import log2\n",
    "import re\n",
    "# Get the path of the script\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Generator, List, Optional, Tuple\n",
    "\n",
    "import Levenshtein\n",
    "from language_tool_python import LanguageTool\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "# Initialize the LanguageTool tool\n",
    "lang_tool = LanguageTool('en-US')\n",
    "spell_tool = SpellChecker()\n",
    "\n",
    "# Get the path of the script\n",
    "script_path = '.'\n",
    "# script_path = os.path.abspath(__file__)\n",
    "\n",
    "# ROOT_DIR = os.path.dirname(script_path)\n",
    "ROOT_DIR = '.'\n",
    "\n",
    "\n",
    "#@title parseRules\n",
    "def parseRules(name, ROOT_DIR=ROOT_DIR) -> Generator[Tuple[str, str], None, None]:\n",
    "   with open(ROOT_DIR + f'/{name}.tsv', 'r') as f:\n",
    "      for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "               continue\n",
    "            line = line.split('\\t')\n",
    "            if len(line) > 1:\n",
    "               yield (line[0], line[1])\n",
    "\n",
    "def compile_first(x:Tuple[str,str])->Tuple[re.Pattern[str],str]:\n",
    "   try:\n",
    "      return (re.compile(x[0]),x[1])\n",
    "   except:\n",
    "      print(x)\n",
    "      raise ValueError(f'compilable {x}')\n",
    "   \n",
    "WORD_CORRECTION_RULES = list(map(compile_first , chain(parseRules('anti.variant'), parseRules('anti.misspelling'))))\n",
    "FAT_CORRECTION_RULES = list(map(compile_first , parseRules('fat.keyboard')))\n",
    "KEYBOARD_CORRECTION_RULES = list(map(compile_first , parseRules('anti.keyboard')))\n",
    "WORD_RULES = list(map(compile_first ,  chain(parseRules('variant'), parseRules('grammatical'), parseRules('misspelling'))))\n",
    "KEYBOARD_RULES = list(map(compile_first,  parseRules('keyboard')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "class FileWriter:\n",
    "\tdef __init__(self, file_path):\n",
    "\t\tself.file = open(file_path, 'a')\n",
    "\n",
    "\tdef write(self, msg):\n",
    "\t\tself.file.write(msg)\n",
    "\t\tself.flush()\n",
    "\n",
    "\tdef flush(self):\n",
    "\t\tself.file.flush()\n",
    "\n",
    "def monkey_patch_print(file_path):\n",
    "\tsys.stdout = FileWriter(file_path)\n",
    "\n",
    "# monkey_patch_print('output.txt')\n",
    "\n",
    "print('Hello, world!') # This will be written to the file 'output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text: str,verbose=False) -> Tuple[List[str],Callable[[List[str],bool],str]]:\n",
    "   # words = word_tokenize(text)\n",
    "   words = list(spell_tool.split_words(text))\n",
    "   spans = []\n",
    "   first_word_hit = text.find(words[0])\n",
    "   spans.append((0,first_word_hit))\n",
    "   last_hit_end = first_word_hit + len(words[0])\n",
    "   for w in words[1:]:\n",
    "      hit_end = text.find(w,last_hit_end) \n",
    "      spans.append((last_hit_end,hit_end))\n",
    "      last_hit_end = hit_end + len(w)\n",
    "   spans.append((last_hit_end,len(text)))\n",
    "   non_words = list(map(lambda x: text[x[0]:x[1]], spans))\n",
    "\n",
    "   def detokenize(words:List[str],verbose=verbose) -> str:\n",
    "      ## combine words with non_words into a single string\n",
    "      tokens = [non_words[0]]\n",
    "      for i in range(len(words)):\n",
    "         tokens.append(words[i])\n",
    "         tokens.append(non_words[i+1])\n",
    "      if verbose:\n",
    "         print(f\"detokenize \\n'{text}' \\n-> \\n'{words}' + \\n'{non_words}' \\n-> \\n'{text}'\")\n",
    "      return ''.join(tokens)\n",
    "   \n",
    "   return  words ,detokenize\n",
    "def show_diff(a: str, b: str):\n",
    "   l_a,_ = get_words(a)\n",
    "   l_b,_ = get_words(b)\n",
    "   for i in range(min(len(l_a), len(l_b))):\n",
    "      if l_a[i] != l_b[i]:\n",
    "         print(f'i:{i} a:\"{l_a[i]}\" b:\"{l_b[i]}\"')\n",
    "def diff(a: str, b: str):\n",
    "   l_a,_ = get_words(a)\n",
    "   l_b,_ = get_words(b)\n",
    "   acc = []\n",
    "   for i in range(min(len(l_a), len(l_b))):\n",
    "      if l_a[i] != l_b[i]:\n",
    "         acc.append((l_a[i], l_b[i]))\n",
    "   return acc\n",
    "def apply_match(text:str, match_result: Tuple[Tuple[int,int],str,re.Pattern], verbose: bool = False) -> str:\n",
    "   span, repl, regex = match_result\n",
    "   if verbose:\n",
    "      print(f\"Before replace: {text}\")\n",
    "   replaced_text = regex.sub(repl, text[span[0]:span[1]])\n",
    "   if verbose:\n",
    "      print(f\"After replace: {replaced_text}\")\n",
    "   return text[:span[0]] + replaced_text + text[span[1]:]\n",
    "def keyboard_rules_scan(text: str)->List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   matches = []\n",
    "   rules = KEYBOARD_RULES\n",
    "   for regex, repl in rules:\n",
    "      for x in regex.finditer(text):\n",
    "         matches.append((x.span(), repl, regex))\n",
    "   return matches\n",
    "def word_rules_scan(text: str)->List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   matches = []\n",
    "   for regex, repl in WORD_RULES:\n",
    "      x = regex.match(text)\n",
    "      if x is not None:\n",
    "         start, end = x.span()\n",
    "         matches.append(((start, end), repl, regex))\n",
    "   return matches\n",
    "def rules_scan(text: str)-> List[Tuple[Tuple[int, int], str, re.Pattern]]:\n",
    "   result = word_rules_scan(text) + keyboard_rules_scan(text)\n",
    "   result.sort()\n",
    "   return result\n",
    "def valid_matches(text:str, slots:List[Tuple[Tuple[int, int], str, re.Pattern]], verbose=False):\n",
    "   text_words, _ = get_words(text)\n",
    "   mutations: List[str] = list(map(lambda x: '', slots))\n",
    "\n",
    "   # Apply the match to the text and get the resulting strings\n",
    "   for match_index, match_result in enumerate(slots):\n",
    "      new_string = apply_match_and_validate(text, match_result, mutations, match_index, text_words, verbose)\n",
    "      norm = normalize(new_string,verbose)\n",
    "      if norm != new_string:\n",
    "         mutations[match_index] = new_string\n",
    "      else:\n",
    "         print(f'rule undetectable \"{norm}\" != \"{text}\"')\n",
    "\n",
    "   # Print the list of matches and their mutations if verbose output is enabled\n",
    "   if verbose:\n",
    "      print(list(zip(slots, mutations)))\n",
    "\n",
    "   # Check for ambiguous and invalid matches\n",
    "   ambiguous_invalid_matches = find_ambiguous_or_invalid_matches(mutations)\n",
    "\n",
    "   # Create a list of valid matches\n",
    "   valid_slots = [elem for i, elem in enumerate(slots) if i not in ambiguous_invalid_matches]\n",
    "\n",
    "   return valid_slots\n",
    "def apply_match_and_validate(text: str, match_result: Tuple[Tuple[int, int], str, re.Pattern], mutations: List[str], match_index: int, text_words: List[str], verbose: bool) -> str:\n",
    "   new_string = apply_match(text, match_result, verbose)\n",
    "   if verbose:\n",
    "      print(f\"MatchValidator: validating match {text} -> {new_string}\")\n",
    "\n",
    "   # Check if the resulting string has the same number of words as the original text\n",
    "   words, _ = get_words(new_string)\n",
    "   if len(words) != len(text_words):\n",
    "      if verbose:\n",
    "            print(f\"MatchValidator: length of words in transformed string ({len(words)}) does not match original string ({len(text_words)})\")\n",
    "      # Set the mutation to an empty string if it is not valid\n",
    "      mutations[match_index] = ''\n",
    "   else:\n",
    "      # Check if the first and last character of each modified word is the same as the original word\n",
    "      for ow, nw in zip(text_words, words):\n",
    "            ow = ow.lower()\n",
    "            nw = nw.lower()\n",
    "            if ow[0] != nw[0] or ow[-1] != nw[-1]:\n",
    "               if verbose:\n",
    "                  print(f\"MatchValidator: first and/or last character of word in transformed string ({nw}) does not match original string ({ow})\")\n",
    "               # Set the mutation to an empty string if it is not valid\n",
    "               mutations[match_index] = ''\n",
    "               break\n",
    "\n",
    "   return new_string\n",
    "def find_ambiguous_or_invalid_matches(mutations: List[str]) -> List[int]:\n",
    "    return [i for i, new_string in enumerate(mutations, start=1)\n",
    "            if not new_string or new_string in mutations[:i-1]]\n",
    "def valid_rules_scan(text:str,verbose=False):\n",
    "   proposed_slots = rules_scan(text)\n",
    "   if verbose:\n",
    "      print('proposed_slots: ',proposed_slots)\n",
    "   valid_slots = valid_matches(text,proposed_slots,verbose=verbose)\n",
    "   if verbose:\n",
    "      print('valid_slots: ',valid_slots)\n",
    "   return valid_slots\n",
    "def chunker(text:str,span_size = 6):\n",
    "   words,_ = get_words(text)\n",
    "   chunks = []\n",
    "   cur_word = 0\n",
    "   last_sep = 0\n",
    "   i = 0\n",
    "   while i < len(text) and cur_word < len(words):\n",
    "      if text[i:i + len(words[cur_word])] == words[cur_word]:\n",
    "            if cur_word % span_size == 0 and cur_word != 0:\n",
    "               chunks.append((last_sep,i))\n",
    "               last_sep = i\n",
    "            i = i + len(words[cur_word]) - 1\n",
    "            cur_word+=1\n",
    "         \n",
    "      i += 1\n",
    "   if last_sep < len(text):\n",
    "      chunks.append((last_sep, len(text)))\n",
    "   return chunks\n",
    "def word_we_misspelled(word:str,spelling:str,verbose=False):\n",
    "   if string_mutation_distance(spelling,word) == 1 \\\n",
    "      and spelling[0].lower() == word[0].lower() \\\n",
    "      and spelling[-1].lower() == word[-1].lower():\n",
    "\n",
    "      for regex,repl in FAT_CORRECTION_RULES:\n",
    "         if regex.sub(repl,word) != spelling:\n",
    "            if verbose:\n",
    "               print(f\"FAT_CORRECTION_RULES ({regex}) ({repl}): {regex.sub(repl,word)} == {spelling}\")\n",
    "            return True\n",
    "      return False\n",
    "   else:\n",
    "      return False # speller is wrong since input is ai generated and the only source for bad spelling is us and it's probably a name of sth\n",
    "def spell_word(word:str,verbose=False) -> str:\n",
    "   spellingOpt = spell_tool.correction(word)\n",
    "   spelling = spellingOpt if spellingOpt is not None else word\n",
    "   return spelling if word_we_misspelled(word,spelling,verbose) else word \n",
    "def normalize(original_text:str,verbose=False):\n",
    "   text = original_text\n",
    "   temp = text\n",
    "   for regex,repl in WORD_CORRECTION_RULES:\n",
    "      temp =  regex.sub(repl, text)\n",
    "      if verbose and text != temp:\n",
    "         print(f'WORD_CORRECTION_RULES rule:{(regex,repl)} \"{text}\" -> \"{temp}\"')\n",
    "      text = temp \n",
    "\n",
    "   for regex,repl in KEYBOARD_CORRECTION_RULES:\n",
    "      temp =  regex.sub(repl, text)\n",
    "      if verbose and text != temp:\n",
    "         print(f'KEYBOARD_CORRECTION_RULES rule:{(regex,repl)} \"{text}\" -> \"{temp}\"')\n",
    "      text = temp \n",
    "\n",
    "   dot_split_sentences = text.split('.')\n",
    "   temp = '.'.join([lang_tool.correct(sentence) for sentence in dot_split_sentences])\n",
    "   if verbose and text != temp:\n",
    "      print(f'lang_tool.correct \\n\"{text}\" \\n-> \\n\"{temp}\"')\n",
    "   text = temp \n",
    "\n",
    "   words, get_sentence  = get_words(text)\n",
    "   spelled_words = [spell_word(w,verbose) for w in words]\n",
    "   if verbose:\n",
    "      for i in range(len(words)):\n",
    "         if words[i] != spelled_words[i]:\n",
    "            print(f\"normalize speller: '{words[i]}' -> '{spelled_words[i]}'\")\n",
    "\n",
    "   spelled_text = get_sentence(spelled_words,verbose)\n",
    "\n",
    "   if verbose:\n",
    "      print(f\"normalize('{original_text}') => '{spelled_text}'\")\n",
    "      print(('='*10)+'diff'+('='*10))\n",
    "      show_diff(original_text,spelled_text)\n",
    "      print('='*20)\n",
    "   return spelled_text\n",
    "def string_mutation_distance(str1: str, str2: str) -> int:\n",
    "   \"\"\"Returns the number of mutations required to transform str1 into str2\"\"\"\n",
    "   return Levenshtein.distance(str1, str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Typo:\n",
    "   \"\"\"Class for Typo Engine.\"\"\"\n",
    "\n",
    "   text: str = field(repr=False)\n",
    "   _length: int = field(init=False, repr=False)\n",
    "   _slots: Optional[List[Tuple[Tuple[int, int], str, re.Pattern]]] = field(init=True, repr=False,default=None)\n",
    "   _spaces: Optional[List[int]] = field(init=True, repr=False,default=None)\n",
    "   verbose: bool = field(init=True,repr=False,default=False)\n",
    "\n",
    "   def __post_init__(self):\n",
    "      if self.text != normalize(self.text,self.verbose):\n",
    "         raise ValueError(\"Text isn't spelled correctly\")\n",
    "\n",
    "   def apply(self, space: int, offset: int, text: str) -> str:\n",
    "      if self.verbose:\n",
    "         print(f\"apply: space={space}, offset={offset}, text={text}\")\n",
    "      if offset == 0:\n",
    "         return text\n",
    "      match_tuple = self.slots[sum(self.spaces[0:space]) + offset - 1]\n",
    "      applied = apply_match(text, match_tuple,self.verbose)\n",
    "      if self.verbose:\n",
    "         print(f\"applied: {applied}\")\n",
    "      return applied\n",
    "   \n",
    "   @property\n",
    "   def slots(self):\n",
    "      if self._slots is None:\n",
    "         self._slots = valid_rules_scan(self.text,self.verbose)\n",
    "      return self._slots\n",
    "\n",
    "   @property\n",
    "   def length(self) -> int:\n",
    "      return len(self.slots)\n",
    "\n",
    "   @length.setter\n",
    "   def length(self, length: int):\n",
    "      pass\n",
    "\n",
    "   @property\n",
    "   def spaces(self) -> List[int]:\n",
    "      if self._spaces is not None:\n",
    "         return self._spaces\n",
    "      \n",
    "      sentence_ranges = chunker(self.text)\n",
    "      \n",
    "      # Initialize an empty list of buckets\n",
    "      num_buckets = len(sentence_ranges)\n",
    "      buckets: List[int] = [0 for _ in range(num_buckets)]\n",
    "      \n",
    "      # Iterate through each element range and put it in the corresponding bucket\n",
    "      for i, (start, end) in enumerate(span for span,_,_ in self.slots):\n",
    "         for j, (sent_start, sent_end) in enumerate(sentence_ranges):\n",
    "               if sent_start <= start < sent_end and sent_start < end <= sent_end:\n",
    "                  buckets[j] += 1\n",
    "                  break\n",
    "                  \n",
    "      return buckets\n",
    "\n",
    "   @spaces.setter\n",
    "   def spaces(self, value):\n",
    "      pass\n",
    "\n",
    "   @property\n",
    "   def bits(self):\n",
    "      return list(map(int, map(lambda x : log2(x + 1), self.spaces)))\n",
    "\n",
    "   @bits.setter\n",
    "   def bits(self, bits: int):\n",
    "      pass\n",
    "\n",
    "   def encode(self, values:List[int]):\n",
    "      spaces = self.spaces\n",
    "      if len(values) > len(spaces):\n",
    "         raise ValueError(\"Can't encode\")\n",
    "      for i in range(len(values)):\n",
    "         if values[i] >= spaces[i]:\n",
    "            raise ValueError(\"Won't fit\")\n",
    "      result = self.text\n",
    "      for i in range(len(values) - 1, -1, -1):\n",
    "         result = self.apply(i, values[i], result)\n",
    "      return result\n",
    "\n",
    "\n",
    "   def decode(self,text:str):\n",
    "      spaces = self.spaces\n",
    "      differences = len(diff(text, self.text))\n",
    "      if differences > len(spaces):\n",
    "         raise ValueError(\"Can't encode\")\n",
    "      values = list(map(lambda x: 0, spaces))\n",
    "      for i in range(len(spaces)):\n",
    "         for j in range(spaces[i] + 1):\n",
    "            if len(diff(text, self.apply(i, j, self.text))) < differences:\n",
    "               values[i] = j\n",
    "               break\n",
    "      return values\n",
    "\n",
    "   def encode_encoder(self, bytes_str: str) -> Tuple[List[int], str]:\n",
    "      if not set(bytes_str) <= set('01'):\n",
    "         raise ValueError(f\"bytes_str isn't a bytes string : '{bytes_str}'\")\n",
    "      values = self.bits\n",
    "      bit_values = []\n",
    "      remaining_bits = bytes_str\n",
    "      for val in values:\n",
    "         if len(remaining_bits) >= val and len(remaining_bits[:val]) > 0:\n",
    "            bit_value = int(remaining_bits[:val], 2)\n",
    "            bit_values.append(bit_value)\n",
    "            remaining_bits = remaining_bits[val:]\n",
    "         else:\n",
    "            bit_values.append(0)\n",
    "      return bit_values, remaining_bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTypoCarrier(text,verbose=False):\n",
    "   t = Typo(text,verbose=verbose)\n",
    "\n",
    "   spaces = t.spaces\n",
    "\n",
    "   print(f\"t.spaces = {spaces}\")\n",
    "   print(f\"t.bits = {t.bits}\")\n",
    "   print(f\"max={max(spaces)}\")\n",
    "   print(f\"len={len(spaces)}\")\n",
    "   \n",
    "   g = (list(map(lambda x: i % x ,spaces)) for i in range(max(spaces))) \n",
    "\n",
    "   for v in g:\n",
    "      print(f'{v}',end='')\n",
    "      x =t.decode(t.encode(v))\n",
    "      if not x == v:\n",
    "         print(f'\\nt.decode(t.encode(v)):{x}')\n",
    "         print(f't.text:{v}')\n",
    "      else:\n",
    "         print(\" passed!\")\n",
    "   return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule undetectable \"Hey, How ate you? Did you see the last John Cena movie?\" != \"Hey, How are you? Did you see the last John Cena movie?\"\n",
      "rule undetectable \"Hey, How are you? Dud you see the last John Cena movie?\" != \"Hey, How are you? Did you see the last John Cena movie?\"\n",
      "t.spaces = [24, 47]\n",
      "t.bits = [4, 5]\n",
      "max=47\n",
      "len=2\n",
      "[0, 0] passed!\n",
      "[1, 1] passed!\n",
      "[2, 2] passed!\n",
      "[3, 3] passed!\n",
      "[4, 4] passed!\n",
      "[5, 5] passed!\n",
      "[6, 6] passed!\n",
      "[7, 7] passed!\n",
      "[8, 8] passed!\n",
      "[9, 9] passed!\n",
      "[10, 10] passed!\n",
      "[11, 11] passed!\n",
      "[12, 12] passed!\n",
      "[13, 13] passed!\n",
      "[14, 14] passed!\n",
      "[15, 15] passed!\n",
      "[16, 16] passed!\n",
      "[17, 17] passed!\n",
      "[18, 18] passed!\n",
      "[19, 19] passed!\n",
      "[20, 20] passed!\n",
      "[21, 21] passed!\n",
      "[22, 22] passed!\n",
      "[23, 23] passed!\n",
      "[0, 24] passed!\n",
      "[1, 25] passed!\n",
      "[2, 26] passed!\n",
      "[3, 27] passed!\n",
      "[4, 28] passed!\n",
      "[5, 29] passed!\n",
      "[6, 30] passed!\n",
      "[7, 31] passed!\n",
      "[8, 32] passed!\n",
      "[9, 33] passed!\n",
      "[10, 34] passed!\n",
      "[11, 35] passed!\n",
      "[12, 36] passed!\n",
      "[13, 37] passed!\n",
      "[14, 38] passed!\n",
      "[15, 39] passed!\n",
      "[16, 40] passed!\n",
      "[17, 41] passed!\n",
      "[18, 42] passed!\n",
      "[19, 43] passed!\n",
      "[20, 44] passed!\n",
      "[21, 45] passed!\n",
      "[22, 46] passed!\n"
     ]
    }
   ],
   "source": [
    "# text = '''Hi, How are you?'''\n",
    "text = '''Hey, How are you? Did you see the last John Cena movie?'''\n",
    "# speller That's -> Thats\n",
    "\n",
    "# text = '''However, you may as well just use a function statement instead; the only advantage that a lambda offers is that you can put a function definition for a simple expression inside a larger expression. But the above lambda is not part of a larger expression, it is only ever part of an assignment statement, binding it to a name. That's exactly what a statement would achieve.'''\n",
    "# text = '''I’ve toyed with the idea of using GPT-3’s API to add much more intelligent capabilities to RC, but I can’t deny that I’m drawn to the idea of running this kind of language model locally and in an environment I control. I’d like to someday increase the speed of RC’s speech synthesis and add a speech-to-text translation model in order to achieve real-time communication between humans and the chatbot. I anticipate that with this handful of improvements, RC will be considered a fully-fledged member of our server. Often, we feel that it already is.'''\n",
    "LAST_TESTED_TYPO = testTypoCarrier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = LAST_TESTED_TYPO\n",
    "\n",
    "spaces = t.spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import struct\n",
    "\n",
    "# # example binary string\n",
    "# orig_bin_str = '1001'*8\n",
    "# bin_str = orig_bin_str\n",
    "\n",
    "# # convert the binary string to bytes\n",
    "# byte_str = int(bin_str, 2).to_bytes((len(bin_str) + 7) // 8, byteorder='big')\n",
    "\n",
    "# # write the bytes to a binary file\n",
    "# with open('example.bin', 'wb') as f:\n",
    "#     f.write(byte_str)\n",
    "\n",
    "# # read the bytes back from the binary file\n",
    "# with open('example.bin', 'rb') as f:\n",
    "#     byte_str = f.read()\n",
    "\n",
    "# # convert the bytes back to a binary string\n",
    "# bin_str = ''.join(format(byte, '08b') for byte in byte_str)\n",
    "\n",
    "# print(bin_str)\n",
    "# assert orig_bin_str == bin_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
