{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from typing import List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "\n",
    "def create_mask(a: List[Tuple[int, int]], b: List[Tuple[int, int]]) -> List[bool]:\n",
    "    \"\"\"\n",
    "    Creates a mask over `a` such that the `a[i]` range is contained in any `b[j]` range.\n",
    "    Only keeps the last `True` if a consecutive `True` occurs.\n",
    "    \"\"\"\n",
    "    mask = [False]\n",
    "    j = 0\n",
    "    for i in range(len(a)):\n",
    "        while j < len(b) and b[j][1] < a[i][0]:\n",
    "            j += 1\n",
    "        mask.append(j < len(b) and b[j][0] <= a[i][0] and b[j][1] >= a[i][1])\n",
    "        \n",
    "    for i in range(1, len(mask)):\n",
    "        if mask[i] and mask[i-1]:\n",
    "            mask[i-1] = False\n",
    "    mask.append(False)\n",
    "    return mask\n",
    "\n",
    "@dataclass\n",
    "class SemanticPositions:\n",
    "    string : str = field(init=True,repr=True)\n",
    "    NVA_words : List[Tuple[int,int]] = field(init=True,repr=True)\n",
    "    tokens : Optional[Tensor] = field(init=True,repr=True)\n",
    "    tokensStrings :Optional[List[str]] = field(init=True,repr=True)\n",
    "    mask : Optional[List[bool]] = field(init=True,repr=True)\n",
    "\n",
    "def Mask(text:str,tokenizer=None):\n",
    "  \"\"\"Extract the start and end positions of verbs, nouns, and adjectives in the given text.\"\"\"\n",
    "  # Load the spaCy English model\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  \n",
    "  # Parse the text with spaCy\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Create a dictionary to store the start and end positions of each POS tag\n",
    "  # pos_dict = {\n",
    "  #   'VERB': [],\n",
    "  #   'NOUN': [],\n",
    "  #   'ADJ': []\n",
    "  # }\n",
    "  pos_list: List[Tuple[int,int]] = []\n",
    "  \n",
    "  # Loop through each token in the parsed text\n",
    "  for token in doc:\n",
    "    if token.pos_ in ['VERB','NOUN','ADJ']:\n",
    "      # If the token's POS tag is a verb, noun, or adjective, add its start and end positions to the dictionary\n",
    "      # pos_dict[token.pos_]\n",
    "      pos_list.append((token.idx, token.idx + len(token)))\n",
    "\n",
    "  pos_list.sort()\n",
    "\n",
    "  if tokenizer is not None:\n",
    "    # tokenize the input string\n",
    "    tokens : List[str] = [t[2:] if t[0:2] == \"##\" else t for t in tokenizer.tokenize(text)]\n",
    "\n",
    "    start_index = 0\n",
    "    token_mask = [False for _ in tokens]\n",
    "    token_spans = []\n",
    "    last_span = 0\n",
    "    for token in tokens:\n",
    "        token_start_index = text.find(token, start_index)\n",
    "        token_end_index = token_start_index + len(token)\n",
    "        span = (token_start_index, token_end_index)\n",
    "        token_spans.append(span)\n",
    "        start_index = token_end_index\n",
    "    \n",
    "    m = create_mask(token_spans,pos_list)\n",
    "    return SemanticPositions(string=text,\n",
    "                             NVA_words=pos_list,\n",
    "                             tokensStrings=tokens,\n",
    "                             tokens=tokenizer(text, return_tensors='pt'),\n",
    "                             mask=m)\n",
    "\n",
    "  return SemanticPositions(string=text,\n",
    "                             NVA_words=pos_list,\n",
    "                             tokens=None,\n",
    "                             tokensStrings=None,\n",
    "                             mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticPositions(string='tokenization is the art of making my life miserable!', NVA_words=[(0, 12), (20, 23), (27, 33), (37, 41), (42, 51)], tokens={'input_ids': tensor([[  101, 22559,  2734,  1110,  1103,  1893,  1104,  1543,  1139,  1297,\n",
       "         14531,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, tokensStrings=['token', 'ization', 'is', 'the', 'art', 'of', 'making', 'my', 'life', 'miserable', '!'], mask=[False, False, True, False, False, True, False, True, False, False, True, False, False])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = Mask(\"tokenization is the art of making my life miserable!\",BertTokenizer.from_pretrained('bert-base-cased'))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization is the art of making my life miserable!\n",
      "[(0, 12), (20, 23), (27, 33), (37, 41), (42, 51)]\n",
      "tensor([[  101, 22559,  2734,  1110,  1103,  1893,  1104,  1543,  1139,  1297,\n",
      "         14531,   106,   102]])\n",
      "['token', 'ization', 'is', 'the', 'art', 'of', 'making', 'my', 'life', 'miserable', '!']\n",
      "[False, False, True, False, False, True, False, True, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "print(result.string)\n",
    "print(result.NVA_words)\n",
    "print(result.tokens['input_ids'])\n",
    "print(result.tokensStrings)\n",
    "print(result.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([101, 22559,  2734,  1110,  1103,  1893,  1104,  1543,  1139,  1297,\n",
    "         14531,   106,   102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result.mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
