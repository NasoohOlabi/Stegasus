{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "LONG_TEXT = \"\"\"Text literals and metacharacters make up this string. The compile function is used to create the pattern.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
        "if parent_dir not in sys.path:\n",
        "    sys.path.insert(0, parent_dir)\n",
        "from StringSpans import StringSpans\n",
        "from SemanticMasking import MaskGen, SemanticPositions\n",
        "import itertools\n",
        "from icecream import ic\n",
        "import csv\n",
        "import itertools\n",
        "import re\n",
        "import urllib.request\n",
        "from math import floor, log2\n",
        "from typing import Any, Generator, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from icecream import ic\n",
        "from scipy.special import softmax  # type: ignore\n",
        "from transformers import AutoModelForSequenceClassification  # type: ignore\n",
        "from transformers import AutoTokenizer  # type: ignore\n",
        "from transformers import TFAutoModelForSequenceClassification  # type: ignore\n",
        "\n",
        "from SemanticMasking import MaskGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title random_bit_stream\n",
        "import random\n",
        "\n",
        "def random_bit_stream(length=None):\n",
        "    \"\"\"Return a random string of zeros and ones of the given length (default: random integer between 0 and 100).\"\"\"\n",
        "    if length is None:\n",
        "        length = random.randint(0, 100)\n",
        "    return ''.join(str(random.randint(0, 1)) for _ in range(length))\n",
        "def int_to_binary_string(n: int, length: int):\n",
        "    binary_str = bin(n)[2:]  # convert to binary string, remove '0b' prefix\n",
        "    padded_str = binary_str.rjust(length, '0')  # pad with zeros to length\n",
        "    return padded_str"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NN Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = ['❤', '😍', '📷', '🇺🇸', '☀', '💜', '😉', '💯', '😁', '🎄', '📸', '😜', '😂', '☹️', '😭', '😔', '😡', '💢', '😤', '😳', '🙃', '😩', '😠', '💕', '🙈', '🙄', '🔥', '😊', '😎', '✨', '💙', '😘']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "augment = {'❤': ['💓', '💖', '💗', '💘', '💞', '💟'],\n",
        " '😍': ['😻', '🥰','🤩'],\n",
        " '📷': ['🎥', '📹', '🎞️', '📽️'],\n",
        " '☀': ['🌞', '🌅', '🌄', '🌤️', '🌻', '🌼'],\n",
        " '💜': ['❤️', '🤎', '🖤', '🤍'],\n",
        " '😉': ['😏', '😋', '😼', '😌', '😬'],\n",
        " '💯': ['👌'],\n",
        " '😁': ['😀', '😃', '😆', '😄', '😅', '😸'],\n",
        " '🎄': ['🎅', '🤶', '🎁', '🌟', '🌲'],\n",
        " '📸': [],\n",
        " '😜': ['😝', '😛'],\n",
        " '😂': ['🤣', '😹'],\n",
        " '☹️': ['🙁', '😞', '😖'],\n",
        " '😭': ['😢', '😥', '😪', '😓'],\n",
        " '😔': ['😟', '😕'],\n",
        " '😡': ['😣', '👿'],\n",
        " '💢': ['💥', '💨', '💣', '💫'],\n",
        " '😤': ['😒'],\n",
        " '😳': ['😮', '😯', '😲', '🙀', '😱'],\n",
        " '🙃': [],\n",
        " '😩': ['😫'],\n",
        " '😠': ['😾'],\n",
        " '💕': ['💔'],\n",
        " '🙈': ['🙉', '🙊', '🐵', '🐒', '🐾'],\n",
        " '🙄': ['😑', '🤨','😐'],\n",
        " '🔥': ['🌋', '🚒'],\n",
        " '😊': ['🙂'],\n",
        " '😎': ['🕶️', '🍻'],\n",
        " '✨': ['🔮', '🎉'],\n",
        " '💙': ['💚', '💛', '🧡'],\n",
        " '😘': ['😗', '😚', '😙']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_labels = list(augment.keys()) + [e for l in augment.values() for e in l]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['❤', '😍', '📷', '☀', '💜', '😉', '💯', '😁', '🎄', '📸', '😜', '😂', '☹️', '😭', '😔', '😡', '💢', '😤', '😳', '🙃', '😩', '😠', '💕', '🙈', '🙄', '🔥', '😊', '😎', '✨', '💙', '😘', '💓', '💖', '💗', '💘', '💞', '💟', '😻', '🥰', '🤩', '🎥', '📹', '🎞️', '📽️', '🌞', '🌅', '🌄', '🌤️', '🌻', '🌼', '❤️', '🤎', '🖤', '🤍', '😏', '😋', '😼', '😌', '😬', '👌', '😀', '😃', '😆', '😄', '😅', '😸', '🎅', '🤶', '🎁', '🌟', '🌲', '😝', '😛', '🤣', '😹', '🙁', '😞', '😖', '😢', '😥', '😪', '😓', '😟', '😕', '😣', '👿', '💥', '💨', '💣', '💫', '😒', '😮', '😯', '😲', '🙀', '😱', '😫', '😾', '💔', '🙉', '🙊', '🐵', '🐒', '🐾', '😑', '🤨', '😐', '🌋', '🚒', '🙂', '🕶️', '🍻', '🔮', '🎉', '💚', '💛', '🧡', '😗', '😚', '😙']\n"
          ]
        }
      ],
      "source": [
        "print(augmented_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pre_texts(string:str)->Generator[str, Any, None]:\n",
        "  spans = [x.span() for x in re.finditer(r'(\\s)+', string)]\n",
        "  for span in spans:\n",
        "    yield string[0:span[0]]\n",
        "  if spans[-1][1] != len(string):\n",
        "    yield string\n",
        "def gaussian_order(lst):\n",
        "    length = len(lst)\n",
        "    max_odd_ind = length - 1 if length % 2 == 0 else length - 2\n",
        "    max_even_ind = length - 1 if length % 2 != 0 else length - 2\n",
        "    dist = itertools.chain(range(max_odd_ind, 0, -2), range(0, max_even_ind + 1, 2))\n",
        "    return [lst[i] for i in dist]\n",
        "models_to_choose = [\n",
        "    \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\",\n",
        "    \"AlekseyDorkin/xlm-roberta-en-ru-emoji\"\n",
        "]\n",
        "BASE_MODEL = models_to_choose[0]\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL)\n",
        "    return model, tokenizer\n",
        "MODEL, TOKENIZER = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Emojier:\n",
        "  BASE_MODEL = \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\"\n",
        "  model: Any = MODEL\n",
        "  tokenizer: Any = TOKENIZER\n",
        "  multiplicity = 3\n",
        "  TopFPercent = 0.1\n",
        "  verbose = False\n",
        "  @staticmethod\n",
        "  def predict( text: str):\n",
        "    inputs = Emojier.tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = Emojier.model(**inputs)\n",
        "    logits = outputs.logits.detach().numpy()[0]\n",
        "    predicted_class = logits.argmax()\n",
        "    return predicted_class\n",
        "    \n",
        "  @staticmethod\n",
        "  def preprocess(text:str):\n",
        "      new_text = []\n",
        "      for t in text.split(\" \"):\n",
        "          t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "          t = 'http' if t.startswith('http') else t\n",
        "          new_text.append(t)\n",
        "      return \" \".join(new_text)\n",
        "  @staticmethod\n",
        "  def _augment(emoticons:List[str]) -> List[str]:\n",
        "    augmented = []\n",
        "    for e in emoticons:\n",
        "      augmented.append(e)\n",
        "      for x in augment[e]:\n",
        "        augmented.append(x)\n",
        "    Emojier.log(f\"_augment({emoticons}) = {augmented}\")\n",
        "    return augmented\n",
        "  @staticmethod\n",
        "  def _predict(text:str) -> List[str]:\n",
        "    # Preprocess text (username and link placeholders)\n",
        "    preprocessed = Emojier.preprocess(text)\n",
        "    inputs = Emojier.tokenizer(preprocessed, return_tensors=\"pt\")\n",
        "    preds = Emojier.model(**inputs).logits\n",
        "    scores = torch.nn.functional.softmax(preds, dim=-1).detach().numpy()\n",
        "    sorted_scores = [float(value) for value in np.sort(scores.squeeze())[::-1]]\n",
        "    ranking = np.argsort(scores)\n",
        "    ranking = ranking.squeeze()[::-1]\n",
        "    emojis = [Emojier.model.config.id2label[i] for i in ranking]\n",
        "    emoticons = [emo for emo, score in zip(emojis, sorted_scores) if emo != '🇺🇸' and score > Emojier.TopFPercent]\n",
        "    return Emojier.addMultiplicities(\n",
        "        Emojier._augment(\n",
        "          emoticons\n",
        "        )\n",
        "      )\n",
        "  @staticmethod\n",
        "  def addMultiplicities(emoticons: List[str]):\n",
        "    new_emoticons = []\n",
        "    for emo in emoticons:\n",
        "      for i in range(1,Emojier.multiplicity+1):\n",
        "        new_emoticons.append(emo * i)\n",
        "    return new_emoticons\n",
        "  @staticmethod\n",
        "  def encode(text:str,bytes_str:str):\n",
        "    Emojier.info(f\"encode({text}, {bytes_str})\")\n",
        "    mask = MaskGen(text)\n",
        "    encoded_so_far = ''\n",
        "    ss = StringSpans(text)\n",
        "    ticks = [(text[:v],(u,v)) for u,v in mask.NVA_words if (u,v) in ss.words]\n",
        "    original_length = len(text)\n",
        "    curr_offset = lambda : (len(text) - original_length)\n",
        "    new_ending = lambda x : curr_offset() + len(x)\n",
        "    for pre_text, (u,v) in ticks:\n",
        "      u, v = (u + curr_offset(), v+ curr_offset())\n",
        "      breakPoint = new_ending(pre_text)\n",
        "      pre_text = text[:breakPoint]\n",
        "      Emojier.log('E>'+'-'*20 + 'tick' + '-'*20 + pre_text)\n",
        "      emoji_options = gaussian_order(Emojier._predict(text[:breakPoint]))\n",
        "      if len(emoji_options) < 2:\n",
        "        Emojier.log('E>'+f'word={text[u:v]},range={(0,breakPoint)},not enough options={emoji_options}')\n",
        "        continue\n",
        "      if bytes_str[0] == \"0\":\n",
        "        Emojier.log('E>'+f'word={text[u:v]},range={(0,breakPoint)},zero start={bytes_str[:5]}')\n",
        "        encoded_so_far += bytes_str[0]\n",
        "        bytes_str = bytes_str[1:]\n",
        "        continue\n",
        "      encoded_so_far += bytes_str[0]\n",
        "      bytes_str = bytes_str[1:] # discard the one\n",
        "      bits = floor(log2(len(emoji_options)))\n",
        "      taken_bits = bytes_str[:bits]\n",
        "      ind = int(taken_bits, 2)\n",
        "      encoded_so_far += bytes_str[:bits]\n",
        "      bytes_str = bytes_str[bits:]\n",
        "      emoji = emoji_options[ind]\n",
        "      Emojier.log('E>'+f\"word={text[u:v]},range={(0,breakPoint)},len({emoji})={len(emoji)},{len(emoji_options)}=len({emoji_options})\")\n",
        "      Emojier.log('E>'+f'encoded_so_far={encoded_so_far}')\n",
        "      if len(emoji) > 0:\n",
        "        text = f'{text[0:breakPoint]} {emoji}{text[breakPoint:]}'\n",
        "    Emojier.info(f\"encoded {encoded_so_far} as: {text}\")\n",
        "    return text, bytes_str\n",
        "  @staticmethod\n",
        "  def int_to_binary_string(n: int, length: int) -> str:\n",
        "    binary_str = bin(n)[2:]  # convert to binary string, remove '0b' prefix\n",
        "    padded_str = binary_str.rjust(length, '0')  # pad with zeros to length\n",
        "    return padded_str\n",
        "  @staticmethod\n",
        "  def cntPrefix(string:str, prefix:str):\n",
        "    for i in range(4,0,-1):\n",
        "    #   Emojier.log(f\"string={string[:len(prefix*i)]},prefix*i={prefix*i},string.startswith(prefix * i)={string.startswith(prefix * i)}\",end='|')\n",
        "      if string.startswith(prefix * i):\n",
        "        # Emojier.log('')\n",
        "        return i\n",
        "    # Emojier.log('')\n",
        "    return 0\n",
        "  @staticmethod\n",
        "  def log(string:str):\n",
        "    if Emojier.verbose:\n",
        "      print(string)\n",
        "    with open('Emojier.log','a', encoding='utf-8') as f:\n",
        "      f.write(string+'\\n') \n",
        "  @staticmethod\n",
        "  def info(string:str):\n",
        "    if Emojier.verbose:\n",
        "      print(string)\n",
        "    with open('Emojier.info','a', encoding='utf-8') as f:\n",
        "      f.write(string+'\\n') \n",
        "  @staticmethod\n",
        "  def strip(text:str):\n",
        "    for label in augmented_labels:\n",
        "      text = text.replace(' '+label,'')\n",
        "    for label in augmented_labels:\n",
        "      text = text.replace(label,'')\n",
        "    return text\n",
        "  @staticmethod\n",
        "  def decode(encoded_text:str):\n",
        "    text = encoded_text\n",
        "    text = Emojier.strip(text)\n",
        "    clear_text = text\n",
        "    mask = MaskGen(text)\n",
        "    decoded_so_far = ''\n",
        "    ss = StringSpans(text)\n",
        "    ticks = [(text[:v],(u,v)) for u,v in mask.NVA_words if (u,v) in ss.words]\n",
        "    original_length = len(text)\n",
        "    curr_offset = lambda : (len(text) - original_length)\n",
        "    new_ending = lambda x : curr_offset() + len(x)\n",
        "    for pre_text, (u,v) in ticks:\n",
        "      u, v = (u + curr_offset(), v+ curr_offset())\n",
        "      breakPoint = new_ending(pre_text)\n",
        "      pre_text = text[:breakPoint]\n",
        "      Emojier.log('D>'+'-'*20 + 'tick' + '-'*20 + pre_text)\n",
        "      emoji_options = gaussian_order(Emojier._predict(text[:breakPoint]))\n",
        "      if len(emoji_options) < 2:\n",
        "        Emojier.log('D>'+f'word={text[u:v]},range={(0,breakPoint)},not enough options={emoji_options}')\n",
        "        continue\n",
        "      \n",
        "      emoji = None\n",
        "      if any((encoded_text[breakPoint:].startswith(' '+label) for label in emoji_options)):\n",
        "        emoticons = [label for label in emoji_options if encoded_text[breakPoint:].startswith(' '+label)]\n",
        "        emoticons.sort(key= lambda x: len(x),reverse=True)\n",
        "        emoji = emoticons[0]\n",
        "              \n",
        "      if emoji is None:\n",
        "        Emojier.log('D>'+f'word={text[u:v]},range={(0,breakPoint)},zero start={text[breakPoint:breakPoint+5]}')\n",
        "        decoded_so_far += \"0\"\n",
        "        continue\n",
        "      decoded_so_far += \"1\"\n",
        "      bits = floor(log2(len(emoji_options)))\n",
        "      idx = emoji_options.index(emoji)\n",
        "      decoded_so_far += Emojier.int_to_binary_string(idx,bits)\n",
        "      text = f'{text[0:breakPoint]} {emoji}{text[breakPoint:]}'\n",
        "      Emojier.log('D>'+f\"word={text[u:v]},range={(0,breakPoint)},len({emoji})={len(emoji)},{len(emoji_options)}=len({emoji_options})\")\n",
        "      Emojier.log('D>'+f'decoded_so_far={decoded_so_far}')\n",
        "    return clear_text, decoded_so_far  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "encodedJoshy = '''Hi Josh! I saw 😁 ypur post on ExperiencedDevs and wanted to tell you how much I listwned to it. It's so true 😊 that tech companires ✨ come 😂😂😂 and go 😁😁😁, but the idea of which tech stavck to assemble 💯💯 and how to design complex sysyems is something that's here to stay 💜💜💜💜. It's been really noce chatting with you about it. Have you guys done any consukting jobs outside of tech engineering ✨? I'm ciurious to know what other experts look for in software engineering 💯. '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "Boby = '''Hey Bob!\n",
        "\n",
        "I just read this post on ExperiencedDevs about backend generalist software engineers and their roles in tech companies. It really resonated with me and I wanted to get your take on it. Have you ever been in a similar role, and what did you think of it? Do you think it's common outside of tech companies? What advice would you give to someone looking to become a backend generalist software engineer?'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# enc, rem = Emojier.encode(Boby,random_bit_stream(400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_stream():\n",
        "  gened = set()\n",
        "  byte_string_length = 48\n",
        "  for i in range(2**byte_string_length):\n",
        "    x = random.randint(0,2**byte_string_length-1)\n",
        "    while x in gened:\n",
        "      x = random.randint(0,2**byte_string_length-1)\n",
        "    gened.add(x)\n",
        "    yield Emojier.int_to_binary_string(x,byte_string_length)\n",
        "    # yield x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def runTests():\n",
        "  Emojier.verbose = True\n",
        "  for i, stream in enumerate(test_stream()):\n",
        "  # for i, stream in enumerate(['00100110001101010000110110110111111101101']):\n",
        "    print(f'test {i} stream {stream}')\n",
        "    enc, rem = Emojier.encode(Boby,stream)\n",
        "    print(\"#\"*60)\n",
        "    print(enc)\n",
        "    print(\"#\"*60)\n",
        "    org, data = Emojier.decode(enc)\n",
        "    print(f'ratio={len(data)}/{len(Boby)}={len(data)/len(Boby)}')\n",
        "    assert Boby == org\n",
        "    assert data+rem == stream\n",
        "# runTests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Emojier Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from SampleData import ConversationsRepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chat_id\tc size\tbits\tratio\n",
            "328\t38\t6\t15\n",
            "328\t30\t1\t3\n",
            "328\t142\t29\t20\n",
            "328\t135\t29\t21\n",
            "328\t96\t21\t21\n",
            "328\t121\t25\t20\n",
            "328\t134\t34\t25\n",
            "328\t34\t2\t5\n",
            "328\t111\t37\t33\n",
            "328\t91\t19\t20\n",
            "328\t110\t17\t15\n",
            "328\t78\t20\t25\n",
            "328\t104\t27\t25\n",
            "328\t62\t7\t11\n",
            "328\t104\t15\t14\n",
            "328\t103\t15\t14\n",
            "328\t104\t20\t19\n",
            "328\t19\t1\t5\n",
            "328\t57\t18\t31\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m         f\u001b[39m.\u001b[39mwrite(line\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m     chat_id \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m1\u001b[39m,ConversationsRepo\u001b[39m.\u001b[39mConversationsCount)\n\u001b[1;32m---> 29\u001b[0m runBenchmark()\n",
            "Cell \u001b[1;32mIn[24], line 10\u001b[0m, in \u001b[0;36mrunBenchmark\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m data \u001b[39m=\u001b[39m random_bit_stream(\u001b[39mlen\u001b[39m(text))\n\u001b[0;32m      8\u001b[0m \u001b[39m# data = '1' * len(text)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# text = 'hi, how are you?'\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m encoded_text,rem \u001b[39m=\u001b[39m Emojier\u001b[39m.\u001b[39;49mencode(text,data)\n\u001b[0;32m     11\u001b[0m \u001b[39m# print('rem=',rem)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m _, deData \u001b[39m=\u001b[39m Emojier\u001b[39m.\u001b[39mdecode(encoded_text)\n",
            "Cell \u001b[1;32mIn[17], line 71\u001b[0m, in \u001b[0;36mEmojier.encode\u001b[1;34m(text, bytes_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m breakPoint \u001b[39m=\u001b[39m new_ending(pre_text)\n\u001b[0;32m     70\u001b[0m pre_text \u001b[39m=\u001b[39m text[:breakPoint]\n\u001b[1;32m---> 71\u001b[0m Emojier\u001b[39m.\u001b[39;49mlog(\u001b[39m'\u001b[39;49m\u001b[39mE>\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m20\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtick\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m20\u001b[39;49m \u001b[39m+\u001b[39;49m pre_text)\n\u001b[0;32m     72\u001b[0m emoji_options \u001b[39m=\u001b[39m gaussian_order(Emojier\u001b[39m.\u001b[39m_predict(text[:breakPoint]))\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(emoji_options) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n",
            "Cell \u001b[1;32mIn[17], line 113\u001b[0m, in \u001b[0;36mEmojier.log\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m Emojier\u001b[39m.\u001b[39mverbose:\n\u001b[0;32m    112\u001b[0m   \u001b[39mprint\u001b[39m(string)\n\u001b[1;32m--> 113\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mEmojier.log\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    114\u001b[0m   f\u001b[39m.\u001b[39mwrite(string\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32m<frozen codecs>:186\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def runBenchmark():\n",
        "  Emojier.TopFPercent = 0.1\n",
        "  chat_id = random.randint(1,ConversationsRepo.ConversationsCount)\n",
        "  print(f\"chat_id\\tc size\\tbits\\tratio\")\n",
        "  for i in range(100):\n",
        "    for text in ConversationsRepo.get(chat_id):\n",
        "      data = random_bit_stream(len(text))\n",
        "      # data = '1' * len(text)\n",
        "      # text = 'hi, how are you?'\n",
        "      encoded_text,rem = Emojier.encode(text,data)\n",
        "      # print('rem=',rem)\n",
        "      _, deData = Emojier.decode(encoded_text)\n",
        "      deData += rem\n",
        "      # print(f'text=\"{text}\"\\n->\\nencoded_text=\"{encoded_text}\" \\ndata=\"{data}\"\\ndeData=\"{deData}\"\\ndata==deData=\"{data==deData}\"')\n",
        "      # print(f'ratio={len(data)-len(rem)} / {len(text)}={(len(data)-len(rem)) / len(text)}')\n",
        "      assert data==deData\n",
        "      # print('\\n')\n",
        "      \n",
        "      \n",
        "      bits = len(text)-len(rem)\n",
        "      coverSize = len(text)\n",
        "      line = f\"{chat_id}\\t{coverSize}\\t{bits}\\t{(bits*100)//coverSize}\"\n",
        "      print(line)\n",
        "      with open('benchmark.tsv','a') as f:\n",
        "        f.write(line+'\\n')\n",
        "    chat_id = random.randint(1,ConversationsRepo.ConversationsCount)\n",
        "\n",
        "\n",
        "runBenchmark()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10 -> true\n",
        "# 00 -> false true\n",
        "# 01 -> false false true\n",
        "# 11 -> false false false true"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
