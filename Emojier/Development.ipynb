{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "LONG_TEXT = \"\"\"Text literals and metacharacters make up this string. The compile function is used to create the pattern.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
        "if parent_dir not in sys.path:\n",
        "    sys.path.insert(0, parent_dir)\n",
        "from StringSpans import StringSpans\n",
        "from SemanticMasking import MaskGen, SemanticPositions\n",
        "import itertools\n",
        "from icecream import ic\n",
        "import csv\n",
        "import itertools\n",
        "import re\n",
        "import urllib.request\n",
        "from math import floor, log2\n",
        "from typing import Any, Generator, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from icecream import ic\n",
        "from scipy.special import softmax  # type: ignore\n",
        "from transformers import AutoModelForSequenceClassification  # type: ignore\n",
        "from transformers import AutoTokenizer  # type: ignore\n",
        "from transformers import TFAutoModelForSequenceClassification  # type: ignore\n",
        "\n",
        "from SemanticMasking import MaskGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title random_bit_stream\n",
        "import random\n",
        "\n",
        "def random_bit_stream(length=None):\n",
        "    \"\"\"Return a random string of zeros and ones of the given length (default: random integer between 0 and 100).\"\"\"\n",
        "    if length is None:\n",
        "        length = random.randint(0, 100)\n",
        "    return ''.join(str(random.randint(0, 1)) for _ in range(length))\n",
        "def int_to_binary_string(n: int, length: int):\n",
        "    binary_str = bin(n)[2:]  # convert to binary string, remove '0b' prefix\n",
        "    padded_str = binary_str.rjust(length, '0')  # pad with zeros to length\n",
        "    return padded_str"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NN Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = ['‚ù§', 'üòç', 'üì∑', 'üá∫üá∏', '‚òÄ', 'üíú', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú', 'üòÇ', '‚òπÔ∏è', 'üò≠', 'üòî', 'üò°', 'üí¢', 'üò§', 'üò≥', 'üôÉ', 'üò©', 'üò†', 'üíï', 'üôà', 'üôÑ', 'üî•', 'üòä', 'üòé', '‚ú®', 'üíô', 'üòò']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pre_texts(string:str)->Generator[str, Any, None]:\n",
        "  spans = [x.span() for x in re.finditer(r'(\\s)+', string)]\n",
        "  for span in spans:\n",
        "    yield string[0:span[0]]\n",
        "  if spans[-1][1] != len(string):\n",
        "    yield string\n",
        "def gaussian_order(lst):\n",
        "    length = len(lst)\n",
        "    max_odd_ind = length - 1 if length % 2 == 0 else length - 2\n",
        "    max_even_ind = length - 1 if length % 2 != 0 else length - 2\n",
        "    dist = itertools.chain(range(max_odd_ind, 0, -2), range(0, max_even_ind + 1, 2))\n",
        "    return [lst[i] for i in dist]\n",
        "models_to_choose = [\n",
        "    \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\",\n",
        "    \"AlekseyDorkin/xlm-roberta-en-ru-emoji\"\n",
        "]\n",
        "BASE_MODEL = models_to_choose[0]\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL)\n",
        "    return model, tokenizer\n",
        "MODEL, TOKENIZER = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Emojier:\n",
        "  BASE_MODEL = \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\"\n",
        "  model: Any = MODEL\n",
        "  tokenizer: Any = TOKENIZER\n",
        "  multiplicityBits = 1\n",
        "  TopFPercent = 0.1\n",
        "  verbose = False\n",
        "  @staticmethod\n",
        "  def predict( text: str):\n",
        "    inputs = Emojier.tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = Emojier.model(**inputs)\n",
        "    logits = outputs.logits.detach().numpy()[0]\n",
        "    predicted_class = logits.argmax()\n",
        "    return predicted_class\n",
        "    \n",
        "  @staticmethod\n",
        "  def preprocess(text:str):\n",
        "      new_text = []\n",
        "      for t in text.split(\" \"):\n",
        "          t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "          t = 'http' if t.startswith('http') else t\n",
        "          new_text.append(t)\n",
        "      return \" \".join(new_text)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _predict(text:str) -> List[str]:\n",
        "    # Preprocess text (username and link placeholders)\n",
        "    preprocessed = Emojier.preprocess(text)\n",
        "    inputs = Emojier.tokenizer(preprocessed, return_tensors=\"pt\")\n",
        "    preds = Emojier.model(**inputs).logits\n",
        "    scores = torch.nn.functional.softmax(preds, dim=-1).detach().numpy()\n",
        "    sorted_scores = [float(value) for value in np.sort(scores.squeeze())[::-1]]\n",
        "    ranking = np.argsort(scores)\n",
        "    ranking = ranking.squeeze()[::-1]\n",
        "    emojis = [Emojier.model.config.id2label[i] for i in ranking]\n",
        "    emoticons = [emo for emo, score in zip(emojis, sorted_scores) if emo != 'üá∫üá∏' and score > Emojier.TopFPercent]\n",
        "    return Emojier.addMultiplicities(emoticons)\n",
        "  @staticmethod\n",
        "  def addMultiplicities(emoticons: List[str]):\n",
        "    new_emoticons = []\n",
        "    for emo in emoticons:\n",
        "      for i in range(1,Emojier.multiplicityBits+2):\n",
        "        new_emoticons.append(emo * i)\n",
        "    return new_emoticons\n",
        "  @staticmethod\n",
        "  def encode(text:str,bytes_str:str):\n",
        "    mask = MaskGen(text)\n",
        "    encoded_so_far = ''\n",
        "    ss = StringSpans(text)\n",
        "    ticks = [(text[:v],(u,v)) for u,v in mask.NVA_words if (u,v) in ss.words]\n",
        "    original_length = len(text)\n",
        "    curr_offset = lambda : (len(text) - original_length)\n",
        "    new_ending = lambda x : curr_offset() + len(x)\n",
        "    for pre_text, (u,v) in ticks:\n",
        "      u, v = (u + curr_offset(), v+ curr_offset())\n",
        "      breakPoint = new_ending(pre_text)\n",
        "      pre_text = text[:breakPoint]\n",
        "      Emojier.log('E>'+'-'*20 + 'tick' + '-'*20 + pre_text)\n",
        "      emoji_options = gaussian_order(Emojier._predict(text[:breakPoint]))\n",
        "      if len(emoji_options) < 2:\n",
        "        Emojier.log('E>'+f'word={text[u:v]},range={(0,breakPoint)},not enough options={emoji_options}')\n",
        "        continue\n",
        "      if bytes_str[0] == \"0\":\n",
        "        Emojier.log('E>'+f'word={text[u:v]},range={(0,breakPoint)},zero start={bytes_str[:5]}')\n",
        "        encoded_so_far += bytes_str[0]\n",
        "        bytes_str = bytes_str[1:]\n",
        "        continue\n",
        "      encoded_so_far += bytes_str[0]\n",
        "      bytes_str = bytes_str[1:] # discard the one\n",
        "      bits = floor(log2(len(emoji_options)))\n",
        "      taken_bits = bytes_str[:bits]\n",
        "      ind = int(taken_bits, 2)\n",
        "      encoded_so_far += bytes_str[:bits]\n",
        "      bytes_str = bytes_str[bits:]\n",
        "      emoji = emoji_options[ind]\n",
        "      Emojier.log('E>'+f\"word={text[u:v]},range={(0,breakPoint)},len({emoji})={len(emoji)},{len(emoji_options)}=len({emoji_options})\")\n",
        "      Emojier.log('E>'+f'encoded_so_far={encoded_so_far}')\n",
        "      if len(emoji) > 0:\n",
        "        text = f'{text[0:breakPoint]} {emoji}{text[breakPoint:]}'\n",
        "    return text, bytes_str\n",
        "  @staticmethod\n",
        "  def int_to_binary_string(n: int, length: int) -> str:\n",
        "    binary_str = bin(n)[2:]  # convert to binary string, remove '0b' prefix\n",
        "    padded_str = binary_str.rjust(length, '0')  # pad with zeros to length\n",
        "    return padded_str\n",
        "  @staticmethod\n",
        "  def cntPrefix(string:str, prefix:str):\n",
        "    for i in range(4,0,-1):\n",
        "    #   Emojier.log(f\"string={string[:len(prefix*i)]},prefix*i={prefix*i},string.startswith(prefix * i)={string.startswith(prefix * i)}\",end='|')\n",
        "      if string.startswith(prefix * i):\n",
        "        # Emojier.log('')\n",
        "        return i\n",
        "    # Emojier.log('')\n",
        "    return 0\n",
        "  @staticmethod\n",
        "  def log(string:str):\n",
        "    if Emojier.verbose:\n",
        "      print(string)\n",
        "    with open('Emojier.log','a') as f:\n",
        "      f.write(string+'\\n') \n",
        "      \n",
        "  @staticmethod\n",
        "  def strip(text:str):\n",
        "    for label in labels:\n",
        "      text = text.replace(' '+label,'')\n",
        "    for label in labels:\n",
        "      text = text.replace(label,'')\n",
        "    return text\n",
        "  @staticmethod\n",
        "  def decode(encoded_text:str):\n",
        "    text = encoded_text\n",
        "    for label in labels:\n",
        "      text = text.replace(' '+label,'').replace(label,'')\n",
        "    clear_text = text\n",
        "    mask = MaskGen(text)\n",
        "    decoded_so_far = ''\n",
        "    ss = StringSpans(text)\n",
        "    ticks = [(text[:v],(u,v)) for u,v in mask.NVA_words if (u,v) in ss.words]\n",
        "    original_length = len(text)\n",
        "    curr_offset = lambda : (len(text) - original_length)\n",
        "    new_ending = lambda x : curr_offset() + len(x)\n",
        "    for pre_text, (u,v) in ticks:\n",
        "      u, v = (u + curr_offset(), v+ curr_offset())\n",
        "      breakPoint = new_ending(pre_text)\n",
        "      pre_text = text[:breakPoint]\n",
        "      Emojier.log('D>'+'-'*20 + 'tick' + '-'*20 + pre_text)\n",
        "      emoji_options = gaussian_order(Emojier._predict(text[:breakPoint]))\n",
        "      if len(emoji_options) < 2:\n",
        "        Emojier.log('D>'+f'word={text[u:v]},range={(0,breakPoint)},not enough options={emoji_options}')\n",
        "        continue\n",
        "      \n",
        "      emoji = None\n",
        "      if any((encoded_text[breakPoint:].startswith(' '+label) for label in emoji_options)):\n",
        "        emoticons = [label for label in emoji_options if encoded_text[breakPoint:].startswith(' '+label)]\n",
        "        emoticons.sort(key= lambda x: len(x),reverse=True)\n",
        "        emoji = emoticons[0]\n",
        "              \n",
        "      if emoji is None:\n",
        "        Emojier.log('D>'+f'word={text[u:v]},range={(0,breakPoint)},zero start={text[breakPoint:breakPoint+5]}')\n",
        "        decoded_so_far += \"0\"\n",
        "        continue\n",
        "      decoded_so_far += \"1\"\n",
        "      bits = floor(log2(len(emoji_options)))\n",
        "      idx = emoji_options.index(emoji)\n",
        "      decoded_so_far += Emojier.int_to_binary_string(idx,bits)\n",
        "      text = f'{text[0:breakPoint]} {emoji}{text[breakPoint:]}'\n",
        "      Emojier.log('D>'+f\"word={text[u:v]},range={(0,breakPoint)},len({emoji})={len(emoji)},{len(emoji_options)}=len({emoji_options})\")\n",
        "      Emojier.log('D>'+f'decoded_so_far={decoded_so_far}')\n",
        "    return clear_text, decoded_so_far  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "encodedJoshy = '''Hi Josh! I saw üòÅ ypur post on ExperiencedDevs and wanted to tell you how much I listwned to it. It's so true üòä that tech companires ‚ú® come üòÇüòÇüòÇ and go üòÅüòÅüòÅ, but the idea of which tech stavck to assemble üíØüíØ and how to design complex sysyems is something that's here to stay üíúüíúüíúüíú. It's been really noce chatting with you about it. Have you guys done any consukting jobs outside of tech engineering ‚ú®? I'm ciurious to know what other experts look for in software engineering üíØ. '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "Boby = '''Hey Bob!\n",
        "\n",
        "I just read this post on ExperiencedDevs about backend generalist software engineers and their roles in tech companies. It really resonated with me and I wanted to get your take on it. Have you ever been in a similar role, and what did you think of it? Do you think it's common outside of tech companies? What advice would you give to someone looking to become a backend generalist software engineer?'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# enc, rem = Emojier.encode(Boby,random_bit_stream(400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_stream():\n",
        "  gened = set()\n",
        "  byte_string_length = 48\n",
        "  for i in range(2**byte_string_length):\n",
        "    x = random.randint(0,2**byte_string_length-1)\n",
        "    while x in gened:\n",
        "      x = random.randint(0,2**byte_string_length-1)\n",
        "    gened.add(x)\n",
        "    yield Emojier.int_to_binary_string(x,byte_string_length)\n",
        "    # yield x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def runTests():\n",
        "  Emojier.verbose = True\n",
        "  for i, stream in enumerate(test_stream()):\n",
        "  # for i, stream in enumerate(['00100110001101010000110110110111111101101']):\n",
        "    print(f'test {i} stream {stream}')\n",
        "    enc, rem = Emojier.encode(Boby,stream)\n",
        "    print(\"#\"*60)\n",
        "    print(enc)\n",
        "    print(\"#\"*60)\n",
        "    org, data = Emojier.decode(enc)\n",
        "    print(f'ratio={len(data)}/{len(Boby)}={len(data)/len(Boby)}')\n",
        "    assert Boby == org\n",
        "    assert data+rem == stream\n",
        "# runTests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Emojier Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "from SampleData import ConversationsRepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chat_id\tc size\tbits\tratio\n",
            "5344\t83\t14\t16\n",
            "5344\t109\t24\t22\n",
            "5344\t90\t15\t16\n",
            "5344\t64\t11\t17\n",
            "5344\t105\t17\t16\n",
            "5344\t93\t10\t10\n",
            "5344\t153\t12\t7\n",
            "5344\t28\t4\t14\n",
            "5344\t69\t8\t11\n",
            "5344\t74\t13\t17\n",
            "5344\t112\t18\t16\n",
            "5344\t383\t64\t16\n",
            "5344\t164\t32\t19\n",
            "5344\t158\t15\t9\n",
            "5344\t149\t16\t10\n",
            "5344\t34\t5\t14\n",
            "5344\t114\t17\t14\n",
            "5344\t92\t15\t16\n",
            "5344\t76\t18\t23\n",
            "5344\t329\t62\t18\n",
            "5344\t101\t19\t18\n",
            "5344\t26\t9\t34\n",
            "5674\t16\t0\t0\n",
            "5674\t106\t25\t23\n",
            "5674\t139\t31\t22\n",
            "5674\t249\t36\t14\n",
            "5674\t65\t12\t18\n",
            "5674\t179\t27\t15\n",
            "5674\t77\t17\t22\n",
            "5674\t156\t24\t15\n",
            "5674\t118\t10\t8\n",
            "5674\t115\t11\t9\n",
            "5674\t31\t4\t12\n",
            "5674\t67\t14\t20\n",
            "5674\t89\t9\t10\n",
            "5674\t69\t8\t11\n",
            "5674\t24\t0\t0\n",
            "5674\t71\t9\t12\n",
            "5674\t57\t5\t8\n",
            "5674\t50\t2\t4\n",
            "5674\t56\t6\t10\n",
            "5674\t76\t18\t23\n",
            "5674\t42\t9\t21\n",
            "5674\t30\t10\t33\n",
            "5674\t13\t0\t0\n",
            "48\t36\t5\t13\n",
            "48\t125\t14\t11\n",
            "48\t93\t4\t4\n",
            "48\t117\t19\t16\n",
            "48\t116\t16\t13\n",
            "48\t113\t14\t12\n",
            "48\t75\t10\t13\n",
            "48\t74\t12\t16\n",
            "48\t96\t10\t10\n",
            "48\t98\t17\t17\n",
            "48\t64\t7\t10\n",
            "48\t98\t17\t17\n",
            "48\t86\t6\t6\n",
            "48\t62\t1\t1\n",
            "48\t114\t8\t7\n",
            "48\t75\t13\t17\n",
            "48\t73\t7\t9\n",
            "48\t38\t5\t13\n",
            "48\t95\t15\t15\n",
            "48\t163\t12\t7\n",
            "48\t41\t7\t17\n",
            "5701\t188\t10\t5\n",
            "5701\t86\t10\t11\n",
            "5701\t296\t47\t15\n",
            "5701\t100\t17\t17\n",
            "5701\t237\t25\t10\n",
            "5701\t117\t15\t12\n",
            "5701\t202\t19\t9\n",
            "5701\t97\t14\t14\n",
            "5701\t139\t17\t12\n",
            "5701\t170\t17\t10\n",
            "5701\t285\t40\t14\n",
            "5701\t138\t24\t17\n",
            "5701\t243\t50\t20\n",
            "5701\t149\t25\t16\n",
            "5701\t257\t37\t14\n",
            "5701\t150\t25\t16\n",
            "5701\t285\t45\t15\n",
            "5701\t128\t13\t10\n",
            "5701\t340\t42\t12\n",
            "5701\t193\t35\t18\n",
            "5701\t304\t23\t7\n",
            "5701\t119\t25\t21\n",
            "2358\t39\t3\t7\n",
            "2358\t75\t10\t13\n",
            "2358\t97\t15\t15\n",
            "2358\t73\t6\t8\n",
            "2358\t151\t19\t12\n",
            "2358\t142\t19\t13\n",
            "2358\t81\t13\t16\n",
            "2358\t113\t10\t8\n",
            "2358\t94\t12\t12\n",
            "2358\t126\t10\t7\n",
            "2358\t82\t10\t12\n",
            "2358\t92\t11\t11\n",
            "2358\t66\t8\t12\n",
            "2358\t109\t14\t12\n",
            "2358\t86\t10\t11\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\nasooh\\Desktop\\Y5Project\\Stegasus\\Emojier\\Development.ipynb Cell 15\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         f\u001b[39m.\u001b[39mwrite(line\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     chat_id \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m1\u001b[39m,ConversationsRepo\u001b[39m.\u001b[39mConversationsCount)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m runBenchmark()\n",
            "\u001b[1;32mc:\\Users\\nasooh\\Desktop\\Y5Project\\Stegasus\\Emojier\\Development.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m encoded_text,rem \u001b[39m=\u001b[39m Emojier\u001b[39m.\u001b[39mencode(text,data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# print('rem=',rem)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m _, deData \u001b[39m=\u001b[39m Emojier\u001b[39m.\u001b[39;49mdecode(encoded_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m deData \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rem\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(f'text=\"{text}\"\\n->\\nencoded_text=\"{encoded_text}\" \\ndata=\"{data}\"\\ndeData=\"{deData}\"\\ndata==deData=\"{data==deData}\"')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# print(f'ratio={len(data)-len(rem)} / {len(text)}={(len(data)-len(rem)) / len(text)}')\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\nasooh\\Desktop\\Y5Project\\Stegasus\\Emojier\\Development.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m   text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mlabel,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(label,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m clear_text \u001b[39m=\u001b[39m text\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m mask \u001b[39m=\u001b[39m MaskGen(text)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m decoded_so_far \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/nasooh/Desktop/Y5Project/Stegasus/Emojier/Development.ipynb#X25sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m ss \u001b[39m=\u001b[39m StringSpans(text)\n",
            "File \u001b[1;32mc:\\Users\\nasooh\\Desktop\\Y5Project\\Stegasus\\SemanticMasking\\SemanticMask.py:31\u001b[0m, in \u001b[0;36mMaskGen\u001b[1;34m(text, tokenizer)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m\"\"\"Extract the start and end positions of verbs, nouns, and adjectives in the given text.\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# Load the spaCy English model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Parse the text with spaCy\u001b[39;00m\n\u001b[0;32m     34\u001b[0m doc \u001b[39m=\u001b[39m nlp(text)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\util.py:432\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[0;32m    434\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\util.py:468\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \n\u001b[0;32m    453\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[1;32m--> 468\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, enable\u001b[39m=\u001b[39;49menable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\en_core_web_sm\\__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\util.py:649\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m    648\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[1;32m--> 649\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[0;32m    650\u001b[0m     data_path,\n\u001b[0;32m    651\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    652\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    653\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    654\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    655\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    656\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    657\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\util.py:506\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    504\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config)\n\u001b[0;32m    505\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m--> 506\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[0;32m    507\u001b[0m     config,\n\u001b[0;32m    508\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    509\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    510\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    511\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    512\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    513\u001b[0m )\n\u001b[0;32m    514\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\util.py:554\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[39m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[39m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    553\u001b[0m lang_cls \u001b[39m=\u001b[39m get_lang_class(nlp_config[\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 554\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls\u001b[39m.\u001b[39;49mfrom_config(\n\u001b[0;32m    555\u001b[0m     config,\n\u001b[0;32m    556\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m    557\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m    558\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m    559\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m    560\u001b[0m     auto_fill\u001b[39m=\u001b[39;49mauto_fill,\n\u001b[0;32m    561\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    562\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[0;32m    563\u001b[0m )\n\u001b[0;32m    564\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\language.py:1773\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[1;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1767\u001b[0m warn_if_jupyter_cupy()\n\u001b[0;32m   1769\u001b[0m \u001b[39m# Note that we don't load vectors here, instead they get loaded explicitly\u001b[39;00m\n\u001b[0;32m   1770\u001b[0m \u001b[39m# inside stuff like the spacy train function. If we loaded them here,\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[39m# then we would load them twice at runtime: once when we make from config,\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[39m# and then again when we load from disk.\u001b[39;00m\n\u001b[1;32m-> 1773\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls(vocab\u001b[39m=\u001b[39;49mvocab, create_tokenizer\u001b[39m=\u001b[39;49mcreate_tokenizer, meta\u001b[39m=\u001b[39;49mmeta)\n\u001b[0;32m   1774\u001b[0m \u001b[39mif\u001b[39;00m after_creation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1775\u001b[0m     nlp \u001b[39m=\u001b[39m after_creation(nlp)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\language.py:190\u001b[0m, in \u001b[0;36mLanguage.__init__\u001b[1;34m(self, vocab, max_length, meta, create_tokenizer, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     tokenizer_cfg \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config[\u001b[39m\"\u001b[39m\u001b[39mnlp\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m    189\u001b[0m     create_tokenizer \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mresolve(tokenizer_cfg)[\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m create_tokenizer(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[0;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_error_handler \u001b[39m=\u001b[39m raise_error\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\language.py:92\u001b[0m, in \u001b[0;36mcreate_tokenizer.<locals>.tokenizer_factory\u001b[1;34m(nlp)\u001b[0m\n\u001b[0;32m     90\u001b[0m suffix_search \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mcompile_suffix_regex(suffixes)\u001b[39m.\u001b[39msearch \u001b[39mif\u001b[39;00m suffixes \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m infix_finditer \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mcompile_infix_regex(infixes)\u001b[39m.\u001b[39mfinditer \u001b[39mif\u001b[39;00m infixes \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[39mreturn\u001b[39;00m Tokenizer(\n\u001b[0;32m     93\u001b[0m     nlp\u001b[39m.\u001b[39;49mvocab,\n\u001b[0;32m     94\u001b[0m     rules\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49mtokenizer_exceptions,\n\u001b[0;32m     95\u001b[0m     prefix_search\u001b[39m=\u001b[39;49mprefix_search,\n\u001b[0;32m     96\u001b[0m     suffix_search\u001b[39m=\u001b[39;49msuffix_search,\n\u001b[0;32m     97\u001b[0m     infix_finditer\u001b[39m=\u001b[39;49minfix_finditer,\n\u001b[0;32m     98\u001b[0m     token_match\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49mtoken_match,\n\u001b[0;32m     99\u001b[0m     url_match\u001b[39m=\u001b[39;49mnlp\u001b[39m.\u001b[39;49mDefaults\u001b[39m.\u001b[39;49murl_match,\n\u001b[0;32m    100\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\tokenizer.pyx:75\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.__init__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\tokenizer.pyx:574\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\tokenizer.pyx:609\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\vocab.pyx:275\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.make_fused_token\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\vocab.pyx:174\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.get_by_orth\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\vocab.pyx:197\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy\\lang\\lex_attrs.py:146\u001b[0m, in \u001b[0;36mlower\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlower\u001b[39m(string: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mreturn\u001b[39;00m string\u001b[39m.\u001b[39;49mlower()\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def runBenchmark():\n",
        "  Emojier.TopFPercent = 0.1\n",
        "  chat_id = random.randint(1,ConversationsRepo.ConversationsCount)\n",
        "  print(f\"chat_id\\tc size\\tbits\\tratio\")\n",
        "  for i in range(100):\n",
        "    for text in ConversationsRepo.get(chat_id):\n",
        "      data = random_bit_stream(len(text))\n",
        "      # data = '1' * len(text)\n",
        "      # text = 'hi, how are you?'\n",
        "      encoded_text,rem = Emojier.encode(text,data)\n",
        "      # print('rem=',rem)\n",
        "      _, deData = Emojier.decode(encoded_text)\n",
        "      deData += rem\n",
        "      # print(f'text=\"{text}\"\\n->\\nencoded_text=\"{encoded_text}\" \\ndata=\"{data}\"\\ndeData=\"{deData}\"\\ndata==deData=\"{data==deData}\"')\n",
        "      # print(f'ratio={len(data)-len(rem)} / {len(text)}={(len(data)-len(rem)) / len(text)}')\n",
        "      assert data==deData\n",
        "      # print('\\n')\n",
        "      \n",
        "      \n",
        "      bits = len(text)-len(rem)\n",
        "      coverSize = len(text)\n",
        "      line = f\"{chat_id}\\t{coverSize}\\t{bits}\\t{(bits*100)//coverSize}\"\n",
        "      print(line)\n",
        "      with open('benchmark.tsv','a') as f:\n",
        "        f.write(line+'\\n')\n",
        "    chat_id = random.randint(1,ConversationsRepo.ConversationsCount)\n",
        "\n",
        "\n",
        "runBenchmark()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
