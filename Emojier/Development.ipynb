{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "LONG_TEXT = \"\"\"Text literals and metacharacters make up this string. The compile function is used to create the pattern.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
        "if parent_dir not in sys.path:\n",
        "    sys.path.insert(0, parent_dir)\n",
        "from StringSpans import StringSpans\n",
        "from SemanticMasking import MaskGen, SemanticPositions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title random_bit_stream\n",
        "import random\n",
        "\n",
        "def random_bit_stream(length=None):\n",
        "    \"\"\"Return a random string of zeros and ones of the given length (default: random integer between 0 and 100).\"\"\"\n",
        "    if length is None:\n",
        "        length = random.randint(0, 100)\n",
        "    return ''.join(str(random.randint(0, 1)) for _ in range(length))\n",
        "def int_to_binary_string(n: int, length: int):\n",
        "    binary_str = bin(n)[2:]  # convert to binary string, remove '0b' prefix\n",
        "    padded_str = binary_str.rjust(length, '0')  # pad with zeros to length\n",
        "    return padded_str"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rule Based\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title https://github.com/farkmarnum/emojify\n",
        "import json\n",
        "import random\n",
        "from math import log2,floor,ceil \n",
        "import itertools\n",
        "import re\n",
        "from typing import Dict, Generator, List, Tuple\n",
        "\n",
        "with open('./emoji-data.json', 'r') as f:\n",
        "    emoji_data: Dict[str,Dict[str,List[str]]] = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Constants\n",
        "regex = re.compile(r'[a-z0-9]+')\n",
        "ALL_EMOJIS = set()\n",
        "for k,v in emoji_data.items():\n",
        "  if regex.match(k) is None:\n",
        "    ALL_EMOJIS.add(k)\n",
        "    # print('k',k)\n",
        "  if isinstance(v,str) and regex.match(v) is None:\n",
        "    ALL_EMOJIS.add(v)\n",
        "    # print('v',v)\n",
        "  else:\n",
        "    for kk,vv in v.items():\n",
        "      if regex.match(kk) is None:\n",
        "        ALL_EMOJIS.add(kk)\n",
        "        # print('kk',kk)\n",
        "      if isinstance(vv,str) and regex.match(vv) is None:\n",
        "        ALL_EMOJIS.add(v)\n",
        "        # print('vv',vv)\n",
        "EMOJIER_COMMON_WORDS = {\n",
        "    'a',\n",
        "    'an',\n",
        "    'as',\n",
        "    'is',\n",
        "    'if',\n",
        "    'of',\n",
        "    'the',\n",
        "    'it',\n",
        "    'its',\n",
        "    'or',\n",
        "    'are',\n",
        "    'this',\n",
        "    'with',\n",
        "    'so',\n",
        "    'to',\n",
        "    'at',\n",
        "    'was',\n",
        "    'and',\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title encode decode\n",
        "class Emojier:\n",
        "  @staticmethod\n",
        "  def gaussian_order(lst):\n",
        "    length = len(lst)\n",
        "    max_odd_ind = length - 1 if length % 2 == 0 else length - 2\n",
        "    max_even_ind = length - 1 if length % 2 != 0 else length - 2\n",
        "    dist = itertools.chain(range(max_odd_ind,0,-2),range(0,max_even_ind + 1 , 2))\n",
        "    return [lst[i] for i in dist]\n",
        "\n",
        "  @staticmethod\n",
        "  def encode(\n",
        "        input_str: str,\n",
        "        bytes_str: str,\n",
        "        verbose=False,\n",
        "        mask=True,\n",
        "        maskStep: int =6,\n",
        "        topX=False,\n",
        "        X: float=0.15\n",
        "    ) -> Tuple[str,str]:\n",
        "    \n",
        "    if verbose:\n",
        "      print('encode:')\n",
        "    input_str_spans = StringSpans(input_str)\n",
        "    word_span_n_words = zip(input_str_spans.words, input_str_spans.get_words())\n",
        "    result = input_str\n",
        "    acc_offset = 0\n",
        "    \n",
        "    word_span_n_words_options: List[Tuple[int,str,List[str]]] = []\n",
        "    for (_,we), word_raw in word_span_n_words:\n",
        "      word = word_raw.lower()\n",
        "      is_too_common = word in EMOJIER_COMMON_WORDS\n",
        "\n",
        "      emoji_options = \\\n",
        "        Emojier.gaussian_order( ['']+\n",
        "          [x[0] for x in\n",
        "            sorted(\n",
        "              emoji_data.get(word, {}).items(),\n",
        "              key=lambda x:x[1],\n",
        "              reverse=True\n",
        "            )\n",
        "          ]\n",
        "        )\n",
        "      if not is_too_common and len(emoji_options)>=2:\n",
        "        word_span_n_words_options.append((we,word_raw, emoji_options))\n",
        "    \n",
        "    if mask:\n",
        "      word_span_n_words_options = word_span_n_words_options[::maskStep]\n",
        "    if topX:\n",
        "      word_span_n_words_options.sort(key=lambda tup : len(tup[2]),reverse=True)\n",
        "      taken_elements = ceil(len(word_span_n_words_options) * X) \n",
        "      word_span_n_words_options = word_span_n_words_options[:taken_elements]\n",
        "      \n",
        "    for we, word_raw, emoji_options in word_span_n_words_options:\n",
        "      word = word_raw.lower()\n",
        "\n",
        "      if verbose:\n",
        "        print(f\"word: {word} \\nlen: {len(emoji_options)} \\temoji_options[:10]: {emoji_options[:10]}\")\n",
        "\n",
        "      bits = floor(log2(len(emoji_options)))\n",
        "      taken_bits = bytes_str[:bits]\n",
        "      ind = int(taken_bits, 2)\n",
        "      bytes_str = bytes_str[bits:]\n",
        "      emojis = emoji_options[ind]\n",
        "      if len(emojis) > 0:\n",
        "        we = we + acc_offset\n",
        "        acc_offset += len(emojis) + 1\n",
        "        if verbose:\n",
        "          print(f'>>>encoding {taken_bits} = {ind} as {emojis}\\nwe={we}\\tacc_offset={acc_offset}')\n",
        "          print(f'result[:we]=\"{result[:we]}\" result[we:]=\"{result[we:]}\"')  \n",
        "        result = f'{result[:we]} {emojis}{result[we:]}'  \n",
        "\n",
        "    return result, bytes_str\n",
        "\n",
        "  @staticmethod\n",
        "  def eat_back(s:str) -> Generator[str,None,None]:\n",
        "    for i in range(len(s),-1,-1):\n",
        "      yield s[0:i]\n",
        "  @staticmethod\n",
        "  def decode(\n",
        "            input_str: str,\n",
        "            verbose=False,\n",
        "            mask: bool =True,\n",
        "            maskStep: int =6,\n",
        "            topX: bool =False,\n",
        "            X: float=0.15\n",
        "      ) -> Tuple[str,str]:\n",
        "    \n",
        "    if verbose:\n",
        "      print('decoding!')\n",
        "    wordish = re.compile(r'^[a-z]*$')\n",
        "    input_str_ss = StringSpans(input_str)\n",
        "    words = [input_str[s:e] for s,e in input_str_ss.non_spaces]\n",
        "    result = input_str\n",
        "    bytes_str = ''\n",
        "    \n",
        "    emoticons_used = []\n",
        "    word_span_n_words_options: List[Tuple[int,str,List[str]]] = []\n",
        "    for i, word_raw in enumerate(words[:-1]):\n",
        "      word = word_raw.lower()\n",
        "      \n",
        "      if wordish.match(word) is None:\n",
        "        continue \n",
        "\n",
        "      is_too_common = word in EMOJIER_COMMON_WORDS\n",
        "\n",
        "      emoji_options = \\\n",
        "        Emojier.gaussian_order( ['']+\n",
        "          [x[0] for x in\n",
        "            sorted(\n",
        "              emoji_data.get(word, {}).items(),\n",
        "              key=lambda x:x[1],\n",
        "              reverse=True\n",
        "            )\n",
        "          ]\n",
        "        )\n",
        "      if not is_too_common and len(emoji_options) >= 2:\n",
        "        word_span_n_words_options.append((i,word_raw,emoji_options))\n",
        "\n",
        "    if mask:\n",
        "      word_span_n_words_options = word_span_n_words_options[::maskStep]\n",
        "    if topX:\n",
        "      word_span_n_words_options.sort(key=lambda tup : len(tup[2]),reverse=True)\n",
        "      taken_elements = ceil(len(word_span_n_words_options) * X) \n",
        "      word_span_n_words_options = word_span_n_words_options[:taken_elements]\n",
        "        \n",
        "    for i, word_raw, emoji_options in word_span_n_words_options:\n",
        "      word = word_raw.lower()\n",
        "\n",
        "      if verbose:\n",
        "        print(f\"word: {word} \\nlen: {len(emoji_options)} \\temoji_options[:10]: {emoji_options[:10]}\")\n",
        "\n",
        "      bits = floor(log2(len(emoji_options)))\n",
        "      index = 0\n",
        "      for w in Emojier.eat_back(words[i+1]):\n",
        "        if w in emoji_options:\n",
        "          index = emoji_options.index(w)\n",
        "          emoticons_used.append((w,i+1))\n",
        "          break\n",
        "        \n",
        "      data_extracted = int_to_binary_string(index,bits)\n",
        "      if verbose:\n",
        "        print(f'>>>decoding word:\"{words[i]}\" next word:\"{words[i+1]}\" length:\"{len(emoji_options)}\"')\n",
        "        print(f'bits:\"{bits}\" data extracted:\"{data_extracted}\" index:\"{index}\"')\n",
        "      bytes_str += data_extracted\n",
        "\n",
        "    for emo,idx in reversed(emoticons_used):\n",
        "      s,e = input_str_ss.non_spaces[idx]\n",
        "      if emo:\n",
        "        result = result[:s-1] + result[s:e].replace(emo,'') + result[e:]\n",
        "  \n",
        "    return result, bytes_str\n",
        "\n",
        "\n",
        "tests = 100\n",
        "acc = 0\n",
        "onlyRatio = False\n",
        "print(f\"Running {tests} tests\")\n",
        "for i in range(tests):\n",
        "  data = random_bit_stream(60)\n",
        "  # text = 'hi, how are you?'\n",
        "  LONG_TEXT = '''Metaphysical solipsism is a variety of solipsism. Based on a philosophy of subjective idealism, metaphysical solipsists maintain that the self is the only existing reality and that all other realities, including the external world and other persons, are representations of that self, and have no independent existence.[citation needed] There are several versions of metaphysical solipsism, such as Caspar Hare's egocentric presentism (or perspectival realism), in which other people are conscious, but their experiences are simply not present.'''\n",
        "  text = LONG_TEXT\n",
        "  verbose = False\n",
        "  encoded_text,rem = Emojier.encode(text,data,verbose=verbose)\n",
        "  if not onlyRatio:\n",
        "    print('rem=',rem)\n",
        "    print('encoded_text=',encoded_text)\n",
        "  original_txt, deData = Emojier.decode(encoded_text,verbose=verbose)\n",
        "  if not onlyRatio:\n",
        "    print('original_txt=',original_txt)\n",
        "  deData += rem\n",
        "  if not onlyRatio:\n",
        "    print(f'text=\"{text}\"\\n->\\nencoded_text=\"{encoded_text}\" \\ndata=\"{data}\"\\ndeData=\"{deData}\"\\ndata==deData=\"{data==deData}\"')\n",
        "  ratio =(len(data)-len(rem)) / len(text)\n",
        "  acc += ratio\n",
        "  if not onlyRatio:\n",
        "    print(f'ratio={len(data)-len(rem)} / {len(text)}={ratio}')\n",
        "  assert data==deData\n",
        "  assert text==original_txt\n",
        "  if not onlyRatio:\n",
        "    print('\\n')\n",
        "    print(\"#\"*100)\n",
        "    print('\\n')\n",
        "\n",
        "print(f'avg ratio = {acc/tests}')\n",
        "print(f'old ratio = 0.8235294117648159')\n",
        "\n",
        "# 0000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://huggingface.co/spaces/adorkin/BilingualEmojiPredictor/blob/main/app.py\n",
        "\n",
        "%pip install transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\"\n",
        "TOP_N = 5\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "    \n",
        "def get_top_emojis(text):\n",
        "    preprocessed = preprocess(text)\n",
        "    inputs = tokenizer(preprocessed, return_tensors=\"pt\")\n",
        "    preds = model(**inputs).logits\n",
        "    scores = torch.nn.functional.softmax(preds, dim=-1).detach().numpy()\n",
        "    sorted_scores = [float(value) for value in np.sort(scores.squeeze())[::-1]]\n",
        "    ranking = np.argsort(scores)\n",
        "    ranking = ranking.squeeze()[::-1]\n",
        "    emojis = [model.config.id2label[i] for i in ranking]\n",
        "    return dict(zip(emojis, sorted_scores))\n",
        "\n",
        "get_top_emojis(preprocess('I’ve toyed with tghe idea of usuing GPT-3’s API to add much more intelligent capabilirties to RC'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NN Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = ['❤', '😍', '📷', '🇺🇸', '☀', '💜', '😉', '💯', '😁', '🎄', '📸', '😜', '😂', '☹️', '😭', '😔', '😡', '💢', '😤', '😳', '🙃', '😩', '😠', '💕', '🙈', '🙄', '🔥', '😊', '😎', '✨', '💙', '😘']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import itertools\n",
        "import re\n",
        "import urllib.request\n",
        "from math import floor, log2\n",
        "from typing import Any, Generator, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from icecream import ic\n",
        "from scipy.special import softmax  # type: ignore\n",
        "from transformers import AutoModelForSequenceClassification  # type: ignore\n",
        "from transformers import AutoTokenizer  # type: ignore\n",
        "from transformers import TFAutoModelForSequenceClassification  # type: ignore\n",
        "\n",
        "\n",
        "def pre_texts(string:str)->Generator[str, Any, None]:\n",
        "  spans = [x.span() for x in re.finditer(r'(\\s)+', string)]\n",
        "  for span in spans:\n",
        "    yield string[0:span[0]]\n",
        "  if spans[-1][1] != len(string):\n",
        "    yield string\n",
        "\n",
        "\n",
        "\n",
        "def gaussian_order(lst):\n",
        "    length = len(lst)\n",
        "    max_odd_ind = length - 1 if length % 2 == 0 else length - 2\n",
        "    max_even_ind = length - 1 if length % 2 != 0 else length - 2\n",
        "    dist = itertools.chain(range(max_odd_ind, 0, -2), range(0, max_even_ind + 1, 2))\n",
        "    return [lst[i] for i in dist]\n",
        "\n",
        "\n",
        "class Emojier:\n",
        "  BASE_MODEL = \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\"\n",
        "  model: Any = None\n",
        "  tokenizer: Any = None\n",
        "\n",
        "  def setVerbose(self, v: bool):\n",
        "    self.verbose = v\n",
        "    return self\n",
        "\n",
        "  def __init__(self, tokenizer, model, interval: int):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.interval = interval\n",
        "    self.verbose = False\n",
        "\n",
        "  def predict(self, text: str):\n",
        "    inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = self.model(**inputs)\n",
        "    logits = outputs.logits.detach().numpy()[0]\n",
        "    predicted_class = logits.argmax()\n",
        "    return predicted_class\n",
        "    \n",
        "  def preprocess(self,text:str):\n",
        "      new_text = []\n",
        "      for t in text.split(\" \"):\n",
        "          t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "          t = 'http' if t.startswith('http') else t\n",
        "          new_text.append(t)\n",
        "      return \" \".join(new_text)\n",
        "    \n",
        "  def _predict(self,text:str) -> List[str]:\n",
        "    # Preprocess text (username and link placeholders)\n",
        "    preprocessed = self.preprocess(text)\n",
        "    inputs = self.tokenizer(preprocessed, return_tensors=\"pt\")\n",
        "    preds = self.model(**inputs).logits\n",
        "    scores = torch.nn.functional.softmax(preds, dim=-1).detach().numpy()\n",
        "    # sorted_scores = [float(value) for value in np.sort(scores.squeeze())[::-1]]\n",
        "    ranking = np.argsort(scores)\n",
        "    ranking = ranking.squeeze()[::-1]\n",
        "    emojis = [self.model.config.id2label[i] for i in ranking]\n",
        "    # return dict(zip(emojis, sorted_scores))\n",
        "    return list(filter(lambda x : x != '🇺🇸',emojis))\n",
        "  \n",
        "  def encode(self,text:str,bytes_str:str):\n",
        "    mask = MaskGen(text)\n",
        "    ticks = [text[:v] for u,v in mask.NVA_words]\n",
        "    original_length = len(text)\n",
        "    new_ending = lambda x : (len(text) - original_length) + len(x)\n",
        "    for pre_text in ticks:\n",
        "      breakPoint = new_ending(pre_text)\n",
        "      emoji_options = gaussian_order(self._predict(text[:breakPoint]))\n",
        "\n",
        "      if bytes_str[0] == \"0\":\n",
        "        bytes_str = bytes_str[1:]\n",
        "        continue\n",
        "      if self.verbose:\n",
        "        print(f\"word: {pre_text} \\nlen: {len(emoji_options)} \\temoji_options: {emoji_options}\")\n",
        "\n",
        "      bits = floor(log2(len(emoji_options)))\n",
        "      taken_bits = bytes_str[:bits]\n",
        "      ind = int(taken_bits, 2)\n",
        "      bytes_str = bytes_str[bits:]\n",
        "      emojis = emoji_options[ind]\n",
        "\n",
        "      # Mutliplicity\n",
        "      taken_bits = bytes_str[:2]\n",
        "      mult = int(taken_bits, 2)+1\n",
        "      bytes_str = bytes_str[2:]\n",
        "      \n",
        "      if len(emojis) > 0:\n",
        "        text = f'{text[0:breakPoint]} {mult * emojis}{text[breakPoint:]}'\n",
        "      if self.verbose:\n",
        "        print(f'>>>encoding {taken_bits} = {ind} as {emojis}\\nencoded text={text}')\n",
        "    return text, bytes_str\n",
        "  @staticmethod\n",
        "  def int_to_binary_string(n: int, length: int) -> str:\n",
        "    binary_str = bin(n)[2:]  # convert to binary string, remove '0b' prefix\n",
        "    padded_str = binary_str.rjust(length, '0')  # pad with zeros to length\n",
        "    return padded_str\n",
        "  @staticmethod\n",
        "  def cntPrefix(string:str, prefix:str):\n",
        "    count = 0\n",
        "    prefix_len = len(prefix)\n",
        "    i = 0\n",
        "    while i < len(string):\n",
        "        if string[i:i+prefix_len] == prefix:\n",
        "            count += 1\n",
        "            i += prefix_len\n",
        "        else:\n",
        "            i += 1\n",
        "    return count\n",
        "  def decode(self,text:str):\n",
        "    bytes_str = ''\n",
        "    mask = MaskGen(text)\n",
        "    ticks = [text[:v] for u,v in mask.NVA_words]\n",
        "    original_length = len(text)\n",
        "    new_ending = lambda x : (len(text) - original_length) + len(x)\n",
        "    emoji_locations = []\n",
        "    for pre_text in ticks:\n",
        "      breakPoint = new_ending(pre_text)\n",
        "\n",
        "      emoji = \\\n",
        "          [label for label in labels if text[breakPoint:].startswith(' '+label)][0] \\\n",
        "            if any((text[breakPoint:].startswith(' '+label) for label in labels)) \\\n",
        "              else None\n",
        "      if emoji is not None:\n",
        "        emoji_options = gaussian_order(self._predict(text[:breakPoint]))\n",
        "        bits = floor(log2(len(emoji_options)))\n",
        "        idx = emoji_options.index(emoji)\n",
        "        bytes_str += Emojier.int_to_binary_string(idx,bits)\n",
        "        # Multiplicity\n",
        "        multi = Emojier.cntPrefix(text[breakPoint+1:],emoji) \n",
        "        bytes_str += Emojier.int_to_binary_string(multi-1,2)\n",
        "        emoji_locations.append((breakPoint, breakPoint + 1 + len(emoji)*multi))\n",
        "        if self.verbose:\n",
        "          print(f\"word: {pre_text} \\nlen: {len(emoji_options)} \\temoji_options: {emoji_options}\")\n",
        "          print(f\"emoji: {emoji} \\nlen: {len(emoji)} \\tmulti: {multi}\")\n",
        "      else:\n",
        "        bytes_str += '0'\n",
        "    # remove emojies\n",
        "    original = text\n",
        "    for s,e in reversed(emoji_locations):\n",
        "      original = original[:s] + original[e:]\n",
        "    return original, bytes_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "emo = Emojier(tokenizer,model,4).setVerbose(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emo.verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('hi, how are you?', '1000001001')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emo.encode(\"hi, how are you?\",\"1000001001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word: tokenization \n",
            "len: 31 \temoji_options: ['🙈', '😳', '☀', '🎄', '😔', '☹️', '📷', '😘', '❤', '💕', '😍', '🔥', '💯', '😎', '😊', '✨', '😂', '😁', '😜', '😉', '💜', '💙', '😤', '📸', '💢', '😡', '😭', '🙄', '😠', '🙃', '😩']\n",
            ">>>encoding 00 = 8 as ❤\n",
            "encoded text=tokenization ❤ is the pain of my existence\n",
            "word: tokenization is the pain \n",
            "len: 31 \temoji_options: ['💕', '📷', '😎', '💙', '📸', '❤', '✨', '☀', '😠', '🙈', '😳', '😂', '😡', '💢', '☹️', '😔', '😭', '😤', '🙄', '😩', '🙃', '💯', '🔥', '😊', '💜', '😜', '😉', '😁', '😘', '🎄', '😍']\n",
            ">>>encoding 01 = 9 as 🙈\n",
            "encoded text=tokenization ❤ is the pain 🙈🙈 of my existence\n"
          ]
        }
      ],
      "source": [
        "encoded, rem = emo.encode(\"tokenization is the pain of my existence\",\"1000001001010101001101010101\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word: tokenization \n",
            "len: 31 \temoji_options: ['🙈', '😳', '☀', '🎄', '😔', '☹️', '📷', '😘', '❤', '💕', '😍', '🔥', '💯', '😎', '😊', '✨', '😂', '😁', '😜', '😉', '💜', '💙', '😤', '📸', '💢', '😡', '😭', '🙄', '😠', '🙃', '😩']\n",
            "emoji: ❤ \n",
            "len: 1 \tmulti: 1\n",
            "word: tokenization ❤ is the pain \n",
            "len: 31 \temoji_options: ['💕', '📷', '😎', '💙', '📸', '❤', '✨', '☀', '😠', '🙈', '😳', '😂', '😡', '💢', '☹️', '😔', '😭', '😤', '🙄', '😩', '🙃', '💯', '🔥', '😊', '💜', '😜', '😉', '😁', '😘', '🎄', '😍']\n",
            "emoji: 🙈 \n",
            "len: 1 \tmulti: 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('tokenization is the pain of my existence', '1000001001010')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emo.decode(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
