{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gM4tXm1xJ_JE"
      },
      "source": [
        "# Stegasus"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ODi5968TKDdR"
      },
      "source": [
        "## Commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MPrAz7BPtck",
        "outputId": "a93d4349-86de-4225-9e48-fd6e8aee4dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "openjdk version \"17.0.7\" 2023-04-18\n",
            "OpenJDK Runtime Environment (build 17.0.7+7-Ubuntu-0ubuntu120.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.7+7-Ubuntu-0ubuntu120.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-17-openjdk-amd64/bin/java\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDREVgr4EIqc",
        "outputId": "45993d81-89c2-4722-cf66-e2b331f0be50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Stegasus'...\n",
            "remote: Enumerating objects: 408, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 408 (delta 21), reused 56 (delta 15), pack-reused 339\u001b[K\n",
            "Receiving objects: 100% (408/408), 103.19 MiB | 21.01 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NasoohOlabi/Stegasus.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Brrd_F4wEMUQ"
      },
      "outputs": [],
      "source": [
        "!mv Stegasus/* .\n",
        "!rm -r Stegasus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qmY1Ys20DReD",
        "outputId": "c22fdfc2-e335-4f75-c8aa-64ad7ee21ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (from -r requirements.txt (line 4))\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting icecream>=2.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: transformers>=4.30.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.30.1)\n",
            "Collecting emoji>=2.1.0 (from -r requirements.txt (line 3))\n",
            "  Downloading emoji-2.5.0.tar.gz (355 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m355.8/355.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints>=0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.15.1)\n",
            "Requirement already satisfied: nltk>=3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.8.1)\n",
            "Collecting numpy>=1.23.0 (from -r requirements.txt (line 8))\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai>=0.27.7 (from -r requirements.txt (line 9))\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.10.1)\n",
            "Requirement already satisfied: spacy>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (3.5.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (2.0.1+cu118)\n",
            "Collecting colorama>=0.3.9 (from icecream>=2.1.3->-r requirements.txt (line 1))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream>=2.1.3->-r requirements.txt (line 1)) (2.14.0)\n",
            "Collecting executing>=0.3.1 (from icecream>=2.1.3->-r requirements.txt (line 1))\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream>=2.1.3->-r requirements.txt (line 1))\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.1->-r requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.1->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.1->-r requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.1->-r requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.1->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.1->-r requirements.txt (line 2)) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.1->-r requirements.txt (line 2)) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.1->-r requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.1->-r requirements.txt (line 6)) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.12.1->-r requirements.txt (line 6)) (4.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0->-r requirements.txt (line 7)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0->-r requirements.txt (line 7)) (1.2.0)\n",
            "Collecting aiohttp (from openai>=0.27.7->-r requirements.txt (line 9))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.1->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.1->-r requirements.txt (line 10)) (2022.7.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.5.0->-r requirements.txt (line 12)) (3.3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->-r requirements.txt (line 13)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->-r requirements.txt (line 13)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->-r requirements.txt (line 13)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->-r requirements.txt (line 13)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->-r requirements.txt (line 13)) (16.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream>=2.1.3->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.1->-r requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.1->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.1->-r requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.1->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.5.0->-r requirements.txt (line 12)) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.5.0->-r requirements.txt (line 12)) (0.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->-r requirements.txt (line 9)) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai>=0.27.7->-r requirements.txt (line 9))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai>=0.27.7->-r requirements.txt (line 9))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai>=0.27.7->-r requirements.txt (line 9))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai>=0.27.7->-r requirements.txt (line 9))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai>=0.27.7->-r requirements.txt (line 9))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.5.0->-r requirements.txt (line 12)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->-r requirements.txt (line 13)) (1.3.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.5.0-py2.py3-none-any.whl size=351211 sha256=0b7abdaa6b973b9442a0d157d706e1f2e76ec6e471412d36c7f0b9de8b92e56c\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/0c/2c/07b5af72b120503fe24590691d24c462a25e5e530db8700a96\n",
            "Successfully built emoji\n",
            "Installing collected packages: executing, numpy, multidict, frozenlist, emoji, colorama, async-timeout, asttokens, yarl, icecream, aiosignal, aiohttp, openai\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 asttokens-2.2.1 async-timeout-4.0.2 colorama-0.4.6 emoji-2.5.0 executing-1.2.0 frozenlist-1.3.3 icecream-2.1.3 multidict-6.0.4 numpy-1.24.3 openai-0.27.8 yarl-1.9.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1AiAjOvuJ-0i"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "XUOmnFcsF4yM"
      },
      "outputs": [],
      "source": [
        "#@title random_bit_stream\n",
        "import random\n",
        "\n",
        "def random_bit_stream(length=None):\n",
        "    \"\"\"Return a random string of zeros and ones of the given length (default: random integer between 0 and 100).\"\"\"\n",
        "    if length is None:\n",
        "        length = random.randint(0, 100)\n",
        "    return ''.join(str(random.randint(0, 1)) for _ in range(length))\n",
        "def int_to_binary_string(n: int, length: int):\n",
        "    binary_str = bin(n)[2:]  # convert to binary string, remove '0b' prefix\n",
        "    padded_str = binary_str.rjust(length, '0')  # pad with zeros to length\n",
        "    return padded_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JMGfFY5CjLHB"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kfxqsa7sDReN"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
        "if parent_dir not in sys.path:\n",
        "    sys.path.insert(0, parent_dir)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy9bCvx4G9KS"
      },
      "source": [
        "## Frustratingly Simple BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI-3fG3eK5aj",
        "outputId": "2a545c8f-d074-4465-9d1a-9de0877dc587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRNpABNiDReR"
      },
      "outputs": [],
      "source": [
        "from FrustratinglySimpleBert import MaskedStego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN9JGVMwVv6k",
        "outputId": "1af8fb91-b9f4-411b-c5c2-71b4b25f7946"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "masked_stego = MaskedStego()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6mXQ2ZdSm0N"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TdQ6lNerHwGG"
      },
      "source": [
        "## Typoceros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8Hz3JvxNNdW7"
      },
      "outputs": [],
      "source": [
        "from TypocerosJar import JavaJarWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mTlY9DvQKCh"
      },
      "outputs": [],
      "source": [
        "!java -jar ./Typoceros4j.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXvmRu2FBqlX"
      },
      "outputs": [],
      "source": [
        "Typo = JavaJarWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "E-NhTA68Lk1W",
        "outputId": "fd93c840-30b4-4139-efb7-a935deb8df87"
      },
      "outputs": [
        {
          "ename": "BrokenPipeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ed91d5eb640b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTypo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi, how are you?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"1010101001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/TypocerosJar.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, string, bytes_str)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mb\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mb\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ],
      "source": [
        "Typo.encode(\"hi, how are you?\",\"1010101001\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "icN5UrprJCh6"
      },
      "source": [
        "## Emojer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4DKuWj8o1QTQ"
      },
      "outputs": [],
      "source": [
        "LONG_TEXT = \"\"\"Text literals and metacharacters make up this string. The compile function is used to create the pattern.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_bkfhgQHnf6A"
      },
      "outputs": [],
      "source": [
        "# https://github.com/huggingface/torchMoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Emojier : Emojier = Emojier\n",
        "# from Emojier import Emojier\n",
        "import csv\n",
        "import itertools\n",
        "import re\n",
        "import urllib.request\n",
        "from math import floor, log2\n",
        "from typing import Any, Generator, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from icecream import ic\n",
        "from scipy.special import softmax  # type: ignore\n",
        "from transformers import AutoModelForSequenceClassification  # type: ignore\n",
        "from transformers import AutoTokenizer  # type: ignore\n",
        "from transformers import TFAutoModelForSequenceClassification  # type: ignore\n",
        "\n",
        "from SemanticMasking import MaskGen\n",
        "\n",
        "labels = ['‚ù§', 'üòç', 'üì∑', 'üá∫üá∏', '‚òÄ', 'üíú', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú', 'üòÇ', '‚òπÔ∏è', 'üò≠', 'üòî', 'üò°', 'üí¢', 'üò§', 'üò≥', 'üôÉ', 'üò©', 'üò†', 'üíï', 'üôà', 'üôÑ', 'üî•', 'üòä', 'üòé', '‚ú®', 'üíô', 'üòò']\n",
        "\n",
        "def pre_texts(string:str)->Generator[str, Any, None]:\n",
        "  spans = [x.span() for x in re.finditer(r'(\\s)+', string)]\n",
        "  for span in spans:\n",
        "    yield string[0:span[0]]\n",
        "  if spans[-1][1] != len(string):\n",
        "    yield string\n",
        "\n",
        "\n",
        "\n",
        "def gaussian_order(lst):\n",
        "    length = len(lst)\n",
        "    max_odd_ind = length - 1 if length % 2 == 0 else length - 2\n",
        "    max_even_ind = length - 1 if length % 2 != 0 else length - 2\n",
        "    dist = itertools.chain(range(max_odd_ind, 0, -2), range(0, max_even_ind + 1, 2))\n",
        "    return [lst[i] for i in dist]\n",
        "\n",
        "models_to_choose = [\n",
        "    \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\",\n",
        "    \"AlekseyDorkin/xlm-roberta-en-ru-emoji\"\n",
        "]\n",
        "\n",
        "BASE_MODEL = models_to_choose[0]\n",
        "def load_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL)\n",
        "    return model, tokenizer\n",
        "# raise Exception('init once') \n",
        "MODEL, TOKENIZER = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "XLMRobertaConfig {\n",
              "  \"_name_or_path\": \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\",\n",
              "  \"architectures\": [\n",
              "    \"XLMRobertaForSequenceClassification\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 1024,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"\\u2764\",\n",
              "    \"1\": \"\\ud83d\\ude0d\",\n",
              "    \"2\": \"\\ud83d\\ude02\",\n",
              "    \"3\": \"\\ud83d\\udc95\",\n",
              "    \"4\": \"\\ud83d\\udd25\",\n",
              "    \"5\": \"\\ud83d\\ude0a\",\n",
              "    \"6\": \"\\ud83d\\ude0e\",\n",
              "    \"7\": \"\\u2728\",\n",
              "    \"8\": \"\\ud83d\\udc99\",\n",
              "    \"9\": \"\\ud83d\\ude18\",\n",
              "    \"10\": \"\\ud83d\\udcf7\",\n",
              "    \"11\": \"\\ud83c\\uddfa\\ud83c\\uddf8\",\n",
              "    \"12\": \"\\u2600\",\n",
              "    \"13\": \"\\ud83d\\udc9c\",\n",
              "    \"14\": \"\\ud83d\\ude09\",\n",
              "    \"15\": \"\\ud83d\\udcaf\",\n",
              "    \"16\": \"\\ud83d\\ude01\",\n",
              "    \"17\": \"\\ud83c\\udf84\",\n",
              "    \"18\": \"\\ud83d\\udcf8\",\n",
              "    \"19\": \"\\ud83d\\ude1c\",\n",
              "    \"20\": \"\\u2639\\ufe0f\",\n",
              "    \"21\": \"\\ud83d\\ude2d\",\n",
              "    \"22\": \"\\ud83d\\ude14\",\n",
              "    \"23\": \"\\ud83d\\ude21\",\n",
              "    \"24\": \"\\ud83d\\udca2\",\n",
              "    \"25\": \"\\ud83d\\ude24\",\n",
              "    \"26\": \"\\ud83d\\ude33\",\n",
              "    \"27\": \"\\ud83d\\ude43\",\n",
              "    \"28\": \"\\ud83d\\ude29\",\n",
              "    \"29\": \"\\ud83d\\ude20\",\n",
              "    \"30\": \"\\ud83d\\ude48\",\n",
              "    \"31\": \"\\ud83d\\ude44\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 4096,\n",
              "  \"label2id\": {\n",
              "    \"\\u2600\": \"12\",\n",
              "    \"\\u2639\\ufe0f\": \"20\",\n",
              "    \"\\u2728\": \"7\",\n",
              "    \"\\u2764\": \"0\",\n",
              "    \"\\ud83c\\uddfa\\ud83c\\uddf8\": \"11\",\n",
              "    \"\\ud83c\\udf84\": \"17\",\n",
              "    \"\\ud83d\\udc95\": \"3\",\n",
              "    \"\\ud83d\\udc99\": \"8\",\n",
              "    \"\\ud83d\\udc9c\": \"13\",\n",
              "    \"\\ud83d\\udca2\": \"24\",\n",
              "    \"\\ud83d\\udcaf\": \"15\",\n",
              "    \"\\ud83d\\udcf7\": \"10\",\n",
              "    \"\\ud83d\\udcf8\": \"18\",\n",
              "    \"\\ud83d\\udd25\": \"4\",\n",
              "    \"\\ud83d\\ude01\": \"16\",\n",
              "    \"\\ud83d\\ude02\": \"2\",\n",
              "    \"\\ud83d\\ude09\": \"14\",\n",
              "    \"\\ud83d\\ude0a\": \"5\",\n",
              "    \"\\ud83d\\ude0d\": \"1\",\n",
              "    \"\\ud83d\\ude0e\": \"6\",\n",
              "    \"\\ud83d\\ude14\": \"22\",\n",
              "    \"\\ud83d\\ude18\": \"9\",\n",
              "    \"\\ud83d\\ude1c\": \"19\",\n",
              "    \"\\ud83d\\ude20\": \"29\",\n",
              "    \"\\ud83d\\ude21\": \"23\",\n",
              "    \"\\ud83d\\ude24\": \"25\",\n",
              "    \"\\ud83d\\ude29\": \"28\",\n",
              "    \"\\ud83d\\ude2d\": \"21\",\n",
              "    \"\\ud83d\\ude33\": \"26\",\n",
              "    \"\\ud83d\\ude43\": \"27\",\n",
              "    \"\\ud83d\\ude44\": \"31\",\n",
              "    \"\\ud83d\\ude48\": \"30\"\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 514,\n",
              "  \"model_type\": \"xlm-roberta\",\n",
              "  \"num_attention_heads\": 16,\n",
              "  \"num_hidden_layers\": 24,\n",
              "  \"output_past\": true,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"problem_type\": \"single_label_classification\",\n",
              "  \"temperature\": 0.0,\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.26.1\",\n",
              "  \"type_vocab_size\": 1,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 35054\n",
              "}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "EhLb0qgQ82we"
      },
      "outputs": [],
      "source": [
        "class Emojier:\n",
        "  BASE_MODEL = \"amazon-sagemaker-community/xlm-roberta-en-ru-emoji-v2\"\n",
        "  model: Any = None\n",
        "  tokenizer: Any = None\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def setVerbose( v: bool):\n",
        "    Emojier.verbose = v\n",
        "\n",
        "  @staticmethod\n",
        "  def predict( text: str):\n",
        "    inputs = Emojier.tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = Emojier.model(**inputs)\n",
        "    logits = outputs.logits.detach().numpy()[0]\n",
        "    predicted_class = logits.argmax()\n",
        "    return predicted_class\n",
        "    \n",
        "  @staticmethod\n",
        "  def preprocess(text:str):\n",
        "      new_text = []\n",
        "      for t in text.split(\" \"):\n",
        "          t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "          t = 'http' if t.startswith('http') else t\n",
        "          new_text.append(t)\n",
        "      return \" \".join(new_text)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _predict(text:str) -> List[str]:\n",
        "    # Preprocess text (username and link placeholders)\n",
        "    preprocessed = Emojier.preprocess(text)\n",
        "    inputs = Emojier.tokenizer(preprocessed, return_tensors=\"pt\")\n",
        "    preds = Emojier.model(**inputs).logits\n",
        "    scores = torch.nn.functional.softmax(preds, dim=-1).detach().numpy()\n",
        "    sorted_scores = [float(value) for value in np.sort(scores.squeeze())[::-1]]\n",
        "    ranking = np.argsort(scores)\n",
        "    ranking = ranking.squeeze()[::-1]\n",
        "    emojis = [Emojier.model.config.id2label[i] for i in ranking]\n",
        "    return [emo for emo, score in zip(emojis, sorted_scores) if emo != 'üá∫üá∏' and score > 0.02]\n",
        "  \n",
        "  @staticmethod\n",
        "  def encode(text:str,bytes_str:str):\n",
        "    mask = MaskGen(text)\n",
        "    encoded_so_far = ''\n",
        "    ticks = [text[:v] for u,v in mask.NVA_words]\n",
        "    original_length = len(text)\n",
        "    new_ending = lambda x : (len(text) - original_length) + len(x)\n",
        "    for pre_text in ticks:\n",
        "      if Emojier.verbose:\n",
        "        print('-'*20 + 'tick' + '-'*20)\n",
        "      breakPoint = new_ending(pre_text)\n",
        "      pre_text = text[:breakPoint]\n",
        "      emoji_options = gaussian_order(Emojier._predict(text[:breakPoint]))\n",
        "      if len(emoji_options) < 2:\n",
        "        print(f'pre_text={pre_text},not enough options={emoji_options}')\n",
        "        continue\n",
        "      if bytes_str[0] == \"0\":\n",
        "        print(f'pre_text={pre_text},zero start={bytes_str[:5]}')\n",
        "        encoded_so_far += bytes_str[0]\n",
        "        bytes_str = bytes_str[1:]\n",
        "        continue\n",
        "      encoded_so_far += bytes_str[0]\n",
        "      bytes_str = bytes_str[1:] # discard the one\n",
        "      if Emojier.verbose:\n",
        "        print(f\"word: {pre_text} \\nlen: {len(emoji_options)} \\temoji_options: {emoji_options}\")\n",
        "\n",
        "      bits = floor(log2(len(emoji_options)))\n",
        "      taken_bits = bytes_str[:bits]\n",
        "      ind = int(taken_bits, 2)\n",
        "      encoded_so_far += bytes_str[:bits]\n",
        "      bytes_str = bytes_str[bits:]\n",
        "      emojis = emoji_options[ind]\n",
        "\n",
        "      # Mutliplicity\n",
        "      taken_bits = bytes_str[:2]\n",
        "      mult = int(taken_bits, 2)+1\n",
        "      encoded_so_far += bytes_str[:2]\n",
        "      bytes_str = bytes_str[2:]\n",
        "      print(f'encoded_so_far={encoded_so_far}')\n",
        "      if len(emojis) > 0:\n",
        "        text = f'{text[0:breakPoint]} {mult * emojis}{text[breakPoint:]}'\n",
        "      if Emojier.verbose:\n",
        "        print(f'>>>encoding {taken_bits} = {ind} as {emojis}\\nencoded text={text}')\n",
        "    return text, bytes_str\n",
        "  @staticmethod\n",
        "  def int_to_binary_string(n: int, length: int) -> str:\n",
        "    binary_str = bin(n)[2:]  # convert to binary string, remove '0b' prefix\n",
        "    padded_str = binary_str.rjust(length, '0')  # pad with zeros to length\n",
        "    return padded_str\n",
        "  @staticmethod\n",
        "  def cntPrefix(string:str, prefix:str):\n",
        "    for i in range(4,0,-1):\n",
        "      # if Emojier.verbose:\n",
        "      #   print(f\"string={string[:len(prefix*i)]},prefix*i={prefix*i},string.startswith(prefix * i)={string.startswith(prefix * i)}\",end='|')\n",
        "      if string.startswith(prefix * i):\n",
        "        # print('')\n",
        "        return i\n",
        "    # print('')\n",
        "    return 0\n",
        "  @staticmethod\n",
        "  def strip(text:str):\n",
        "    for label in labels:\n",
        "      text = text.replace(' '+label,'')\n",
        "    for label in labels:\n",
        "      text = text.replace(label,'')\n",
        "    return text\n",
        "  @staticmethod\n",
        "  def decode(text:str):\n",
        "    print(\"#\"*20 + \"decoding\" + \"#\"*20)\n",
        "    bytes_str:str = ''\n",
        "    mask = MaskGen(text)\n",
        "    ticks = [text[:v] for u,v in mask.NVA_words if text[u:v] not in labels]\n",
        "    original_length = len(text)\n",
        "    new_ending = lambda x : (len(text) - original_length) + len(x)\n",
        "    emoji_locations = []\n",
        "    for pre_text in ticks:\n",
        "      if Emojier.verbose:\n",
        "        print('-'*20 + 'tick' + '-'*20)\n",
        "      breakPoint = new_ending(pre_text)\n",
        "      pre_text = text[:breakPoint]\n",
        "      print(f'pre_text={pre_text}')\n",
        "      emoji_options = gaussian_order(Emojier._predict(text[:breakPoint]))\n",
        "\n",
        "      emoji = \\\n",
        "          [label for label in labels if text[breakPoint:].startswith(' '+label)][0] \\\n",
        "            if any((text[breakPoint:].startswith(' '+label) for label in labels)) \\\n",
        "              else None\n",
        "      if emoji is not None:\n",
        "        bytes_str += '1' # emoji exist\n",
        "        bits = floor(log2(len(emoji_options)))\n",
        "        idx = emoji_options.index(emoji)\n",
        "        bytes_str += Emojier.int_to_binary_string(idx,bits)\n",
        "        # Multiplicity\n",
        "        # if Emojier.verbose:\n",
        "        #   print('counting multiplicity')\n",
        "        multi = Emojier.cntPrefix(text[breakPoint+1:],emoji)\n",
        "        bytes_str += Emojier.int_to_binary_string(multi-1,2)\n",
        "        emoji_locations.append((breakPoint, breakPoint + 1 + len(emoji)*multi))\n",
        "        if Emojier.verbose:\n",
        "          print(f\"word={pre_text},len(emoji_options)={len(emoji_options)},emoji_options={emoji_options},emoji={emoji},len(emoji)={len(emoji)},multi={multi}\")\n",
        "      else:\n",
        "        if len(emoji_options) < 2:\n",
        "          print(f\"nothing encoded emoji_options={emoji_options}\")\n",
        "        else:\n",
        "          print(f\"no emoji = zero emoji_options={emoji_options}\")\n",
        "          bytes_str += '0'\n",
        "      print(f'bytes_str={bytes_str}')\n",
        "    # remove emojies\n",
        "    original = text\n",
        "    for s,e in reversed(emoji_locations):\n",
        "      original = original[:s] + original[e:]\n",
        "    return original, bytes_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "Emojier.model, Emojier.tokenizer = MODEL, TOKENIZER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "Emojier.verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "_d72MrhPzJA3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running 10 tests\n",
            "--------------------tick--------------------\n",
            "pre_text=Text,zero start=01001\n",
            "--------------------tick--------------------\n",
            "word: Text literals \n",
            "len: 8 \temoji_options: ['üòú', 'üòé', 'üòä', 'üíØ', 'üòÇ', 'üî•', 'üòÅ', 'üòç']\n",
            "encoded_so_far=0100111\n",
            ">>>encoding 11 = 1 as üòé\n",
            "encoded text=Text literals üòéüòéüòéüòé and metacharacters make up this string. The compile function is used to create the pattern.\n",
            "--------------------tick--------------------\n",
            "word: Text literals üòéüòéüòéüòé and metacharacters \n",
            "len: 14 \temoji_options: ['üò≠', '‚ù§', 'üò§', 'üòÅ', '‚òπÔ∏è', 'üòâ', '‚ú®', 'üòä', 'üòÇ', 'üòî', 'üíØ', 'üí¢', 'üòú', 'üò°']\n",
            "encoded_so_far=0100111111000\n",
            ">>>encoding 00 = 6 as ‚ú®\n",
            "encoded text=Text literals üòéüòéüòéüòé and metacharacters ‚ú® make up this string. The compile function is used to create the pattern.\n",
            "--------------------tick--------------------\n",
            "word: Text literals üòéüòéüòéüòé and metacharacters ‚ú® make \n",
            "len: 11 \temoji_options: ['üòú', 'üòâ', 'üòé', '‚ù§', '‚ú®', 'üíØ', 'üòä', 'üî•', 'üòÇ', 'üòÅ', 'üòî']\n",
            "encoded_so_far=0100111111000100000\n",
            ">>>encoding 00 = 0 as üòú\n",
            "encoded text=Text literals üòéüòéüòéüòé and metacharacters ‚ú® make üòú up this string. The compile function is used to create the pattern.\n",
            "--------------------tick--------------------\n",
            "word: Text literals üòéüòéüòéüòé and metacharacters ‚ú® make üòú up this string \n",
            "len: 16 \temoji_options: ['üòé', 'üòÅ', 'üò≠', 'üò°', 'üí¢', 'üòî', 'üòä', 'üò§', 'üòÇ', 'üíØ', 'üòâ', '‚òπÔ∏è', '‚ú®', 'üòú', '‚ù§', 'üî•']\n",
            "encoded_so_far=01001111110001000001001110\n",
            ">>>encoding 10 = 3 as üò°\n",
            "encoded text=Text literals üòéüòéüòéüòé and metacharacters ‚ú® make üòú up this string üò°üò°üò°. The compile function is used to create the pattern.\n",
            "--------------------tick--------------------\n",
            "pre_text=Text literals üòéüòéüòéüòé and metacharacters ‚ú® make üòú up this string üò°üò°üò°. The compile,zero start=00010\n",
            "--------------------tick--------------------\n"
          ]
        }
      ],
      "source": [
        "tests = 10\n",
        "print(f\"Running {tests} tests\")\n",
        "for i in range(tests):\n",
        "  data = random_bit_stream(600)\n",
        "  # text = 'hi, how are you?'\n",
        "  text = LONG_TEXT\n",
        "  verbose = True\n",
        "  encoded_text,rem = Emojier.encode(text,data)\n",
        "  print('rem=',rem)\n",
        "  _, deData = Emojier.decode(encoded_text)\n",
        "  deData += rem\n",
        "  print(f'text=\"{text}\"\\n->\\nencoded_text=\"{encoded_text}\" \\ndata=\"{data}\"\\ndeData=\"{deData}\"\\ndata==deData=\"{data==deData}\"')\n",
        "  print(f'ratio={len(data)-len(rem)} / {len(text)}={(len(data)-len(rem)) / len(text)}')\n",
        "  assert data==deData\n",
        "  print('\\n')\n",
        "  print(\"#\"*100)\n",
        "  print('\\n')\n",
        "\n",
        "# 0000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=  \"010101010101000001111101100011001111010101110010101101101010\"\n",
        "deData=\"01010101010101000000111110110001001001111010101110010101101101010\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "trFfVFPYieTF"
      },
      "source": [
        "## Mental Deterministic Bot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pbY0v-eJJT1J"
      },
      "source": [
        "### Working solution!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJQ82ZyRmfcY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import Dict, List, Tuple\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "ACTION_SPACE = [ 'ask about kids.', \"ask about pets.\", 'talk about work.',\n",
        "               'ask about marital status.', 'talk about travel.', 'ask about age and gender.',\n",
        "        'ask about hobbies.', 'ask about favorite food.', 'talk about movies.',\n",
        "        'talk about music.', 'talk about politics.']\n",
        "\n",
        "@dataclass\n",
        "class Message:\n",
        "    persona: str\n",
        "    text: str\n",
        "\n",
        "class PersonaGPTBot:\n",
        "    def __init__(self, personas:Dict[str, List[str]], action_space=ACTION_SPACE, model_name=\"af1tang/personaGPT\", use_fast=False):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=use_fast)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = self.model.cuda()\n",
        "        if 'Bot' not in personas:\n",
        "          personas['Bot'] = [\"My name is RobotMan\",\"I used to be called cliff steel\",\"I was a Nascar Racer\"]\n",
        "        self.personas = dict()\n",
        "        for k,v in personas.items():\n",
        "          self.personas[k] = self.tokenizer.encode(''.join(['<|p2|>'] + v + ['<|sep|>'] + ['<|start|>']))\n",
        "        self.action_space = action_space\n",
        "        self.dialog_hx = []\n",
        "\n",
        "    def flatten(self, l):\n",
        "        return [item for sublist in l for item in sublist]\n",
        "\n",
        "    def to_data(self, x):\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.cpu()\n",
        "        return x.data.numpy()\n",
        "\n",
        "    def to_var(self, x):\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.Tensor(x)\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.cuda()\n",
        "        return x\n",
        "\n",
        "    def generate_next(self,\n",
        "                  bot_input_ids,\n",
        "                  do_sample=False,  # Default to sampling\n",
        "                  top_k=10,         # Default to no top-k sampling\n",
        "                  top_p=.6,        # Default to no top-p sampling\n",
        "                  temperature=1e-5,   # Default temperature\n",
        "                  max_length=1000, # Maximum length of generated message\n",
        "                  pad_token=None   # Token used for padding if pad_token_id is not provided\n",
        "                  ):\n",
        "        pad_token = pad_token if pad_token is not None else self.tokenizer.eos_token_id\n",
        "\n",
        "        # Generate a full message using the provided model and parameters\n",
        "        full_msg = self.model.generate(\n",
        "            bot_input_ids,\n",
        "            do_sample=do_sample,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            max_length=max_length,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Extract the message from the full generated output and return it\n",
        "        msg = self.to_data(full_msg.detach()[0])[bot_input_ids.shape[-1]:]\n",
        "        return msg\n",
        "\n",
        "\n",
        "    def reply(self,user_input,persona:str = \"Bot\",dialog_hx=None):\n",
        "        # respond to input\n",
        "        dialog_hx= dialog_hx if dialog_hx is not None else self.dialog_hx\n",
        "\n",
        "        # encode the user input\n",
        "        user_inp = self.tokenizer.encode(user_input + self.tokenizer.eos_token)\n",
        "        # append to the chat history\n",
        "        dialog_hx.append(user_inp)\n",
        "\n",
        "        # generate a response while limiting the total chat history to 1000 tokens\n",
        "        bot_input_ids = self.to_var([self.personas[persona] + self.flatten(dialog_hx)]).long()\n",
        "        msg = self.generate_next(bot_input_ids)\n",
        "        response = self.tokenizer.decode(msg, skip_special_tokens=True)\n",
        "\n",
        "        return response, dialog_hx\n",
        "\n",
        "    def ask(self,action:str,dialog_hx=None):\n",
        "        # respond to input\n",
        "        if isinstance(action, int):\n",
        "          action = self.action_space[action]\n",
        "        dialog_hx= dialog_hx if dialog_hx is not None else self.dialog_hx\n",
        "        action_prefix = self.tokenizer.encode(f'<|act|>{action}<|p1|><|sep|><|start|>')\n",
        "        bot_input_ids = self.to_var([action_prefix + self.flatten(dialog_hx)]).long()\n",
        "\n",
        "        # generate query conditioned on action\n",
        "        msg = self.generate_next(bot_input_ids)\n",
        "\n",
        "        query = self.tokenizer.decode(msg, skip_special_tokens=True)\n",
        "\n",
        "        return query, dialog_hx\n",
        "\n",
        "    def resume_chat(self, dialog_hx):\n",
        "        self.dialog_hx = dialog_hx\n",
        "\n",
        "    def converse(self,length, personas:Tuple[str,str], action=None, startMessage:Message=None, dialog_hx=None):\n",
        "        dialog_hx= dialog_hx if dialog_hx is not None else self.dialog_hx\n",
        "        if action is None and startMessage is None:\n",
        "          startMessage = Message(persona=personas[0],text='')\n",
        "        res = ''\n",
        "        responses = []\n",
        "        if action is not None:\n",
        "          res, dialog_hx = self.ask(action,dialog_hx)\n",
        "        else:\n",
        "          res, dialog_hx = self.reply(startMessage.text,startMessage.persona,dialog_hx)\n",
        "        responses.append(Message(text=res,persona=personas[0]))\n",
        "        turn = 1\n",
        "        for i in range(length-1):\n",
        "          res, dialog_hx = self.reply(res,personas[turn],dialog_hx)\n",
        "          responses.append(Message(text=res,persona=personas[turn]))\n",
        "          turn = (turn+1) % 2\n",
        "\n",
        "        return responses , dialog_hx\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxbINN8BpyzH"
      },
      "outputs": [],
      "source": [
        "pg = PersonaGPTBot({'Alice':[\"I'm a french girl\",\"I love art\",\"my name is Alice\"],\"Bob\" :[\"I'm a french boy\",\"I love art\",\"my name is Bob\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxoMbH3wmUZv"
      },
      "outputs": [],
      "source": [
        "dia = []\n",
        "res = [Message(persona='Bob',text='Have you Heard? I\\'m getting married!')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HQS1slVV2DP"
      },
      "outputs": [],
      "source": [
        "print(res[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjUqdCTXERvR"
      },
      "outputs": [],
      "source": [
        "res, dia = pg.converse(10,('Alice','Bob'),startMessage=res[-1],dialog_hx=dia)\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDw_i0bBKXmg"
      },
      "outputs": [],
      "source": [
        "res = None\n",
        "old_res = None\n",
        "for i in range(100):\n",
        "  dia = []\n",
        "  res = [Message(persona='Bob',text='')]\n",
        "  res, dia = pg.converse(10,('Alice','Bob'),startMessage=res[-1],dialog_hx=dia)\n",
        "  if res is not None and old_res is not None and res != old_res:\n",
        "    print('old_res',old_res)\n",
        "    print('res',res)\n",
        "  else:\n",
        "    print(f'test {i} passed!')\n",
        "  old_res = res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j81jFtFilAHT"
      },
      "outputs": [],
      "source": [
        "a = \"\"\"[Message(persona='Alice', text='hi how are you doing'),\n",
        " Message(persona='Bob', text=\"i'm good thanks for asking\"),\n",
        " Message(persona='Alice', text='what do you do for a living'),\n",
        " Message(persona='Bob', text='i am a french boy'),\n",
        " Message(persona='Alice', text='do you have any hobbies'),\n",
        " Message(persona='Bob', text='i like to play sports'),\n",
        " Message(persona='Alice', text='what do you like about sports'),\n",
        " Message(persona='Bob', text=\"they're fun to play\"),\n",
        " Message(persona='Alice', text='do you play often then'),\n",
        " Message(persona='Bob', text='yeah i play soccer a lot')]\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH1QMbgrvCRE"
      },
      "outputs": [],
      "source": [
        "pg.ask('ask about sports',[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mapiHunEp6DV"
      },
      "outputs": [],
      "source": [
        "pg.reply(\"hello i'm mary and yes i've a daughter. do you have any children?\",\"Bob\", [[31373, 466, 345, 423, 597, 1751, 286, 534, 898, 30, 50256]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g6r0AQp4ChsJ"
      },
      "source": [
        "## Putting it all Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWxoFfxfzu6F"
      },
      "outputs": [],
      "source": [
        "#@title Pipe\n",
        "from typing import List, Any, Dict, Callable\n",
        "\n",
        "def pipe(callbacks: List[Callable], config: Dict[str, Any]={}, index=0):\n",
        "    def process_callbacks(state, callbacks: List[Callable], config: Dict[str, Any]={}, index=0):\n",
        "        # Get the current callback\n",
        "        callback = callbacks[index]\n",
        "\n",
        "        # Get the next callback (if exists)\n",
        "        next_callback = None\n",
        "        if index < len(callbacks) - 1:\n",
        "            next_callback = lambda s, c, cf=config: process_callbacks(s, callbacks, cf, index + 1)\n",
        "\n",
        "        # Call the callback with the current state, next callback, and config\n",
        "        state = callback(state, next_callback, config)\n",
        "\n",
        "        # Return the final state\n",
        "        return state\n",
        "\n",
        "    def _pipe(state):\n",
        "        return process_callbacks(state, callbacks, config, index)\n",
        "\n",
        "    return _pipe\n",
        "\n",
        "def bert_callback(state, next_callback, config):\n",
        "    if state is None:\n",
        "        raise ValueError('State is None')\n",
        "\n",
        "    pipe_verbose = config['pipe_verbose']\n",
        "    encode = config['encode']\n",
        "    decode = config['decode']\n",
        "    message_pipe, bytes_pipe = state\n",
        "\n",
        "    if encode:\n",
        "      stega_bert = masked_stego(message_pipe[-1],bytes_pipe[-1], 3, 0.01)\n",
        "      message_pipe.append(stega_bert.encoded_text)\n",
        "      bytes_pipe.append(stega_bert.remaining_bytes)\n",
        "\n",
        "    if next_callback is not None:\n",
        "        state = next_callback(state, next_callback, config)\n",
        "\n",
        "    if decode:\n",
        "      encoded_text = message_pipe.pop()\n",
        "      remaining_bytes = bytes_pipe.pop()\n",
        "      encoded_bytes = masked_stego.decode(encoded_text,3,0.01)\n",
        "      if encode and decode:\n",
        "        assert encoded_bytes + remaining_bytes == bytes_pipe[-1]\n",
        "      else:\n",
        "        message_pipe.append(encoded_text)\n",
        "        bytes_pipe.append(encoded_bytes + remaining_bytes)\n",
        "\n",
        "    return state\n",
        "\n",
        "def emojer_callback(state, next_callback, config):\n",
        "    if state is None:\n",
        "        raise ValueError('State is None')\n",
        "\n",
        "    message_pipe, bytes_pipe = state\n",
        "    text = message_pipe[-1]\n",
        "    data = bytes_pipe[-1]\n",
        "    verbose = config['verbose']\n",
        "\n",
        "    pipe_verbose = config['pipe_verbose']\n",
        "    encode = config['encode']\n",
        "    decode = config['decode']\n",
        "\n",
        "    if encode:\n",
        "      encoded_text,rem = Emojier.encode(text,data,verbose=verbose)\n",
        "      message_pipe.append(encoded_text)\n",
        "      bytes_pipe.append(rem)\n",
        "\n",
        "    if next_callback is not None:\n",
        "        state = next_callback(state, next_callback, config)\n",
        "    else:\n",
        "      print(state)\n",
        "\n",
        "    if decode:\n",
        "      encoded_pipe_text = message_pipe.pop()\n",
        "      rem_pipe_bytes = bytes_pipe.pop()\n",
        "\n",
        "      original_text, deData = Emojier.decode(encoded_pipe_text,verbose=verbose)\n",
        "      deData += rem_pipe_bytes\n",
        "      if encode and decode:\n",
        "        assert deData == bytes_pipe[-1]\n",
        "        assert original_text == message_pipe[-1]\n",
        "      else:\n",
        "        message_pipe.append(original_text)\n",
        "        bytes_pipe.append(deData)\n",
        "\n",
        "    return state\n",
        "\n",
        "def typo_callback(state, next_callback, config):\n",
        "    if state is None:\n",
        "        raise ValueError('State is None')\n",
        "\n",
        "    message_pipe, bytes_pipe = state\n",
        "    text = message_pipe[-1]\n",
        "    data = bytes_pipe[-1]\n",
        "    verbose = config['verbose']\n",
        "    pipe_verbose = config['pipe_verbose']\n",
        "    encode = config['encode']\n",
        "    decode = config['decode']\n",
        "\n",
        "    if pipe_verbose:\n",
        "      print(state)\n",
        "    if encode:\n",
        "\n",
        "      encoded_text, rem = Typo.encode(text,data)\n",
        "\n",
        "      message_pipe.append(encoded_text)\n",
        "      bytes_pipe.append(rem)\n",
        "\n",
        "    if next_callback is not None:\n",
        "        state = next_callback(state, next_callback, config)\n",
        "\n",
        "    if decode :\n",
        "      encoded_pipe_text = message_pipe.pop()\n",
        "      rem_pipe_bytes = bytes_pipe.pop()\n",
        "\n",
        "      original_string, values = Typo.decode(encoded_pipe_text)\n",
        "      if encode and decode:\n",
        "        assert original_string == text\n",
        "        assert values == data\n",
        "      else:\n",
        "        message_pipe.append(original_string)\n",
        "        bytes_pipe.append()\n",
        "\n",
        "\n",
        "    return state\n",
        "\n",
        "callbacks = [bert_callback, emojer_callback,typo_callback]\n",
        "# callbacks = [typo_callback]\n",
        "\n",
        "# Apply the function with an initial state\n",
        "initial_state = [['Hi, How are you?'],[random_bit_stream(30)]]\n",
        "p = pipe(callbacks, {\"verbose\": False,\"pipe_verbose\": False,\"encode\":True,\"decode\":False,\"test\":False})\n",
        "mq, bq = p(initial_state)\n",
        "\n",
        "print(mq[-1],bq[-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DbJ101XFbKa"
      },
      "outputs": [],
      "source": [
        "# PersonaGPTBot_Singleton = PersonaGPTBot({'Alice':[\"I'm a french girl\",\"I love art\",\"my name is Alice\"],\"Bob\" :[\"I'm a french boy\",\"I love art\",\"my name is Bob\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNnH7AK-YIgX"
      },
      "outputs": [],
      "source": [
        "def StegasusEncode(text,bytes_str):\n",
        "  initial_state = [[text],[bytes_str]]\n",
        "  callbacks = [bert_callback, emojer_callback,typo_callback]\n",
        "  p = pipe(callbacks, {\"verbose\": False,\"pipe_verbose\": False,\"encode\":True,\"decode\":False,\"test\":False})\n",
        "  mq, bq = p(initial_state)\n",
        "  return (mq[-1],bq[-1])\n",
        "def StegasusDecode(text):\n",
        "  initial_state = [[text],['']]\n",
        "  callbacks = [bert_callback, emojer_callback,typo_callback]\n",
        "  p = pipe(callbacks, {\"verbose\": False,\"pipe_verbose\": False,\"encode\":False,\"decode\":True,\"test\":False})\n",
        "  mq, bq = p(initial_state)\n",
        "  return (mq[-1],bq[-1])\n",
        "def StegasusTest(text):\n",
        "  initial_state = [[text],[random_bit_stream(len(text))]]\n",
        "  callbacks = [bert_callback, emojer_callback,typo_callback]\n",
        "  p = pipe(callbacks, {\"verbose\": False,\"pipe_verbose\": False,\"encode\":False,\"decode\":True,\"test\":False})\n",
        "  mq, bq = p(initial_state)\n",
        "  return (mq[-1],bq[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUNaP0tPHnI4"
      },
      "outputs": [],
      "source": [
        "LONG_TEXT = '''Metaphysical solipsism is a variety of solipsism. Based on a philosophy of subjective idealism, metaphysical solipsists maintain that the self is the only existing reality and that all other realities, including the external world and other persons.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtZsf0z7hLt-"
      },
      "outputs": [],
      "source": [
        "Famous_Demo = 'Hi, How are you?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWQFFntewLUX"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# xtx = LONG_TEXT\n",
        "xtx = Famous_Demo\n",
        "data = random_bit_stream(len(xtx))\n",
        "returned = StegasusEncode(xtx,data)\n",
        "print((returned,(len(xtx) - len(returned[1])) / len(xtx)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5wq3h1aDRet"
      },
      "outputs": [],
      "source": [
        "from Bot import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro0zbvGLDReu"
      },
      "outputs": [],
      "source": [
        "## OpenAi\n",
        "import openai\n",
        "import re\n",
        "demo_post = re.sub(r'\\s+', ' ', \"\"\"235r/ExperiencedDevs‚Ä¢Posted byu/EcstaticAssignment12 hours agoThe backend generalist software engineer\n",
        "\n",
        "            I think both myself and a lot of my coworkers/friends fall under this category of \"backend non web-dev stack agnostic generalist software engineer\" that seem to hang out in product companies.While I've gotten experience in domains by virtue of the teams and projects I've worked with, I wouldn't really identify them as being my \"specialty\". I've also never really identified with my tech stack, both because it changes a lot and because frankly the complexity of my work never seems to boil down to low level implementation expertise. There are almost never any serious design meetings where the main point of contention is anything that is on the layer of programming patterns or language details (but obviously yes to system design). The problems that I mainly solve seem to be more \"engineering\" than programming, and while I'd say they are complex, they seem to be mostly a function of general analytical reasoning and more system design-level understanding.Is this sort of position actually that common outside of tech companies? I'm asking mostly out of curiosity, but also while I was lucky to land in another tech company after getting laid off in January, if I get laid off again and don't have the same luck, I'm not sure if I should take steps to brand myself as something less generalist when exploring other options.51 commentsAwardsharesave22 people hereu/getsentry¬∑promotedPaste this line into your terminal to use Next.js with Sentry.\n",
        "\n",
        "            sentry.ioInstallComment as No_Door_3720CommentBoldItalicsLinkStrikethroughInline CodeSuperscriptSpoilerHeadingBulleted ListNumbered ListQuote BlockCode BlockTableMarkdown ModeSort by: best|\n",
        "\n",
        "              level 1_sw00 ¬∑ 10 hr. agoLead Developer | 11 YOEGeneralist who works on higher level design, development practices and techniques?Welcome to consulting, brother/sister.156ReplyGive AwardShareReportSaveFollowlevel 2rkeet ¬∑ 8 hr. agoLead Application Engineer / 9 YoE / NLDCan I... Bother you for some tips? ;)OP sounds like me, and I'm looking into different applicable jobs at this moment.So, I'll take any hints, options, etc. as to what to look at, because I don't know whether to look at Lead Developer, Solution Architect, Facility Manager, Integration/service consultant, or how to find a mix.Bit of a problem when I like what I do. Just not where I do it.20ReplyGive AwardShareReportSaveFollowlevel 2bwainfweeze ¬∑ 3 hr. agolevel 2mrcrassic ¬∑ 5 hr. agoYup! Exactly where I landed.2ReplyGive AwardShareReportSaveFollowlevel 1d0s4gw ¬∑ 7 hr. agoAny given system is not supposed to have a high degree of technical complexity. The point of being a senior or staff engineer is to enable juniors and mid level engineers to deliver impact quickly with low risk. If the system is easy to extend and operate then that‚Äôs because the people that designed it did a good job.Your job is to quickly translate vague information into clear requirements into shipped code. No one cares which data structures was used when you recovered $50m a year in opex. Your resume should explain the value that you delivered. The tech stack is ancillary.33ReplyGive AwardShareReportSaveFollowlevel 2cjrun ¬∑ 4 hr. agoProblem is, if you don‚Äôt have those buzzwords on your r√©sum√©, even the hiring manager won‚Äôt be interested.6ReplyGive AwardShareReportSaveFollowlevel 3d0s4gw ¬∑ 3 hr. agoYea exactly. But it‚Äôs at the end of the block on the resume. It‚Äôs not the top line. The focus is the business result.Senior Software Engineer, Company, City, State (Start date - End date)Technical lead for the <name of service>, which <achieved X quantitative result> for <customer type> by <method of solving the problem>distributed systems, Java, SQL, AWS, S3, Linux, and Bash3ReplyGive AwardShareReportSaveFollowlevel 1GargantuChet ¬∑ 10 hr. agoThis sounds like a joy to me. This describes my role in a big manufacturer, and most of the time I feel like I‚Äôm the only one on the planet.Mind if I PM?64ReplyGive AwardShareReportSaveFollowlevel 2EcstaticAssignmentOp ¬∑ 10 hr. agohaha go for it4ReplyGive AwardShareReportSaveFollowlevel 2bizcs ¬∑ 3 hr. agoI also work for a manufacturer in this sort of role, though. I consider us to be large but others from megalith manufacturers might beg to differ.1ReplyGive AwardShareReportSaveFollowlevel 1gabs_ ¬∑ 10 hr. agoI also fit in this category! I'm only a mid-level developer, but I have worked at tech companies previously and I'm now developing a Big Data project at a logistics company.9ReplyGive AwardShareReportSaveFollowlevel 1Inside_Dimension5308 ¬∑ 8 hr. agoI always advocate the backend generalist software engineer. Tech stacks are replaceable. Knowledge to determine which tech stack to use will be eternal.8ReplyGive AwardShareReportSaveFollowlevel 1nutrecht ¬∑ 8 hr. agoLead Software Engineer / EU / 18+ YXPThe problem with being a generalist is that, if you're not careful, your experience remains very shallow. You can end up not having 10 years of experience, but 1 year repeated 10 times.For the most part  my 'brand' as a self employed contractor who focusses on the Java ecosystem doesn't have much to do with Java itself, but more with the type of work I do. I focus on complex enterprise systems, often with a ton of different systems interacting, and providing my clients with deep expertise in how to not turn those into big balls of mud. If you're mostly doing the same simple back-end projects (like in wordpress as an extreme example) you don't get that experience.So I don't agree that what you're describing is 'good' or 'bad'. It really depends on how you plan and advance your career. For example as this generalist if you don't now have cloud-native experience you're IMHO falling behind the curve.28ReplyGive AwardShareReportSaveFollowlevel 1ir0nuckles ¬∑ 6 hr. agoThere are almost never any serious design meetings where the main point of contention is anything that is on the layer of programming patterns or language details (but obviously yes to system design). The problems that I mainly solve seem to be more \"engineering\" than programming, and while I'd say they are complex, they seem to be mostly a function of general analytical reasoning and more system design-level understanding.I'm confused. Isn't this what being a software engineer is?I've never done \"design reviews\" of programming patterns. That's for a code review, or if needed, you can engage your team before you start a project to ensure you're following best practices.This post is really strange to me. If you're asking \"how do I prepare my skillset for find a job outside of tech\" then I would suggest you become really good cloud computing platforms and patterns. Almost every enterprise is using a cloud provider at this point. If you're the expert in AWS, GCP, or Azure, you're probably guaranteed to find some work somewhere in the world working with one of these platforms.4ReplyGive AwardShareReportSaveFollowlevel 1FlutterLovers ¬∑ 10 hr. agoGeneralist will make you a better engineer, but focus will get you hired. Try to become an expert at one backend framework that is currently in demand, while also learning the basics of adjacent systems.30ReplyGive AwardShareReportSaveFollowlevel 2chrismv48 ¬∑ 9 hr. agoWhenever I hear this sentiment I feel confused; all the best tech companies I‚Äôm aware of are explicitly tech agnostic (FAANG as well as best paying startups). It‚Äôs the companies that insist on having experience in a very specific stack that tend to pay poorly in my experience. What am I missing?72ReplyGive AwardShareReportSaveFollowlevel 3Successful_Leg_707 ¬∑ 8 hr. agoMy understanding is the specific tech stack companies tend to ‚Äúhire when it hurts‚Äù.  They want someone already up to speed on a language and framework in demand.  They are less willing to gamble on long term potential.  You get a salary but no RSU to retain you.Tech agnostic companies hire for long term potential and projected growth.  Amazon for example will use a language like Java but develop their own in house framework, so knowledge in a specific framework like Spring is less useful.  The tech companies will have some sort of leetcode interview process that is an indicator for general cognitive ability and fundamental comp sci concepts.  On top of a base salary, you get the RSUs which are like golden handcuffs that encourage you to stay until they vest41ReplyGive AwardShareReportSaveFollowlevel 4generatedcode ¬∑ 8 hr. agotech stack companies tend to ‚Äúhire when it hurts‚Äù.you deserve an award !18ReplyGive AwardShareReportSaveFollowlevel 4EcstaticAssignmentOp ¬∑ 3 hr. agoTech agnostic companies hire for long term potential and projected growth.I think this trend may be part of the picture, but I'm not sure if it's the full picture. The top startups also tend to hire the \"general cognitive ability + fundamentals\" way, despite having the same short timeline requirements, while some legacy companies that tend to have longer tenures hire the more specific way. It's possible it's more a function of a higher hiring bar tending to correlate with the agnostic approach, whichever way that causation goes.1ReplyGive AwardShareReportSaveFollowlevel 3Acidic-Soil ¬∑ 8 hr. agolevel 2ExistentialDroid23 ¬∑ 9 hr. agoI see those claims like \"generalists are better engineers\" but I don't see the connection. Wouldn't diving to one language/framework deep for 2-3 years give you a deeper understanding that you carry around easier later than fumbling 2-3 frameworks on the same timeframe?I guess what I dislike is the equivalency of \"more languages = better engineer\" when in fact what matters is the proper use of the tool, not necessarily how many tools you have.3ReplyGive AwardShareReportSaveFollowlevel 3slightly_offtopic ¬∑ 9 hr. agoEach language/framework has a preference for a certain way of solving problems. Learning several tools is a proxy for learning sev\"\"\")\n",
        "\n",
        "\n",
        "bob = Person(first_name='Bob Doe', gender='male',age=13,city='France') \\\n",
        "  .add_favorite('color','blue') \\\n",
        "  .add_interest('travelling') \\\n",
        "  .add_favorite('dog','Pitbull')\n",
        "alice = Person(first_name='alice wonderland', gender='girl',age=13,city='France') \\\n",
        "  .add_favorite('color','pink') \\\n",
        "  .add_interest('fashon') \\\n",
        "  .add_favorite('dog','Corgie')\n",
        "\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "token = os.getenv('OPEN_AI_KEY')\n",
        "\n",
        "def askGPT(text):\n",
        "  openai.api_key = token\n",
        "  response = openai.Completion.create(\n",
        "    engine = \"text-davinci-003\",\n",
        "    prompt = text,\n",
        "    temperature = 0.6,\n",
        "    max_tokens = 150,\n",
        "  )\n",
        "  return response.choices[0].text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jv9mL161DRex"
      },
      "outputs": [],
      "source": [
        "chat = Chat(alice,bob,askGPT)\n",
        "\n",
        "chat.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky1ukuuZDRfK"
      },
      "outputs": [],
      "source": [
        "chat.messages\n",
        "\n",
        "data = random_bit_stream(10000)\n",
        "\n",
        "for m in chat.stream():\n",
        "  enc,rem = StegasusEncode(m.text,data)\n",
        "  print(enc,rem)\n",
        "  data = rem"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ODi5968TKDdR",
        "Dy9bCvx4G9KS",
        "xfisFBGLfQo6",
        "kYBmlY9oLT2m",
        "TdQ6lNerHwGG",
        "icN5UrprJCh6",
        "JQRmozHXHHDS",
        "qXD3IGdrc9fY",
        "trFfVFPYieTF",
        "utLcUrgQJncb"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
